import cv2
import mediapipe as mp
import numpy as np
import torch
import time
import json
import os

from PIL import ImageFont, ImageDraw, Image
from utils import Vector_Normalization # â­ 1. Vector_Normalization í•¨ìˆ˜ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.

# Mediapipe ì´ˆê¸°í™”
mp_pose = mp.solutions.pose
mp_hands = mp.solutions.hands
pose = mp_pose.Pose(model_complexity=0, min_detection_confidence=0.5, min_tracking_confidence=0.5)
hands = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5)
mp_drawing = mp.solutions.drawing_utils

# ì›¹ìº  ë˜ëŠ” IP ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¼ ì—´ê¸°
# cap = cv2.VideoCapture('http://10.101.112.153:4747/video')
cap = cv2.VideoCapture(0)

if not cap.isOpened():
    print("Error: Could not open video stream.")
    exit()

# --- ì„¤ì •ê°’ ë¡œë“œ ---
MODEL_DIR = "D:\Bit\server\model"
model_path = os.path.join(MODEL_DIR, "lstm_sign_language_model_scripted.pt")
mean_path = os.path.join(MODEL_DIR, "data_mean.npy")
std_path = os.path.join(MODEL_DIR, "data_std.npy")
label_map_path = os.path.join(MODEL_DIR, "label_map.json")

SEQ_LEN = 90
OVERLAP_LEN = 85

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device for GUI inference: {device}")

# ëª¨ë¸ ë¡œë“œ
try:
    model = torch.jit.load(model_path)
    model.eval()
    model.to(device)
    print(f"Model loaded successfully from {model_path} and moved to {device}.")
except Exception as e:
    print(f"Error loading model from {model_path}: {e}")
    exit()

# í‘œì¤€í™” íŒŒë¼ë¯¸í„° ë¡œë“œ
try:
    data_mean = np.load(mean_path)
    data_std = np.load(std_path)
    print("Standardization mean and std loaded successfully.")
except Exception as e:
    print(f"Error loading standardization parameters: {e}")
    exit()

# ë¼ë²¨ ë§µ ë¡œë“œ
try:
    with open(label_map_path, 'r', encoding='utf-8') as f:
        labels_map_raw = json.load(f)
    labels_map = {v: k for k, v in labels_map_raw.items()}
    print("Label map loaded successfully.")
except Exception as e:
    print(f"Error loading label map from {label_map_path}: {e}")
    exit()

# ë²„í¼ ë° ì˜ˆì¸¡ ê´€ë ¨ ë³€ìˆ˜ ì´ˆê¸°í™”
keypoints_buffer = []
prediction_history = [] # ìµœê·¼ ì˜ˆì¸¡ ë‹¨ì–´ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
STABLE_THRESHOLD = 3 # ëª‡ ë²ˆ ì—°ì†ìœ¼ë¡œ ì¼ì¹˜í•´ì•¼ ì¸ì •í• ì§€ ê²°ì • (3ë²ˆ)
stable_prediction = "Waiting..." # ì•ˆì •í™”ëœ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼
stable_confidence = 0.0 
current_prediction = "Waiting..."
current_confidence = 0.0
current_probabilities = {label: 0.0 for label in labels_map.values()}

# --- ğŸ‘‡ ìœ íœ´ ìƒíƒœ ê°ì§€ìš© ë³€ìˆ˜ ì¶”ê°€ ---
IDLE_THRESHOLD = 0.5  # ì›€ì§ì„ ê°ì§€ë¥¼ ìœ„í•œ ì„ê³„ê°’. ì´ ê°’ë³´ë‹¤ ì‘ìœ¼ë©´ ì •ì§€ ìƒíƒœë¡œ ê°„ì£¼.
IDLE_TIME_THRESHOLD = 45 # ìœ íœ´ ìƒíƒœë¡œ íŒë‹¨í•˜ê¸° ìœ„í•œ í”„ë ˆì„ ìˆ˜ (ì•½ 1.5ì´ˆ, 30fps ê¸°ì¤€)
idle_counter = 0      # ì •ì§€ ìƒíƒœê°€ ì§€ì†ëœ í”„ë ˆì„ ìˆ˜ë¥¼ ì„¸ëŠ” ì¹´ìš´í„°

# --- ğŸ’¡ ìˆ˜ì •/ì¶”ê°€ëœ ë¶€ë¶„ ì‹œì‘ ---
# ë™ì  íŠ¹ì§• ê³„ì‚°ì„ ìœ„í•œ ë³€ìˆ˜ ì¶”ê°€
previous_features = None
previous_velocity = None
FEATURE_DIM = 166 # ì›ë³¸ íŠ¹ì§•(ì¢Œí‘œ)ì˜ ì°¨ì›
# --- ğŸ’¡ ìˆ˜ì •/ì¶”ê°€ëœ ë¶€ë¶„ ë ---



# í°íŠ¸ ì„¤ì •
try:
    font_path = "C:/Windows/Fonts/malgunbd.ttf"
    font = ImageFont.truetype(font_path, 30)
    font_small = ImageFont.truetype(font_path, 20)
except IOError:
    print("Error: Could not load font. Using default.")
    font = ImageFont.load_default()
    font_small = ImageFont.load_default()

# --- í”„ë ˆì„ ì²˜ë¦¬ ê´€ë ¨ ë³€ìˆ˜ ---
frame_process_counter = 0
last_results_pose = None
last_results_hands = None
start_time = time.time()
fps_frame_count = 0

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        print("Failed to grab frame.")
        break


    frame_process_counter += 1
    frame_bgr = frame.copy()
    
    # --------------------------------------------------------------------------
    # 1. íŠ¹ì • í”„ë ˆì„ë§ˆë‹¤ ëª¨ë¸ ì…ë ¥ ë°ì´í„° ìƒì„± ë° ì˜ˆì¸¡ (í•µì‹¬ ì²˜ë¦¬ ë¡œì§)
    # --------------------------------------------------------------------------
    if frame_process_counter % 2 == 0:
        frame_small = cv2.resize(frame, (480, 360), interpolation=cv2.INTER_AREA)
        frame_rgb = cv2.cvtColor(frame_small, cv2.COLOR_BGR2RGB)

        results_pose = pose.process(frame_rgb)
        results_hands = hands.process(frame_rgb)
        # ë‚˜ì¤‘ì— ë¶€ë“œëŸ¬ìš´ ì‹œê°í™”ë¥¼ ìœ„í•´ ê²°ê³¼ ì €ì¥
        last_results_pose = results_pose
        last_results_hands = results_hands

        # â­ 2. í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ë° ì •ê·œí™” ë¡œì§ ì „ì²´ ë³€ê²½
        final_features = np.zeros(FEATURE_DIM) # í¬ì¦ˆ(16) + ì™¼ì†(75) + ì˜¤ë¥¸ì†(75) = 166


        if results_pose.pose_landmarks:# í¬ì¦ˆê°€ ê°ì§€ë˜ë©´, ì‹¤ì œ ëœë“œë§ˆí¬ ê°’ìœ¼ë¡œ final_featuresë¥¼ ì±„ì›€
            pose_lm = results_pose.pose_landmarks.landmark

            # ëª¸ì˜ ê¸°ì¤€ì (ì–´ê¹¨ ì¤‘ì‹¬)ê³¼ í¬ê¸°(ì–´ê¹¨ë„ˆë¹„) ê³„ì‚°
            shoulder_center_x = (pose_lm[11].x + pose_lm[12].x) / 2
            shoulder_center_y = (pose_lm[11].y + pose_lm[12].y) / 2
            shoulder_width = np.linalg.norm(
                [pose_lm[11].x - pose_lm[12].x, pose_lm[11].y - pose_lm[12].y]
            ) + 1e-6

            # ìƒì²´ í¬ì¦ˆ íŠ¹ì§• ì¶”ì¶œ ë° ì •ê·œí™”
            pose_indices = [11, 12, 13, 14, 15, 16, 23, 24] # ì–´ê¹¨, íŒ”ê¿ˆì¹˜, ì†ëª©, ê³¨ë°˜
            pose_features = []
            for idx in pose_indices:
                pose_features.append((pose_lm[idx].x - shoulder_center_x) / shoulder_width)
                pose_features.append((pose_lm[idx].y - shoulder_center_y) / shoulder_width)
            
            # ì–‘ì† íŠ¹ì§• ì¶”ì¶œ (ë²¡í„° ì •ê·œí™”)
            left_hand_features = np.zeros(75)
            right_hand_features = np.zeros(75)
            
            if results_hands.multi_hand_landmarks:
                for i, hand_landmarks in enumerate(results_hands.multi_hand_landmarks):
                    handedness = results_hands.multi_handedness[i].classification[0].label
                    joint = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])
                    
                    v, angle_label = Vector_Normalization(joint)
                    hand_features = np.concatenate([v.flatten(), angle_label.flatten()])
                    
                    if handedness == "Left":
                        left_hand_features = hand_features
                    elif handedness == "Right":
                        right_hand_features = hand_features

            # ëª¨ë“  íŠ¹ì§• ê²°í•©
            final_features = np.concatenate([pose_features, left_hand_features, right_hand_features])
            
         # --- ë™ì  íŠ¹ì§• ë° ìœ íœ´ ìƒíƒœ ê°ì§€ (í¬ì¦ˆ ê°ì§€ ì—¬ë¶€ì™€ ìƒê´€ì—†ì´ ì‹¤í–‰) ---
        velocity = np.zeros_like(final_features) if previous_features is None else final_features - previous_features
        acceleration = np.zeros_like(velocity) if previous_velocity is None else velocity - previous_velocity
            
        # ìœ íœ´ ìƒíƒœ ê°ì§€
        movement = np.sum(np.abs(velocity))
        if movement < IDLE_THRESHOLD:
            idle_counter += 1
        else:
            idle_counter = 0

        if idle_counter >= IDLE_TIME_THRESHOLD:
            stable_prediction = "Waiting..."
            stable_confidence = 0.0
            prediction_history.clear()
            idle_counter = 0
            
        # ëª¨ë“  íŠ¹ì§• ê²°í•© í›„ ë²„í¼ì— ì¶”ê°€
        combined_features = np.concatenate([final_features, velocity, acceleration])
        keypoints_buffer.append(combined_features)
        
        # ë‹¤ìŒ í”„ë ˆì„ ê³„ì‚°ì„ ìœ„í•´ í˜„ì¬ ê°’ ì—…ë°ì´íŠ¸
        previous_features = final_features
        previous_velocity = velocity

         # --- ëª¨ë¸ ì˜ˆì¸¡ (ë²„í¼ê°€ ë‹¤ ì°¼ì„ ë•Œë§Œ ì‹¤í–‰) ---
        if len(keypoints_buffer) >= SEQ_LEN:
            sequence_to_predict = np.array(keypoints_buffer[-SEQ_LEN:])
            sequence_standardized = (sequence_to_predict - data_mean) / (data_std + 1e-8)
            input_tensor = torch.tensor(sequence_standardized, dtype=torch.float32).unsqueeze(0).to(device)

            with torch.no_grad():
                outputs = model(input_tensor)
                probabilities = torch.softmax(outputs, dim=1)
            
            confidence, predicted_idx = torch.max(probabilities, 1)
            
            # ì•ˆì •í™” í•„í„° ë¡œì§
            if confidence.item() > 0.7:
                predicted_label = labels_map.get(predicted_idx.item(), "Unknown")
                prediction_history.append(predicted_label)
                if len(prediction_history) > STABLE_THRESHOLD:
                    prediction_history.pop(0)
                if len(prediction_history) == STABLE_THRESHOLD and len(set(prediction_history)) == 1:
                    if stable_prediction != prediction_history[0]:
                        stable_prediction = prediction_history[0]
                        stable_confidence = confidence.item() * 100
            
            # ì˜¤ë˜ëœ ë²„í¼ ì œê±° (ìŠ¬ë¼ì´ë”© ìœˆë„ìš°)
            del keypoints_buffer[:SEQ_LEN - OVERLAP_LEN]
    
    # --------------------------------------------------------------------------
    # 2. ì‹œê°í™” ë¡œì§ (ë§¤ í”„ë ˆì„ë§ˆë‹¤ ë¶€ë“œëŸ½ê²Œ ë³´ì—¬ì£¼ê¸°)
    # --------------------------------------------------------------------------
    if last_results_pose and last_results_pose.pose_landmarks:
        # ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸° (í•„ìš” ì‹œ ì£¼ì„ í•´ì œ)
        # mp_drawing.draw_landmarks(...) 
        pass
    
    img_pil = Image.fromarray(cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    
    # ì˜ˆì¸¡ ê²°ê³¼ í…ìŠ¤íŠ¸ í‘œì‹œ
    text_prediction = f"Prediction: {stable_prediction} ({stable_confidence:.2f}%)"
    draw.text((10, 30), text_prediction, font=font, fill=(255, 255, 0))
    
    # ë²„í¼ ìƒíƒœ í…ìŠ¤íŠ¸ í‘œì‹œ
    text_frame_count = f"Frames in buffer: {len(keypoints_buffer)} / {SEQ_LEN}"
    draw.text((10, 65), text_frame_count, font=font_small, fill=(255, 165, 0))
    
    frame_bgr = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
    cv2.imshow('Sign Language Recognition', frame_bgr)

    # --------------------------------------------------------------------------
    # 3. í‚¤ë³´ë“œ ì…ë ¥ ì²˜ë¦¬
    # --------------------------------------------------------------------------
    key = cv2.waitKey(1) & 0xFF
    if key == ord('q'):
        break
    elif key == ord('r'):
        print("Prediction state has been reset by user.")
        keypoints_buffer.clear()
        prediction_history.clear()
        stable_prediction = "Waiting..."
        stable_confidence = 0.0
        idle_counter = 0

cap.release()
cv2.destroyAllWindows()