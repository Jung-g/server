# model/SignLanguageRecognizer.py

from collections import deque
import cv2
import mediapipe as mp
import numpy as np
import torch
import tensorflow as tf
import json
import os

CONFIG = {
    "MODEL_DIR": "./model",
    "SEQ_LEN_WORD": 45,
    "STABLE_THRESHOLD_WORD": 1,
    "CONF_THRESHOLD_WORD": 0.85,
    "SEQ_LEN_ALPHABET": 10,
    "CONF_THRESHOLD_ALPHABET": 0.78,
    "IDLE_TIME_SECS": 1.8,
    "MOVEMENT_THRESHOLD": 0.6,
}

class FeatureExtractor:
    """
    Mediapipeë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ í”„ë ˆì„ì—ì„œ ìˆ˜ì–´ ì¸ì‹ì„ ìœ„í•œ í¬ì¦ˆ ë° ì† íŠ¹ì§•ì ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ë‹¨ì–´ ëª¨ë¸ê³¼ ì§€ë¬¸ì ëª¨ë¸ì„ ìœ„í•œ íŠ¹ì§•ì„ ëª¨ë‘ ê³„ì‚°í•˜ê³  ì›€ì§ì„ì„ ê°ì§€í•©ë‹ˆë‹¤.
    (ê¸°ì¡´ LSTM_video_OOP2B.pyì˜ í´ë˜ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•¨)
    """
    def __init__(self):
        self.mp_pose = mp.solutions.pose
        self.mp_hands = mp.solutions.hands
        self.pose = self.mp_pose.Pose(model_complexity=0, min_detection_confidence=0.5, min_tracking_confidence=0.8)
        self.hands = self.mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.8)
        self.previous_features = None
        self.previous_velocity = None

    def _get_75d_hand_features(self, joint):
        v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :]
        v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :]
        v = v2 - v1
        v = v / (np.linalg.norm(v, axis=1)[:, np.newaxis] + 1e-6)
        angle_v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :2]
        angle_v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :2]
        angle_v = angle_v2 - angle_v1
        angle_v = angle_v / (np.linalg.norm(angle_v, axis=1)[:, np.newaxis] + 1e-6)
        angle = np.degrees(np.arccos(np.einsum('nt,nt->n',
            angle_v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:],
            angle_v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])))
        return np.concatenate([v.flatten(), angle])

    def _get_55d_hand_features(self, joint):
        v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :2]
        v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :2]
        v = v2 - v1
        v = v / (np.linalg.norm(v, axis=1)[:, np.newaxis] + 1e-6)
        angle = np.degrees(np.arccos(np.einsum('nt,nt->n',
            v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:],
            v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])))
        return np.concatenate([v.flatten(), angle])

    def extract(self, frame):
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results_pose = self.pose.process(frame_rgb)
        results_hands = self.hands.process(frame_rgb)

        pose_features = np.zeros(16)
        if results_pose.pose_landmarks:
            pose_lm = results_pose.pose_landmarks.landmark
            shoulder_center_x = (pose_lm[11].x + pose_lm[12].x) / 2
            shoulder_center_y = (pose_lm[11].y + pose_lm[12].y) / 2
            shoulder_width = np.linalg.norm([pose_lm[11].x - pose_lm[12].x, pose_lm[11].y - pose_lm[12].y]) + 1e-6
            temp_pose_features = []
            for idx in [11, 12, 13, 14, 15, 16, 23, 24]:
                temp_pose_features.append((pose_lm[idx].x - shoulder_center_x) / shoulder_width)
                temp_pose_features.append((pose_lm[idx].y - shoulder_center_y) / shoulder_width)
            pose_features = np.array(temp_pose_features)

        left_hand_features, right_hand_features = np.zeros(75), np.zeros(75)
        alphabet_feature = None
        if results_hands.multi_hand_landmarks:
            for i, hand_landmarks in enumerate(results_hands.multi_hand_landmarks):
                handedness = results_hands.multi_handedness[i].classification[0].label
                joint = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])
                hand_features_75d = self._get_75d_hand_features(joint)
                if handedness == "Left":
                    left_hand_features = hand_features_75d
                elif handedness == "Right":
                    right_hand_features = hand_features_75d
                    alphabet_feature = self._get_55d_hand_features(joint)

        word_model_features = np.concatenate([pose_features, left_hand_features, right_hand_features])
        if self.previous_features is None: self.previous_features = np.zeros_like(word_model_features)
        velocity = word_model_features - self.previous_features
        if self.previous_velocity is None: self.previous_velocity = np.zeros_like(velocity)
        acceleration = velocity - self.previous_velocity
        movement = np.sum(np.abs(velocity))
        self.previous_features = word_model_features
        self.previous_velocity = velocity
        dynamic_word_features = np.concatenate([word_model_features, velocity, acceleration])
        return dynamic_word_features, alphabet_feature, movement


        #num_hands_detected = len(results_hands.multi_hand_landmarks) if results_hands.multi_hand_landmarks else 0
        #return dynamic_word_features, alphabet_feature, movement,num_hands_detected

    def close(self):
        self.pose.close()
        self.hands.close()

class Predictor:
    """
    ë‹¨ì–´(PyTorch) ë° ì§€ë¬¸ì(TFLite) ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    (ê¸°ì¡´ LSTM_video_OOP2B.pyì˜ í´ë˜ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•¨)
    """
    def __init__(self, config):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.config = config

        # Word Model (PyTorch)
        self.word_model = torch.jit.load(os.path.join(config['MODEL_DIR'], "lstm_sign_language_model_scripted.pt")).to(self.device)
        self.word_model.eval()
        self.data_mean = np.load(os.path.join(config['MODEL_DIR'], "data_mean.npy"))
        self.data_std = np.load(os.path.join(config['MODEL_DIR'], "data_std.npy"))
        with open(os.path.join(config['MODEL_DIR'], "label_map.json"), 'r', encoding='utf-8') as f:
            self.word_labels_map = {v: k for k, v in json.load(f).items()}

        # Fingerspelling Model (TFLite)
        tflite_path = os.path.join(config['MODEL_DIR'], "multi_hand_gesture_classifier.tflite")
        self.alphabet_interpreter = tf.lite.Interpreter(model_path=tflite_path)
        self.alphabet_interpreter.allocate_tensors()
        self.alphabet_input_details = self.alphabet_interpreter.get_input_details()
        self.alphabet_output_details = self.alphabet_interpreter.get_output_details()
        self.alphabet_actions = ['ã„±', 'ã„´', 'ã„·', 'ã„¹', 'ã…', 'ã…‚', 'ã……', 'ã…‡', 'ã…ˆ', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…','ã…', 'ã…‘', 'ã…“', 'ã…•', 'ã…—', 'ã…›', 'ã…œ', 'ã… ', 'ã…¡', 'ã…£','ã…', 'ã…’', 'ã…”', 'ã…–', 'ã…¢', 'ã…š', 'ã…Ÿ']

        # Buffers
        self.word_buffer = deque()
        self.word_history = []
        self.alphabet_buffer = deque(maxlen=config['SEQ_LEN_ALPHABET'])
        self.alphabet_confirm_buffer = deque(maxlen=5)
        self.last_confirmed_alphabet = None

    def predict_word(self, features):
        self.word_buffer.append(features)
        if len(self.word_buffer) > self.config['SEQ_LEN_WORD']: self.word_buffer.popleft()
        if len(self.word_buffer) < self.config['SEQ_LEN_WORD']: return None, 0.0

        sequence = np.array(list(self.word_buffer))
        normalized_sequence = (sequence - self.data_mean) / (self.data_std + 1e-8)
        input_tensor = torch.tensor(normalized_sequence, dtype=torch.float32).unsqueeze(0).to(self.device)

        with torch.no_grad():
            probabilities = torch.softmax(self.word_model(input_tensor), dim=1)
        confidence, idx = torch.max(probabilities, 1)
        confidence_item = confidence.item()
        
        # ë””ë²„ê¹…ìš© ì¶œë ¥
        label_for_debug = self.word_labels_map.get(idx.item(), "Unknown")
        print(f"    [Predictor|Word] Raw Predict: '{label_for_debug}' (Conf: {confidence_item:.4f})")

        if confidence_item < self.config['CONF_THRESHOLD_WORD']: return None, 0.0
        label = self.word_labels_map.get(idx.item(), "Unknown")
        self.word_history.append(label)
        if len(self.word_history) > self.config['STABLE_THRESHOLD_WORD']: self.word_history.pop(0)
        if len(self.word_history) == self.config['STABLE_THRESHOLD_WORD'] and len(set(self.word_history)) == 1:
            return self.word_history[0], confidence_item
        return None, 0.0

    def predict_fingerspelling(self, features):
        if features is None:
            self.alphabet_confirm_buffer.append(None)
            self.last_confirmed_alphabet = None
            return None, 0.0

        self.alphabet_buffer.append(features)
        if len(self.alphabet_buffer) < self.config['SEQ_LEN_ALPHABET']: return None, 0.0

        input_data = np.expand_dims(np.array(self.alphabet_buffer, dtype=np.float32), axis=0)
        self.alphabet_interpreter.set_tensor(self.alphabet_input_details[0]['index'], input_data)
        self.alphabet_interpreter.invoke()
        y_pred = self.alphabet_interpreter.get_tensor(self.alphabet_output_details[0]['index'])
        i_pred = int(np.argmax(y_pred[0]))
        confidence = y_pred[0][i_pred]
        
        # ë””ë²„ê¹…ìš© ì¶œë ¥
        char_for_debug = self.alphabet_actions[i_pred]
        print(f"    [ğŸ”Predictor|Alphabet] Raw Predict: '{char_for_debug}' (Conf: {confidence:.4f})")

        if confidence > self.config['CONF_THRESHOLD_ALPHABET']:
            self.alphabet_confirm_buffer.append(self.alphabet_actions[i_pred])
        else:
            self.alphabet_confirm_buffer.append(None)
            self.last_confirmed_alphabet = None

        if (len(self.alphabet_confirm_buffer) == self.alphabet_confirm_buffer.maxlen and
                len(set(self.alphabet_confirm_buffer)) == 1 and self.alphabet_confirm_buffer[0] is not None):
            current_stable_prediction = self.alphabet_confirm_buffer[0]
            if current_stable_prediction != self.last_confirmed_alphabet:
                self.last_confirmed_alphabet = current_stable_prediction
                return current_stable_prediction, confidence
        return None, 0.0

    def reset_word_buffer(self):
        self.word_buffer.clear()
        self.word_history.clear()

    def reset(self):
        self.reset_word_buffer()
        self.alphabet_buffer.clear()
        self.alphabet_confirm_buffer.clear()
        self.last_confirmed_alphabet = None


class SignLanguageRecognizer:
    """
    í†µí•©ëœ ìˆ˜ì–´ ì¸ì‹ê¸° í´ë˜ìŠ¤.
    ì‹¤ì‹œê°„ í”„ë ˆì„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ì™€ ë‹¨ì¼ ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ë¥¼ ëª¨ë‘ ë‹´ë‹¹í•©ë‹ˆë‹¤.
    """
    def __init__(self, config):
        self.config = config
        self.feature_extractor = FeatureExtractor()
        self.predictor = Predictor(config)
        self.sentence_words = []
        self.idle_counter = 0
        fps = 30  # Assume 30 fps
        self.IDLE_TIME_THRESHOLD_FRAMES = int(self.config['IDLE_TIME_SECS'] * fps)
        
        
        #self.HAND_HISTORY_LENGTH = 15 
        #self.hand_presence_history = deque(maxlen=self.HAND_HISTORY_LENGTH)
        
        self.frame_process_counter = 0

    def reset(self):
        """ì¸ì‹ê¸° ìƒíƒœ(ëˆ„ì  ë‹¨ì–´, ì¹´ìš´í„°, ì˜ˆì¸¡ ë²„í¼)ë¥¼ ì™„ì „íˆ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.sentence_words.clear()
        self.idle_counter = 0
        self.predictor.reset()
        self.frame_process_counter = 0
        print("--- Recognizer state has been reset. ---")

    def process_frame(self, frame: np.ndarray) -> str | None:
        """
        ë‹¨ì¼ í”„ë ˆì„ì„ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´/ì§€ë¬¸ìë¥¼ ì¸ì‹í•˜ê³  ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        ìƒˆë¡œìš´ í† í°(ë‹¨ì–´/ì§€ë¬¸ì)ì´ ì•ˆì •ì ìœ¼ë¡œ ì¸ì‹ë˜ë©´ í•´ë‹¹ í† í°ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ìš© ë©”ì†Œë“œ)
        """
        
        self.frame_process_counter += 1
        if self.frame_process_counter % 2 != 0:
            print(f"    [SKIPPING FRAME #{self.frame_process_counter}]")
            return None
        
        
        word_feats, alphabet_feats, movement = self.feature_extractor.extract(frame)

        if movement < self.config['MOVEMENT_THRESHOLD']:
            self.idle_counter += 1
        else:
            self.idle_counter = 0

        if self.idle_counter >= self.IDLE_TIME_THRESHOLD_FRAMES:
            if self.sentence_words:
                print(f"IDLE - Resetting sentence: {' '.join(self.sentence_words)}")
            self.reset()
            return None

        predicted_word, word_conf = self.predictor.predict_word(word_feats)
        predicted_alphabet, alphabet_conf = self.predictor.predict_fingerspelling(alphabet_feats)

        # ë””ë²„ê¹…ìš© ì¶œë ¥
        print(f"[âš™ï¸Recognizer|Stats] WordConf={word_conf:.2f}, AlphaConf={alphabet_conf:.2f}, Movement={movement:.2f}, Idle={self.idle_counter}")
        
        newly_recognized_token = None
        # ì§€ë¬¸ì ì‹ ë¢°ë„ê°€ ë‹¨ì–´ ì‹ ë¢°ë„ë³´ë‹¤ 0.1(10%) ì´ìƒ ë†’ì„ ë•Œë§Œ ì§€ë¬¸ìë¡œ ì¸ì •
        if predicted_alphabet and alphabet_conf > self.config.get('CONF_THRESHOLD_ALPHABET', 0.8) and alphabet_conf > word_conf + 0.1:
            self.idle_counter = 0
            if not self.sentence_words or self.sentence_words[-1] != predicted_alphabet:
                self.sentence_words.append(predicted_alphabet)
                newly_recognized_token = predicted_alphabet
                print(f"    âœ… [Recognizer] Alphabet Appended: '{predicted_alphabet}' (Conf: {alphabet_conf:.2f}) -> Current: '{' '.join(self.sentence_words)}'")
            self.predictor.reset_word_buffer()
        elif predicted_word and word_conf > self.config.get('CONF_THRESHOLD_WORD', 0.89):
            self.idle_counter = 0
            if not self.sentence_words or self.sentence_words[-1] != predicted_word:
                self.sentence_words.append(predicted_word)
                newly_recognized_token = predicted_word
                print(f"    âœ… [Recognizer] Word Appended: '{predicted_word}' (Conf: {word_conf:.2f}) -> Current: '{' '.join(self.sentence_words)}'")
            self.predictor.reset_word_buffer()

        return newly_recognized_token

    
    def get_full_sentence(self) -> str:
        """í˜„ì¬ê¹Œì§€ ëˆ„ì ëœ ì „ì²´ ë¬¸ì¥ì„ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return " ".join(self.sentence_words)

    def recognize_from_video_file(self, video_path: str) -> str:
        """
        í•˜ë‚˜ì˜ ë¹„ë””ì˜¤ íŒŒì¼ ì „ì²´ë¥¼ ì²˜ë¦¬í•˜ì—¬ ìµœì¢… ì¸ì‹ëœ ë¬¸ì¥(ë˜ëŠ” ë‹¨ì–´)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        (í•™ìŠµ ë° ë™ì˜ìƒ íŒŒì¼ ë²ˆì—­ìš© ë©”ì†Œë“œ)
        """
        self.reset()  # ë©”ì†Œë“œ ì‹¤í–‰ ì „ ìƒíƒœë¥¼ ê¹¨ë—í•˜ê²Œ ì´ˆê¸°í™”
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"Error: Could not open video file at {video_path}")
            return "ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        frame_idx = 0 # ---  ë””ë²„ê¹…ìš© í”„ë ˆì„ ì¹´ìš´í„°  ---
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_idx += 1
            
            
            
            if frame_idx % 100 == 0:
                print(f"    [â„¹ï¸] Processing video file... Frame {frame_idx}")
            
            # ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ë’¤ì§‘ì–´ ì²˜ë¦¬ (í•„ìš”ì‹œ)
            frame = cv2.flip(frame, 1)
            self.process_frame(frame) # ê° í”„ë ˆì„ì„ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬í•˜ì—¬ ë‚´ë¶€ ìƒíƒœì— ë‹¨ì–´ë¥¼ ì¶•ì 

        cap.release()
        final_sentence = self.get_full_sentence()
        print(f"Video processing finished. Result: '{final_sentence}'")
        self.reset()  
        
        
        if not final_sentence:
            return "í•™ìŠµë˜ì§€ ì•Šì€ ë™ì‘ì…ë‹ˆë‹¤"
            
        return final_sentence

    def close(self):
        """Mediapipe ë¦¬ì†ŒìŠ¤ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."""
        self.feature_extractor.close()