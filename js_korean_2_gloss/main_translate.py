# main.py

# 1. í•„ìš”í•œ ëª¨ë“ˆê³¼ í•¨ìˆ˜ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
# resource_loaderë¥¼ ê°€ì¥ ë¨¼ì € importí•˜ë©´, í”„ë¡œê·¸ë¨ ì‹œì‘ ì‹œ ëª¨ë“  ë¦¬ì†ŒìŠ¤ê°€ ë¡œë”©ë©ë‹ˆë‹¤.
from resource_loader import resources
from calculate_korean2gloss import get_best_translation
from sbert_kosimcse_module import sbert_kosimcse_search
from best_similar import find_best_candidate_iterative

def process_sentence(text):
    """
    í•œ ë¬¸ì¥ì„ ì…ë ¥ë°›ì•„ í‘œì œì–´ ë³€í™˜ì˜ ëª¨ë“  ë‹¨ê³„ë¥¼ ì²˜ë¦¬í•˜ê³ ,
    ìµœì¢… ê²°ê³¼ë¥¼ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # --- [0ë‹¨ê³„: 1ì°¨ ë³€í™˜] ---
    best_result = get_best_translation(text)
    model_name = best_result['model_name']
    translation = best_result['translation']
    avg_similarity = best_result['avg_similarity']
    
    print(f"\n[âœ… ì„ íƒëœ ëª¨ë¸] {model_name}")
    print(f"[ë³€í™˜ ê²°ê³¼] {translation}")
    print(f"[í‰ê·  ìœ ì‚¬ë„] {avg_similarity:.4f}")
    
    words = translation.split()
    # ì¤‘ì•™ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ìì˜ ë‹¨ì–´ ì‚¬ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    word_status = {word: word in resources.lemmas_set for word in words}
    words_to_search = [word for word, status in word_status.items() if not status]

    # --- [1ë‹¨ê³„: ì‚¬ì „ ë‹¨ì–´ í™•ì¸] ---
    print("\n--- [1ë‹¨ê³„: ì‚¬ì „ ë‹¨ì–´ í™•ì¸ ê²°ê³¼] ---")
    print(f"ë‹¨ì–´ë³„ ìƒíƒœ: {word_status}")
    
    # --- [2ë‹¨ê³„ & 3ë‹¨ê³„: ìœ ì‚¬ì–´ ê²€ìƒ‰ ë° ìµœì¢… ì„ íƒ] ---
    # ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ê°€ ìˆì„ ê²½ìš°ì—ë§Œ ì‹¤í–‰í•©ë‹ˆë‹¤.
    if words_to_search:
        print(f"ğŸ‘‰ ìœ ì‚¬ì–´ ê²€ìƒ‰ í•„ìš” ë‹¨ì–´: {words_to_search}")
        search_results = sbert_kosimcse_search(words_to_search, topn=3)

        final_sentence_parts = []
        for word in words:
            if word_status[word]:
                final_sentence_parts.append(word)
            else:
                results_for_word = search_results.get(word, {})
                result_details, reason = find_best_candidate_iterative(results_for_word)

                if result_details:
                    final_word = result_details['word']
                    final_sentence_parts.append(final_word)
                    print(f"'{word}' â¡ï¸ '{final_word}' (ë¹ˆë„:{result_details['frequency']}, ì ìˆ˜:{result_details['score']:.4f}) | {reason}")
                else:
                    failed_word = f"<{word}?>"
                    final_sentence_parts.append(failed_word)
                    print(f"'{word}' â¡ï¸ ? | {reason}")
        
        return final_sentence_parts
    
    # ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ê°€ ì—†ì„ ê²½ìš°, 1ì°¨ ë³€í™˜ ê²°ê³¼ë¥¼ ë°”ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    else:
        print("\nâœ… ëª¨ë“  ë‹¨ì–´ê°€ ì‚¬ì „ì— ì¡´ì¬í•˜ì—¬ ì¶”ê°€ ê²€ìƒ‰ ì—†ì´ ë³€í™˜ì„ ì™„ë£Œí•©ë‹ˆë‹¤.")
        return translation.split()

def main_translate(text: str):
        
    # í•µì‹¬ ë¡œì§ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
    final_list = process_sentence(text)
        
    # ìµœì¢… ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥í•©ë‹ˆë‹¤.
    print("\n" + "="*50)
    print(" ìµœì¢… í‘œì œì–´ ë¦¬ìŠ¤íŠ¸ ".center(50, "="))
    print(final_list)
    # [ì¶”ê°€] final_list ë³€ìˆ˜ì˜ íƒ€ì…ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
    print(f"(íƒ€ì…: {type(final_list)})")
    print("="*50 + "\n")
    
    return final_list