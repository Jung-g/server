{
  "best_global_step": 146000,
  "best_metric": 0.6177476644515991,
  "best_model_checkpoint": "D:\\kot5_finetune\\checkpoint-146000",
  "epoch": 21.575291857543963,
  "eval_steps": 2000,
  "global_step": 146000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.014777597162701345,
      "grad_norm": 163.15318298339844,
      "learning_rate": 4.9853701788089255e-05,
      "loss": 81.1365,
      "step": 100
    },
    {
      "epoch": 0.02955519432540269,
      "grad_norm": 23.64801788330078,
      "learning_rate": 4.9705925816462244e-05,
      "loss": 34.1611,
      "step": 200
    },
    {
      "epoch": 0.04433279148810403,
      "grad_norm": 11.149774551391602,
      "learning_rate": 4.955814984483523e-05,
      "loss": 12.9858,
      "step": 300
    },
    {
      "epoch": 0.05911038865080538,
      "grad_norm": 3.7206881046295166,
      "learning_rate": 4.941037387320822e-05,
      "loss": 6.3344,
      "step": 400
    },
    {
      "epoch": 0.07388798581350672,
      "grad_norm": 1.0845375061035156,
      "learning_rate": 4.92625979015812e-05,
      "loss": 3.5802,
      "step": 500
    },
    {
      "epoch": 0.08866558297620807,
      "grad_norm": 1.035652756690979,
      "learning_rate": 4.911482192995419e-05,
      "loss": 3.3159,
      "step": 600
    },
    {
      "epoch": 0.10344318013890941,
      "grad_norm": 0.6640552878379822,
      "learning_rate": 4.896704595832718e-05,
      "loss": 3.1377,
      "step": 700
    },
    {
      "epoch": 0.11822077730161076,
      "grad_norm": 1.092692494392395,
      "learning_rate": 4.8819269986700166e-05,
      "loss": 3.0453,
      "step": 800
    },
    {
      "epoch": 0.1329983744643121,
      "grad_norm": 1.023925542831421,
      "learning_rate": 4.867149401507315e-05,
      "loss": 2.8767,
      "step": 900
    },
    {
      "epoch": 0.14777597162701345,
      "grad_norm": 0.8848371505737305,
      "learning_rate": 4.8523718043446136e-05,
      "loss": 2.9006,
      "step": 1000
    },
    {
      "epoch": 0.1625535687897148,
      "grad_norm": 0.6704682111740112,
      "learning_rate": 4.8375942071819124e-05,
      "loss": 2.7536,
      "step": 1100
    },
    {
      "epoch": 0.17733116595241613,
      "grad_norm": 0.8540563583374023,
      "learning_rate": 4.822816610019211e-05,
      "loss": 2.5884,
      "step": 1200
    },
    {
      "epoch": 0.1921087631151175,
      "grad_norm": 1.005766749382019,
      "learning_rate": 4.80803901285651e-05,
      "loss": 2.5651,
      "step": 1300
    },
    {
      "epoch": 0.20688636027781881,
      "grad_norm": 0.8642913699150085,
      "learning_rate": 4.793261415693808e-05,
      "loss": 2.5095,
      "step": 1400
    },
    {
      "epoch": 0.22166395744052017,
      "grad_norm": 1.269862413406372,
      "learning_rate": 4.778483818531107e-05,
      "loss": 2.4738,
      "step": 1500
    },
    {
      "epoch": 0.23644155460322153,
      "grad_norm": 0.7830051183700562,
      "learning_rate": 4.763706221368406e-05,
      "loss": 2.4016,
      "step": 1600
    },
    {
      "epoch": 0.25121915176592285,
      "grad_norm": 0.8306373953819275,
      "learning_rate": 4.748928624205705e-05,
      "loss": 2.3221,
      "step": 1700
    },
    {
      "epoch": 0.2659967489286242,
      "grad_norm": 1.13076651096344,
      "learning_rate": 4.734151027043003e-05,
      "loss": 2.3497,
      "step": 1800
    },
    {
      "epoch": 0.28077434609132557,
      "grad_norm": 13.63926887512207,
      "learning_rate": 4.719373429880302e-05,
      "loss": 2.2888,
      "step": 1900
    },
    {
      "epoch": 0.2955519432540269,
      "grad_norm": 0.9141480922698975,
      "learning_rate": 4.7045958327176005e-05,
      "loss": 2.2644,
      "step": 2000
    },
    {
      "epoch": 0.2955519432540269,
      "eval_loss": 2.0192031860351562,
      "eval_runtime": 71.7357,
      "eval_samples_per_second": 167.699,
      "eval_steps_per_second": 20.966,
      "step": 2000
    },
    {
      "epoch": 0.3103295404167282,
      "grad_norm": 1.449074387550354,
      "learning_rate": 4.689818235554899e-05,
      "loss": 2.2153,
      "step": 2100
    },
    {
      "epoch": 0.3251071375794296,
      "grad_norm": 1.0782365798950195,
      "learning_rate": 4.6750406383921975e-05,
      "loss": 2.1633,
      "step": 2200
    },
    {
      "epoch": 0.33988473474213093,
      "grad_norm": 0.995663583278656,
      "learning_rate": 4.660263041229496e-05,
      "loss": 2.1386,
      "step": 2300
    },
    {
      "epoch": 0.35466233190483226,
      "grad_norm": 0.9378241300582886,
      "learning_rate": 4.645485444066795e-05,
      "loss": 2.1289,
      "step": 2400
    },
    {
      "epoch": 0.36943992906753365,
      "grad_norm": 1.0964031219482422,
      "learning_rate": 4.630707846904094e-05,
      "loss": 2.0908,
      "step": 2500
    },
    {
      "epoch": 0.384217526230235,
      "grad_norm": 3.550722599029541,
      "learning_rate": 4.615930249741393e-05,
      "loss": 2.051,
      "step": 2600
    },
    {
      "epoch": 0.3989951233929363,
      "grad_norm": 0.9874096512794495,
      "learning_rate": 4.601152652578691e-05,
      "loss": 1.9575,
      "step": 2700
    },
    {
      "epoch": 0.41377272055563763,
      "grad_norm": 1.0265294313430786,
      "learning_rate": 4.58637505541599e-05,
      "loss": 2.0166,
      "step": 2800
    },
    {
      "epoch": 0.428550317718339,
      "grad_norm": 88.97537231445312,
      "learning_rate": 4.5715974582532886e-05,
      "loss": 1.936,
      "step": 2900
    },
    {
      "epoch": 0.44332791488104034,
      "grad_norm": 1.0658111572265625,
      "learning_rate": 4.5568198610905874e-05,
      "loss": 1.9448,
      "step": 3000
    },
    {
      "epoch": 0.45810551204374167,
      "grad_norm": 1.5389461517333984,
      "learning_rate": 4.5420422639278855e-05,
      "loss": 1.8852,
      "step": 3100
    },
    {
      "epoch": 0.47288310920644305,
      "grad_norm": 1.1327712535858154,
      "learning_rate": 4.5272646667651844e-05,
      "loss": 1.8717,
      "step": 3200
    },
    {
      "epoch": 0.4876607063691444,
      "grad_norm": 1.244969129562378,
      "learning_rate": 4.512487069602483e-05,
      "loss": 1.8838,
      "step": 3300
    },
    {
      "epoch": 0.5024383035318457,
      "grad_norm": 1.2410794496536255,
      "learning_rate": 4.497709472439782e-05,
      "loss": 1.8438,
      "step": 3400
    },
    {
      "epoch": 0.517215900694547,
      "grad_norm": 1.1122093200683594,
      "learning_rate": 4.48293187527708e-05,
      "loss": 1.8194,
      "step": 3500
    },
    {
      "epoch": 0.5319934978572484,
      "grad_norm": 1.380450963973999,
      "learning_rate": 4.468154278114379e-05,
      "loss": 1.7978,
      "step": 3600
    },
    {
      "epoch": 0.5467710950199498,
      "grad_norm": 1.3253835439682007,
      "learning_rate": 4.453376680951678e-05,
      "loss": 1.7614,
      "step": 3700
    },
    {
      "epoch": 0.5615486921826511,
      "grad_norm": 1.0835174322128296,
      "learning_rate": 4.438599083788976e-05,
      "loss": 1.719,
      "step": 3800
    },
    {
      "epoch": 0.5763262893453525,
      "grad_norm": 1.2348461151123047,
      "learning_rate": 4.423821486626275e-05,
      "loss": 1.7528,
      "step": 3900
    },
    {
      "epoch": 0.5911038865080538,
      "grad_norm": 1.2625148296356201,
      "learning_rate": 4.4090438894635736e-05,
      "loss": 1.7143,
      "step": 4000
    },
    {
      "epoch": 0.5911038865080538,
      "eval_loss": 1.461576223373413,
      "eval_runtime": 70.7398,
      "eval_samples_per_second": 170.06,
      "eval_steps_per_second": 21.261,
      "step": 4000
    },
    {
      "epoch": 0.6058814836707551,
      "grad_norm": 1.103553056716919,
      "learning_rate": 4.394266292300872e-05,
      "loss": 1.7156,
      "step": 4100
    },
    {
      "epoch": 0.6206590808334564,
      "grad_norm": 1.386269211769104,
      "learning_rate": 4.3794886951381706e-05,
      "loss": 1.6827,
      "step": 4200
    },
    {
      "epoch": 0.6354366779961578,
      "grad_norm": 1.191638469696045,
      "learning_rate": 4.3647110979754694e-05,
      "loss": 1.7089,
      "step": 4300
    },
    {
      "epoch": 0.6502142751588592,
      "grad_norm": 1.0451081991195679,
      "learning_rate": 4.3499335008127675e-05,
      "loss": 1.6487,
      "step": 4400
    },
    {
      "epoch": 0.6649918723215605,
      "grad_norm": 1.3918300867080688,
      "learning_rate": 4.3351559036500664e-05,
      "loss": 1.6321,
      "step": 4500
    },
    {
      "epoch": 0.6797694694842619,
      "grad_norm": 1.3129271268844604,
      "learning_rate": 4.320378306487365e-05,
      "loss": 1.6613,
      "step": 4600
    },
    {
      "epoch": 0.6945470666469632,
      "grad_norm": 1.4445844888687134,
      "learning_rate": 4.305600709324664e-05,
      "loss": 1.6095,
      "step": 4700
    },
    {
      "epoch": 0.7093246638096645,
      "grad_norm": 1.1146917343139648,
      "learning_rate": 4.290823112161962e-05,
      "loss": 1.6146,
      "step": 4800
    },
    {
      "epoch": 0.7241022609723659,
      "grad_norm": 1.4585609436035156,
      "learning_rate": 4.276045514999261e-05,
      "loss": 1.5163,
      "step": 4900
    },
    {
      "epoch": 0.7388798581350673,
      "grad_norm": 1.2868297100067139,
      "learning_rate": 4.26126791783656e-05,
      "loss": 1.6615,
      "step": 5000
    },
    {
      "epoch": 0.7536574552977686,
      "grad_norm": 1.7625128030776978,
      "learning_rate": 4.2464903206738586e-05,
      "loss": 1.5613,
      "step": 5100
    },
    {
      "epoch": 0.76843505246047,
      "grad_norm": 1.2156234979629517,
      "learning_rate": 4.2317127235111574e-05,
      "loss": 1.5347,
      "step": 5200
    },
    {
      "epoch": 0.7832126496231713,
      "grad_norm": 1.3928102254867554,
      "learning_rate": 4.2169351263484556e-05,
      "loss": 1.5004,
      "step": 5300
    },
    {
      "epoch": 0.7979902467858726,
      "grad_norm": 1.3313205242156982,
      "learning_rate": 4.2021575291857544e-05,
      "loss": 1.5161,
      "step": 5400
    },
    {
      "epoch": 0.8127678439485739,
      "grad_norm": 1.3108397722244263,
      "learning_rate": 4.187379932023053e-05,
      "loss": 1.4995,
      "step": 5500
    },
    {
      "epoch": 0.8275454411112753,
      "grad_norm": 1.3346328735351562,
      "learning_rate": 4.172602334860352e-05,
      "loss": 1.4964,
      "step": 5600
    },
    {
      "epoch": 0.8423230382739767,
      "grad_norm": 1.7497504949569702,
      "learning_rate": 4.15782473769765e-05,
      "loss": 1.4579,
      "step": 5700
    },
    {
      "epoch": 0.857100635436678,
      "grad_norm": 1.281504511833191,
      "learning_rate": 4.143047140534949e-05,
      "loss": 1.4968,
      "step": 5800
    },
    {
      "epoch": 0.8718782325993794,
      "grad_norm": 1.4867252111434937,
      "learning_rate": 4.128269543372248e-05,
      "loss": 1.4049,
      "step": 5900
    },
    {
      "epoch": 0.8866558297620807,
      "grad_norm": 1.1593858003616333,
      "learning_rate": 4.113491946209547e-05,
      "loss": 1.4669,
      "step": 6000
    },
    {
      "epoch": 0.8866558297620807,
      "eval_loss": 1.193649172782898,
      "eval_runtime": 70.8045,
      "eval_samples_per_second": 169.904,
      "eval_steps_per_second": 21.242,
      "step": 6000
    },
    {
      "epoch": 0.901433426924782,
      "grad_norm": 1.2781939506530762,
      "learning_rate": 4.098714349046845e-05,
      "loss": 1.4456,
      "step": 6100
    },
    {
      "epoch": 0.9162110240874833,
      "grad_norm": 1.5275073051452637,
      "learning_rate": 4.0839367518841437e-05,
      "loss": 1.4644,
      "step": 6200
    },
    {
      "epoch": 0.9309886212501847,
      "grad_norm": 1.6733344793319702,
      "learning_rate": 4.0691591547214425e-05,
      "loss": 1.4273,
      "step": 6300
    },
    {
      "epoch": 0.9457662184128861,
      "grad_norm": 1.201153039932251,
      "learning_rate": 4.054381557558741e-05,
      "loss": 1.4387,
      "step": 6400
    },
    {
      "epoch": 0.9605438155755874,
      "grad_norm": 1.7189440727233887,
      "learning_rate": 4.03960396039604e-05,
      "loss": 1.3862,
      "step": 6500
    },
    {
      "epoch": 0.9753214127382888,
      "grad_norm": 1.3072876930236816,
      "learning_rate": 4.024826363233338e-05,
      "loss": 1.4182,
      "step": 6600
    },
    {
      "epoch": 0.9900990099009901,
      "grad_norm": 1.2943512201309204,
      "learning_rate": 4.010048766070637e-05,
      "loss": 1.3931,
      "step": 6700
    },
    {
      "epoch": 1.0048766070636914,
      "grad_norm": 1.1267354488372803,
      "learning_rate": 3.995271168907936e-05,
      "loss": 1.434,
      "step": 6800
    },
    {
      "epoch": 1.0196542042263927,
      "grad_norm": 1.4274924993515015,
      "learning_rate": 3.980493571745235e-05,
      "loss": 1.3615,
      "step": 6900
    },
    {
      "epoch": 1.034431801389094,
      "grad_norm": 1.0830458402633667,
      "learning_rate": 3.965715974582533e-05,
      "loss": 1.3121,
      "step": 7000
    },
    {
      "epoch": 1.0492093985517954,
      "grad_norm": 1.2378909587860107,
      "learning_rate": 3.950938377419832e-05,
      "loss": 1.3517,
      "step": 7100
    },
    {
      "epoch": 1.0639869957144967,
      "grad_norm": 1.6271368265151978,
      "learning_rate": 3.9361607802571305e-05,
      "loss": 1.3323,
      "step": 7200
    },
    {
      "epoch": 1.078764592877198,
      "grad_norm": 1.4643564224243164,
      "learning_rate": 3.9213831830944294e-05,
      "loss": 1.3503,
      "step": 7300
    },
    {
      "epoch": 1.0935421900398996,
      "grad_norm": 9.466465950012207,
      "learning_rate": 3.9066055859317275e-05,
      "loss": 1.2977,
      "step": 7400
    },
    {
      "epoch": 1.108319787202601,
      "grad_norm": 1.4162508249282837,
      "learning_rate": 3.891827988769026e-05,
      "loss": 1.2596,
      "step": 7500
    },
    {
      "epoch": 1.1230973843653023,
      "grad_norm": 1.5725083351135254,
      "learning_rate": 3.877050391606325e-05,
      "loss": 1.2933,
      "step": 7600
    },
    {
      "epoch": 1.1378749815280036,
      "grad_norm": 1.1986124515533447,
      "learning_rate": 3.862272794443624e-05,
      "loss": 1.2868,
      "step": 7700
    },
    {
      "epoch": 1.152652578690705,
      "grad_norm": 1.5278902053833008,
      "learning_rate": 3.847495197280923e-05,
      "loss": 1.2938,
      "step": 7800
    },
    {
      "epoch": 1.1674301758534062,
      "grad_norm": 1.3617432117462158,
      "learning_rate": 3.832717600118221e-05,
      "loss": 1.3098,
      "step": 7900
    },
    {
      "epoch": 1.1822077730161076,
      "grad_norm": 1.4569262266159058,
      "learning_rate": 3.81794000295552e-05,
      "loss": 1.2492,
      "step": 8000
    },
    {
      "epoch": 1.1822077730161076,
      "eval_loss": 1.0601216554641724,
      "eval_runtime": 71.0527,
      "eval_samples_per_second": 169.311,
      "eval_steps_per_second": 21.167,
      "step": 8000
    },
    {
      "epoch": 1.196985370178809,
      "grad_norm": 1.3850239515304565,
      "learning_rate": 3.8031624057928186e-05,
      "loss": 1.3043,
      "step": 8100
    },
    {
      "epoch": 1.2117629673415102,
      "grad_norm": 1.5370728969573975,
      "learning_rate": 3.7883848086301174e-05,
      "loss": 1.2635,
      "step": 8200
    },
    {
      "epoch": 1.2265405645042116,
      "grad_norm": 1.1752023696899414,
      "learning_rate": 3.7736072114674156e-05,
      "loss": 1.2529,
      "step": 8300
    },
    {
      "epoch": 1.241318161666913,
      "grad_norm": 1.851589322090149,
      "learning_rate": 3.7588296143047144e-05,
      "loss": 1.195,
      "step": 8400
    },
    {
      "epoch": 1.2560957588296144,
      "grad_norm": 1.3987637758255005,
      "learning_rate": 3.744052017142013e-05,
      "loss": 1.2393,
      "step": 8500
    },
    {
      "epoch": 1.2708733559923155,
      "grad_norm": 1.1146167516708374,
      "learning_rate": 3.7292744199793114e-05,
      "loss": 1.212,
      "step": 8600
    },
    {
      "epoch": 1.285650953155017,
      "grad_norm": 1.4355353116989136,
      "learning_rate": 3.71449682281661e-05,
      "loss": 1.2108,
      "step": 8700
    },
    {
      "epoch": 1.3004285503177182,
      "grad_norm": 1.467399001121521,
      "learning_rate": 3.699719225653909e-05,
      "loss": 1.2399,
      "step": 8800
    },
    {
      "epoch": 1.3152061474804198,
      "grad_norm": 1.0188469886779785,
      "learning_rate": 3.684941628491207e-05,
      "loss": 1.2407,
      "step": 8900
    },
    {
      "epoch": 1.329983744643121,
      "grad_norm": 1.4437298774719238,
      "learning_rate": 3.670164031328506e-05,
      "loss": 1.2332,
      "step": 9000
    },
    {
      "epoch": 1.3447613418058224,
      "grad_norm": 1.1326117515563965,
      "learning_rate": 3.655386434165805e-05,
      "loss": 1.1724,
      "step": 9100
    },
    {
      "epoch": 1.3595389389685237,
      "grad_norm": 0.9320288300514221,
      "learning_rate": 3.640608837003103e-05,
      "loss": 1.2177,
      "step": 9200
    },
    {
      "epoch": 1.374316536131225,
      "grad_norm": 1.54095458984375,
      "learning_rate": 3.625831239840402e-05,
      "loss": 1.1728,
      "step": 9300
    },
    {
      "epoch": 1.3890941332939264,
      "grad_norm": 1.1796875,
      "learning_rate": 3.6110536426777006e-05,
      "loss": 1.2281,
      "step": 9400
    },
    {
      "epoch": 1.4038717304566277,
      "grad_norm": 1.3919486999511719,
      "learning_rate": 3.5962760455149994e-05,
      "loss": 1.195,
      "step": 9500
    },
    {
      "epoch": 1.418649327619329,
      "grad_norm": 1.8513600826263428,
      "learning_rate": 3.5814984483522976e-05,
      "loss": 1.1802,
      "step": 9600
    },
    {
      "epoch": 1.4334269247820304,
      "grad_norm": 1.1938663721084595,
      "learning_rate": 3.5667208511895964e-05,
      "loss": 1.1436,
      "step": 9700
    },
    {
      "epoch": 1.4482045219447317,
      "grad_norm": 1.204258918762207,
      "learning_rate": 3.551943254026895e-05,
      "loss": 1.1959,
      "step": 9800
    },
    {
      "epoch": 1.462982119107433,
      "grad_norm": 1.3835303783416748,
      "learning_rate": 3.537165656864194e-05,
      "loss": 1.1877,
      "step": 9900
    },
    {
      "epoch": 1.4777597162701346,
      "grad_norm": 1.366773247718811,
      "learning_rate": 3.522388059701492e-05,
      "loss": 1.1619,
      "step": 10000
    },
    {
      "epoch": 1.4777597162701346,
      "eval_loss": 0.9728577136993408,
      "eval_runtime": 71.0265,
      "eval_samples_per_second": 169.373,
      "eval_steps_per_second": 21.175,
      "step": 10000
    },
    {
      "epoch": 1.4925373134328357,
      "grad_norm": 1.0084329843521118,
      "learning_rate": 3.507610462538791e-05,
      "loss": 1.1426,
      "step": 10100
    },
    {
      "epoch": 1.5073149105955372,
      "grad_norm": 1.8717926740646362,
      "learning_rate": 3.49283286537609e-05,
      "loss": 1.1997,
      "step": 10200
    },
    {
      "epoch": 1.5220925077582383,
      "grad_norm": 1.3973884582519531,
      "learning_rate": 3.478055268213389e-05,
      "loss": 1.1338,
      "step": 10300
    },
    {
      "epoch": 1.53687010492094,
      "grad_norm": 1.03579580783844,
      "learning_rate": 3.4632776710506875e-05,
      "loss": 1.1496,
      "step": 10400
    },
    {
      "epoch": 1.5516477020836412,
      "grad_norm": 1.5874742269515991,
      "learning_rate": 3.4485000738879856e-05,
      "loss": 1.1491,
      "step": 10500
    },
    {
      "epoch": 1.5664252992463426,
      "grad_norm": 1.0266096591949463,
      "learning_rate": 3.4337224767252845e-05,
      "loss": 1.1211,
      "step": 10600
    },
    {
      "epoch": 1.5812028964090439,
      "grad_norm": 1.6895384788513184,
      "learning_rate": 3.418944879562583e-05,
      "loss": 1.171,
      "step": 10700
    },
    {
      "epoch": 1.5959804935717452,
      "grad_norm": 2.3646273612976074,
      "learning_rate": 3.404167282399882e-05,
      "loss": 1.1517,
      "step": 10800
    },
    {
      "epoch": 1.6107580907344465,
      "grad_norm": 1.2133712768554688,
      "learning_rate": 3.38938968523718e-05,
      "loss": 1.1455,
      "step": 10900
    },
    {
      "epoch": 1.6255356878971479,
      "grad_norm": 1.5783271789550781,
      "learning_rate": 3.374612088074479e-05,
      "loss": 1.1282,
      "step": 11000
    },
    {
      "epoch": 1.6403132850598494,
      "grad_norm": 1.0969607830047607,
      "learning_rate": 3.359834490911778e-05,
      "loss": 1.1415,
      "step": 11100
    },
    {
      "epoch": 1.6550908822225505,
      "grad_norm": 1.531530737876892,
      "learning_rate": 3.345056893749077e-05,
      "loss": 1.1083,
      "step": 11200
    },
    {
      "epoch": 1.669868479385252,
      "grad_norm": 1.0774450302124023,
      "learning_rate": 3.330279296586375e-05,
      "loss": 1.1307,
      "step": 11300
    },
    {
      "epoch": 1.6846460765479532,
      "grad_norm": 1.277205228805542,
      "learning_rate": 3.315501699423674e-05,
      "loss": 1.1082,
      "step": 11400
    },
    {
      "epoch": 1.6994236737106547,
      "grad_norm": 1.320263385772705,
      "learning_rate": 3.3007241022609725e-05,
      "loss": 1.1217,
      "step": 11500
    },
    {
      "epoch": 1.7142012708733558,
      "grad_norm": 1.0651326179504395,
      "learning_rate": 3.2859465050982714e-05,
      "loss": 1.1209,
      "step": 11600
    },
    {
      "epoch": 1.7289788680360574,
      "grad_norm": 1.3168293237686157,
      "learning_rate": 3.27116890793557e-05,
      "loss": 1.1103,
      "step": 11700
    },
    {
      "epoch": 1.7437564651987587,
      "grad_norm": 1.50478196144104,
      "learning_rate": 3.256391310772868e-05,
      "loss": 1.1239,
      "step": 11800
    },
    {
      "epoch": 1.75853406236146,
      "grad_norm": 1.7181370258331299,
      "learning_rate": 3.241613713610167e-05,
      "loss": 1.0776,
      "step": 11900
    },
    {
      "epoch": 1.7733116595241614,
      "grad_norm": 1.4376132488250732,
      "learning_rate": 3.226836116447466e-05,
      "loss": 1.1046,
      "step": 12000
    },
    {
      "epoch": 1.7733116595241614,
      "eval_loss": 0.9152714610099792,
      "eval_runtime": 70.6254,
      "eval_samples_per_second": 170.335,
      "eval_steps_per_second": 21.295,
      "step": 12000
    },
    {
      "epoch": 1.7880892566868627,
      "grad_norm": 1.1821390390396118,
      "learning_rate": 3.212058519284765e-05,
      "loss": 1.1292,
      "step": 12100
    },
    {
      "epoch": 1.802866853849564,
      "grad_norm": 1.1780580282211304,
      "learning_rate": 3.197280922122063e-05,
      "loss": 1.1209,
      "step": 12200
    },
    {
      "epoch": 1.8176444510122653,
      "grad_norm": 1.2715905904769897,
      "learning_rate": 3.182503324959362e-05,
      "loss": 1.0903,
      "step": 12300
    },
    {
      "epoch": 1.832422048174967,
      "grad_norm": 1.29457426071167,
      "learning_rate": 3.1677257277966606e-05,
      "loss": 1.0903,
      "step": 12400
    },
    {
      "epoch": 1.847199645337668,
      "grad_norm": 0.8546916842460632,
      "learning_rate": 3.1529481306339594e-05,
      "loss": 1.0815,
      "step": 12500
    },
    {
      "epoch": 1.8619772425003696,
      "grad_norm": 1.10230553150177,
      "learning_rate": 3.138170533471258e-05,
      "loss": 1.0586,
      "step": 12600
    },
    {
      "epoch": 1.8767548396630707,
      "grad_norm": 1.223464846611023,
      "learning_rate": 3.1233929363085564e-05,
      "loss": 1.0674,
      "step": 12700
    },
    {
      "epoch": 1.8915324368257722,
      "grad_norm": 1.2744052410125732,
      "learning_rate": 3.108615339145855e-05,
      "loss": 1.0536,
      "step": 12800
    },
    {
      "epoch": 1.9063100339884733,
      "grad_norm": 1.7029958963394165,
      "learning_rate": 3.093837741983154e-05,
      "loss": 1.0576,
      "step": 12900
    },
    {
      "epoch": 1.9210876311511749,
      "grad_norm": 1.4204658269882202,
      "learning_rate": 3.079060144820453e-05,
      "loss": 1.0911,
      "step": 13000
    },
    {
      "epoch": 1.9358652283138762,
      "grad_norm": 1.249884009361267,
      "learning_rate": 3.064282547657751e-05,
      "loss": 1.0863,
      "step": 13100
    },
    {
      "epoch": 1.9506428254765775,
      "grad_norm": 1.0467064380645752,
      "learning_rate": 3.0495049504950495e-05,
      "loss": 1.0745,
      "step": 13200
    },
    {
      "epoch": 1.9654204226392789,
      "grad_norm": 1.369687557220459,
      "learning_rate": 3.0347273533323483e-05,
      "loss": 1.1271,
      "step": 13300
    },
    {
      "epoch": 1.9801980198019802,
      "grad_norm": 1.3050718307495117,
      "learning_rate": 3.019949756169647e-05,
      "loss": 1.098,
      "step": 13400
    },
    {
      "epoch": 1.9949756169646815,
      "grad_norm": 1.3629289865493774,
      "learning_rate": 3.0051721590069453e-05,
      "loss": 1.057,
      "step": 13500
    },
    {
      "epoch": 2.009753214127383,
      "grad_norm": 1.4681482315063477,
      "learning_rate": 2.990394561844244e-05,
      "loss": 1.0607,
      "step": 13600
    },
    {
      "epoch": 2.0245308112900844,
      "grad_norm": 1.3994752168655396,
      "learning_rate": 2.975616964681543e-05,
      "loss": 0.9972,
      "step": 13700
    },
    {
      "epoch": 2.0393084084527855,
      "grad_norm": 1.2010782957077026,
      "learning_rate": 2.9608393675188418e-05,
      "loss": 1.0397,
      "step": 13800
    },
    {
      "epoch": 2.054086005615487,
      "grad_norm": 2.2240302562713623,
      "learning_rate": 2.9460617703561406e-05,
      "loss": 1.0643,
      "step": 13900
    },
    {
      "epoch": 2.068863602778188,
      "grad_norm": 1.2611430883407593,
      "learning_rate": 2.9312841731934387e-05,
      "loss": 1.0247,
      "step": 14000
    },
    {
      "epoch": 2.068863602778188,
      "eval_loss": 0.8797813653945923,
      "eval_runtime": 70.7575,
      "eval_samples_per_second": 170.017,
      "eval_steps_per_second": 21.256,
      "step": 14000
    },
    {
      "epoch": 2.0836411999408897,
      "grad_norm": 1.2423484325408936,
      "learning_rate": 2.9165065760307376e-05,
      "loss": 1.0531,
      "step": 14100
    },
    {
      "epoch": 2.098418797103591,
      "grad_norm": 1.4807305335998535,
      "learning_rate": 2.9017289788680364e-05,
      "loss": 0.9997,
      "step": 14200
    },
    {
      "epoch": 2.1131963942662924,
      "grad_norm": 1.4190956354141235,
      "learning_rate": 2.8869513817053352e-05,
      "loss": 1.0416,
      "step": 14300
    },
    {
      "epoch": 2.1279739914289935,
      "grad_norm": 1.106839895248413,
      "learning_rate": 2.8721737845426333e-05,
      "loss": 1.0039,
      "step": 14400
    },
    {
      "epoch": 2.142751588591695,
      "grad_norm": 1.515112042427063,
      "learning_rate": 2.8573961873799322e-05,
      "loss": 1.0171,
      "step": 14500
    },
    {
      "epoch": 2.157529185754396,
      "grad_norm": 1.2940447330474854,
      "learning_rate": 2.842618590217231e-05,
      "loss": 1.0319,
      "step": 14600
    },
    {
      "epoch": 2.1723067829170977,
      "grad_norm": 1.397547721862793,
      "learning_rate": 2.8278409930545298e-05,
      "loss": 1.0315,
      "step": 14700
    },
    {
      "epoch": 2.187084380079799,
      "grad_norm": 1.037704348564148,
      "learning_rate": 2.813063395891828e-05,
      "loss": 1.0332,
      "step": 14800
    },
    {
      "epoch": 2.2018619772425003,
      "grad_norm": 2.316878318786621,
      "learning_rate": 2.7982857987291268e-05,
      "loss": 1.0043,
      "step": 14900
    },
    {
      "epoch": 2.216639574405202,
      "grad_norm": 1.2353575229644775,
      "learning_rate": 2.7835082015664253e-05,
      "loss": 1.0084,
      "step": 15000
    },
    {
      "epoch": 2.231417171567903,
      "grad_norm": 1.2833298444747925,
      "learning_rate": 2.768730604403724e-05,
      "loss": 1.0138,
      "step": 15100
    },
    {
      "epoch": 2.2461947687306045,
      "grad_norm": 1.3133132457733154,
      "learning_rate": 2.753953007241023e-05,
      "loss": 0.9717,
      "step": 15200
    },
    {
      "epoch": 2.2609723658933056,
      "grad_norm": 1.2356092929840088,
      "learning_rate": 2.739175410078321e-05,
      "loss": 1.0261,
      "step": 15300
    },
    {
      "epoch": 2.275749963056007,
      "grad_norm": 1.3204020261764526,
      "learning_rate": 2.72439781291562e-05,
      "loss": 1.0319,
      "step": 15400
    },
    {
      "epoch": 2.2905275602187083,
      "grad_norm": 1.7673349380493164,
      "learning_rate": 2.7096202157529187e-05,
      "loss": 1.0355,
      "step": 15500
    },
    {
      "epoch": 2.30530515738141,
      "grad_norm": 1.4049359560012817,
      "learning_rate": 2.6948426185902175e-05,
      "loss": 1.0134,
      "step": 15600
    },
    {
      "epoch": 2.320082754544111,
      "grad_norm": 1.3272442817687988,
      "learning_rate": 2.6800650214275157e-05,
      "loss": 1.0145,
      "step": 15700
    },
    {
      "epoch": 2.3348603517068125,
      "grad_norm": 0.9733226895332336,
      "learning_rate": 2.6652874242648145e-05,
      "loss": 1.0202,
      "step": 15800
    },
    {
      "epoch": 2.349637948869514,
      "grad_norm": 1.3170469999313354,
      "learning_rate": 2.6505098271021133e-05,
      "loss": 0.9737,
      "step": 15900
    },
    {
      "epoch": 2.364415546032215,
      "grad_norm": 1.1015355587005615,
      "learning_rate": 2.635732229939412e-05,
      "loss": 0.9801,
      "step": 16000
    },
    {
      "epoch": 2.364415546032215,
      "eval_loss": 0.8534833192825317,
      "eval_runtime": 70.6912,
      "eval_samples_per_second": 170.177,
      "eval_steps_per_second": 21.276,
      "step": 16000
    },
    {
      "epoch": 2.3791931431949163,
      "grad_norm": 1.2174428701400757,
      "learning_rate": 2.6209546327767103e-05,
      "loss": 1.0037,
      "step": 16100
    },
    {
      "epoch": 2.393970740357618,
      "grad_norm": 1.4061589241027832,
      "learning_rate": 2.606177035614009e-05,
      "loss": 0.9937,
      "step": 16200
    },
    {
      "epoch": 2.4087483375203194,
      "grad_norm": 1.1149306297302246,
      "learning_rate": 2.591399438451308e-05,
      "loss": 1.0091,
      "step": 16300
    },
    {
      "epoch": 2.4235259346830205,
      "grad_norm": 1.0211488008499146,
      "learning_rate": 2.5766218412886068e-05,
      "loss": 0.9884,
      "step": 16400
    },
    {
      "epoch": 2.438303531845722,
      "grad_norm": 0.998824417591095,
      "learning_rate": 2.5618442441259056e-05,
      "loss": 0.9785,
      "step": 16500
    },
    {
      "epoch": 2.453081129008423,
      "grad_norm": 1.4910155534744263,
      "learning_rate": 2.5470666469632038e-05,
      "loss": 1.0086,
      "step": 16600
    },
    {
      "epoch": 2.4678587261711247,
      "grad_norm": 1.2589030265808105,
      "learning_rate": 2.5322890498005026e-05,
      "loss": 0.9658,
      "step": 16700
    },
    {
      "epoch": 2.482636323333826,
      "grad_norm": 1.1207524538040161,
      "learning_rate": 2.5175114526378014e-05,
      "loss": 0.9889,
      "step": 16800
    },
    {
      "epoch": 2.4974139204965273,
      "grad_norm": 1.4582858085632324,
      "learning_rate": 2.5027338554751002e-05,
      "loss": 0.9448,
      "step": 16900
    },
    {
      "epoch": 2.512191517659229,
      "grad_norm": 1.2781704664230347,
      "learning_rate": 2.4879562583123987e-05,
      "loss": 0.9572,
      "step": 17000
    },
    {
      "epoch": 2.52696911482193,
      "grad_norm": 0.9081103205680847,
      "learning_rate": 2.4731786611496972e-05,
      "loss": 1.0042,
      "step": 17100
    },
    {
      "epoch": 2.541746711984631,
      "grad_norm": 29.751781463623047,
      "learning_rate": 2.458401063986996e-05,
      "loss": 0.9827,
      "step": 17200
    },
    {
      "epoch": 2.5565243091473326,
      "grad_norm": 1.0755784511566162,
      "learning_rate": 2.4436234668242945e-05,
      "loss": 0.966,
      "step": 17300
    },
    {
      "epoch": 2.571301906310034,
      "grad_norm": 1.1218608617782593,
      "learning_rate": 2.4288458696615933e-05,
      "loss": 0.9623,
      "step": 17400
    },
    {
      "epoch": 2.5860795034727353,
      "grad_norm": 1.351563811302185,
      "learning_rate": 2.4140682724988918e-05,
      "loss": 1.0305,
      "step": 17500
    },
    {
      "epoch": 2.6008571006354364,
      "grad_norm": 1.1850550174713135,
      "learning_rate": 2.3992906753361903e-05,
      "loss": 0.9859,
      "step": 17600
    },
    {
      "epoch": 2.615634697798138,
      "grad_norm": 1.4026156663894653,
      "learning_rate": 2.384513078173489e-05,
      "loss": 0.9703,
      "step": 17700
    },
    {
      "epoch": 2.6304122949608395,
      "grad_norm": 1.2604386806488037,
      "learning_rate": 2.3697354810107876e-05,
      "loss": 0.9527,
      "step": 17800
    },
    {
      "epoch": 2.6451898921235406,
      "grad_norm": 1.216314673423767,
      "learning_rate": 2.3549578838480864e-05,
      "loss": 0.9728,
      "step": 17900
    },
    {
      "epoch": 2.659967489286242,
      "grad_norm": 1.62935471534729,
      "learning_rate": 2.340180286685385e-05,
      "loss": 0.9515,
      "step": 18000
    },
    {
      "epoch": 2.659967489286242,
      "eval_loss": 0.8324642777442932,
      "eval_runtime": 70.7553,
      "eval_samples_per_second": 170.023,
      "eval_steps_per_second": 21.256,
      "step": 18000
    },
    {
      "epoch": 2.6747450864489433,
      "grad_norm": 1.2921124696731567,
      "learning_rate": 2.3254026895226837e-05,
      "loss": 0.9956,
      "step": 18100
    },
    {
      "epoch": 2.689522683611645,
      "grad_norm": 1.0383400917053223,
      "learning_rate": 2.3106250923599822e-05,
      "loss": 1.0111,
      "step": 18200
    },
    {
      "epoch": 2.704300280774346,
      "grad_norm": 0.8253582715988159,
      "learning_rate": 2.295847495197281e-05,
      "loss": 0.9594,
      "step": 18300
    },
    {
      "epoch": 2.7190778779370475,
      "grad_norm": 1.571354627609253,
      "learning_rate": 2.2810698980345795e-05,
      "loss": 0.9457,
      "step": 18400
    },
    {
      "epoch": 2.733855475099749,
      "grad_norm": 1.5494909286499023,
      "learning_rate": 2.2662923008718784e-05,
      "loss": 0.9716,
      "step": 18500
    },
    {
      "epoch": 2.74863307226245,
      "grad_norm": 0.9532424807548523,
      "learning_rate": 2.251514703709177e-05,
      "loss": 0.9601,
      "step": 18600
    },
    {
      "epoch": 2.7634106694251512,
      "grad_norm": 1.032069206237793,
      "learning_rate": 2.2367371065464757e-05,
      "loss": 0.9426,
      "step": 18700
    },
    {
      "epoch": 2.778188266587853,
      "grad_norm": 0.9291874766349792,
      "learning_rate": 2.221959509383774e-05,
      "loss": 0.9634,
      "step": 18800
    },
    {
      "epoch": 2.7929658637505543,
      "grad_norm": 1.0161612033843994,
      "learning_rate": 2.207181912221073e-05,
      "loss": 1.0249,
      "step": 18900
    },
    {
      "epoch": 2.8077434609132554,
      "grad_norm": 1.0882186889648438,
      "learning_rate": 2.1924043150583718e-05,
      "loss": 0.9368,
      "step": 19000
    },
    {
      "epoch": 2.822521058075957,
      "grad_norm": 1.0220930576324463,
      "learning_rate": 2.1776267178956703e-05,
      "loss": 0.9906,
      "step": 19100
    },
    {
      "epoch": 2.837298655238658,
      "grad_norm": 1.3424080610275269,
      "learning_rate": 2.162849120732969e-05,
      "loss": 0.945,
      "step": 19200
    },
    {
      "epoch": 2.8520762524013596,
      "grad_norm": 1.091536283493042,
      "learning_rate": 2.1480715235702676e-05,
      "loss": 0.9864,
      "step": 19300
    },
    {
      "epoch": 2.8668538495640608,
      "grad_norm": 0.9947521090507507,
      "learning_rate": 2.1332939264075664e-05,
      "loss": 0.9086,
      "step": 19400
    },
    {
      "epoch": 2.8816314467267623,
      "grad_norm": 1.5116474628448486,
      "learning_rate": 2.118516329244865e-05,
      "loss": 0.9534,
      "step": 19500
    },
    {
      "epoch": 2.8964090438894634,
      "grad_norm": 1.4547115564346313,
      "learning_rate": 2.1037387320821637e-05,
      "loss": 0.9646,
      "step": 19600
    },
    {
      "epoch": 2.911186641052165,
      "grad_norm": 1.05988347530365,
      "learning_rate": 2.0889611349194622e-05,
      "loss": 0.9755,
      "step": 19700
    },
    {
      "epoch": 2.925964238214866,
      "grad_norm": 1.6021158695220947,
      "learning_rate": 2.074183537756761e-05,
      "loss": 0.9761,
      "step": 19800
    },
    {
      "epoch": 2.9407418353775676,
      "grad_norm": 1.364562749862671,
      "learning_rate": 2.0594059405940595e-05,
      "loss": 0.948,
      "step": 19900
    },
    {
      "epoch": 2.955519432540269,
      "grad_norm": 0.8201131820678711,
      "learning_rate": 2.044628343431358e-05,
      "loss": 0.9595,
      "step": 20000
    },
    {
      "epoch": 2.955519432540269,
      "eval_loss": 0.8181558847427368,
      "eval_runtime": 72.8951,
      "eval_samples_per_second": 165.032,
      "eval_steps_per_second": 20.632,
      "step": 20000
    },
    {
      "epoch": 2.9702970297029703,
      "grad_norm": 1.0890936851501465,
      "learning_rate": 2.029850746268657e-05,
      "loss": 0.9532,
      "step": 20100
    },
    {
      "epoch": 2.9850746268656714,
      "grad_norm": 1.3150925636291504,
      "learning_rate": 2.0150731491059553e-05,
      "loss": 0.9395,
      "step": 20200
    },
    {
      "epoch": 2.999852224028373,
      "grad_norm": 1.6762231588363647,
      "learning_rate": 2.000295551943254e-05,
      "loss": 0.9146,
      "step": 20300
    },
    {
      "epoch": 3.0146298211910745,
      "grad_norm": 0.8751043081283569,
      "learning_rate": 1.9855179547805526e-05,
      "loss": 0.9572,
      "step": 20400
    },
    {
      "epoch": 3.0294074183537756,
      "grad_norm": 1.3986773490905762,
      "learning_rate": 1.9707403576178515e-05,
      "loss": 0.9224,
      "step": 20500
    },
    {
      "epoch": 3.044185015516477,
      "grad_norm": 1.3026117086410522,
      "learning_rate": 1.95596276045515e-05,
      "loss": 0.9558,
      "step": 20600
    },
    {
      "epoch": 3.0589626126791782,
      "grad_norm": 13.18566608428955,
      "learning_rate": 1.9411851632924488e-05,
      "loss": 0.9301,
      "step": 20700
    },
    {
      "epoch": 3.07374020984188,
      "grad_norm": 1.3721495866775513,
      "learning_rate": 1.9264075661297473e-05,
      "loss": 0.9401,
      "step": 20800
    },
    {
      "epoch": 3.088517807004581,
      "grad_norm": 1.2136660814285278,
      "learning_rate": 1.911629968967046e-05,
      "loss": 0.9335,
      "step": 20900
    },
    {
      "epoch": 3.1032954041672824,
      "grad_norm": 1.1391736268997192,
      "learning_rate": 1.8968523718043446e-05,
      "loss": 0.9399,
      "step": 21000
    },
    {
      "epoch": 3.1180730013299836,
      "grad_norm": 1.326490044593811,
      "learning_rate": 1.8820747746416434e-05,
      "loss": 0.9129,
      "step": 21100
    },
    {
      "epoch": 3.132850598492685,
      "grad_norm": 1.1322174072265625,
      "learning_rate": 1.867297177478942e-05,
      "loss": 0.9113,
      "step": 21200
    },
    {
      "epoch": 3.147628195655386,
      "grad_norm": 1.0820308923721313,
      "learning_rate": 1.8525195803162407e-05,
      "loss": 0.9255,
      "step": 21300
    },
    {
      "epoch": 3.1624057928180878,
      "grad_norm": 1.1996362209320068,
      "learning_rate": 1.8377419831535395e-05,
      "loss": 0.916,
      "step": 21400
    },
    {
      "epoch": 3.1771833899807893,
      "grad_norm": 1.05464768409729,
      "learning_rate": 1.822964385990838e-05,
      "loss": 0.9148,
      "step": 21500
    },
    {
      "epoch": 3.1919609871434904,
      "grad_norm": 1.0148699283599854,
      "learning_rate": 1.8081867888281368e-05,
      "loss": 0.9297,
      "step": 21600
    },
    {
      "epoch": 3.206738584306192,
      "grad_norm": 1.0506612062454224,
      "learning_rate": 1.7934091916654353e-05,
      "loss": 0.9001,
      "step": 21700
    },
    {
      "epoch": 3.221516181468893,
      "grad_norm": 1.0729738473892212,
      "learning_rate": 1.778631594502734e-05,
      "loss": 0.8958,
      "step": 21800
    },
    {
      "epoch": 3.2362937786315946,
      "grad_norm": 0.7693794965744019,
      "learning_rate": 1.7638539973400326e-05,
      "loss": 0.8955,
      "step": 21900
    },
    {
      "epoch": 3.2510713757942957,
      "grad_norm": 1.059853434562683,
      "learning_rate": 1.7490764001773314e-05,
      "loss": 0.9245,
      "step": 22000
    },
    {
      "epoch": 3.2510713757942957,
      "eval_loss": 0.7990727424621582,
      "eval_runtime": 72.4923,
      "eval_samples_per_second": 165.949,
      "eval_steps_per_second": 20.747,
      "step": 22000
    },
    {
      "epoch": 3.2658489729569973,
      "grad_norm": 1.3082098960876465,
      "learning_rate": 1.73429880301463e-05,
      "loss": 0.9353,
      "step": 22100
    },
    {
      "epoch": 3.2806265701196984,
      "grad_norm": 0.8843540549278259,
      "learning_rate": 1.7195212058519288e-05,
      "loss": 0.915,
      "step": 22200
    },
    {
      "epoch": 3.2954041672824,
      "grad_norm": 1.2705564498901367,
      "learning_rate": 1.7047436086892272e-05,
      "loss": 0.9344,
      "step": 22300
    },
    {
      "epoch": 3.310181764445101,
      "grad_norm": 0.9974242448806763,
      "learning_rate": 1.6899660115265257e-05,
      "loss": 0.9285,
      "step": 22400
    },
    {
      "epoch": 3.3249593616078026,
      "grad_norm": 1.2020047903060913,
      "learning_rate": 1.6751884143638246e-05,
      "loss": 0.9197,
      "step": 22500
    },
    {
      "epoch": 3.339736958770504,
      "grad_norm": 1.1115676164627075,
      "learning_rate": 1.660410817201123e-05,
      "loss": 0.8854,
      "step": 22600
    },
    {
      "epoch": 3.3545145559332052,
      "grad_norm": 0.9195966720581055,
      "learning_rate": 1.645633220038422e-05,
      "loss": 0.9201,
      "step": 22700
    },
    {
      "epoch": 3.369292153095907,
      "grad_norm": 0.9262402057647705,
      "learning_rate": 1.6308556228757203e-05,
      "loss": 0.9478,
      "step": 22800
    },
    {
      "epoch": 3.384069750258608,
      "grad_norm": 1.6266942024230957,
      "learning_rate": 1.6160780257130192e-05,
      "loss": 0.9005,
      "step": 22900
    },
    {
      "epoch": 3.3988473474213095,
      "grad_norm": 1.027726650238037,
      "learning_rate": 1.6013004285503177e-05,
      "loss": 0.9086,
      "step": 23000
    },
    {
      "epoch": 3.4136249445840106,
      "grad_norm": 1.2202086448669434,
      "learning_rate": 1.5865228313876165e-05,
      "loss": 0.9212,
      "step": 23100
    },
    {
      "epoch": 3.428402541746712,
      "grad_norm": 1.0743848085403442,
      "learning_rate": 1.571745234224915e-05,
      "loss": 0.8856,
      "step": 23200
    },
    {
      "epoch": 3.443180138909413,
      "grad_norm": 1.0252935886383057,
      "learning_rate": 1.5569676370622138e-05,
      "loss": 0.8964,
      "step": 23300
    },
    {
      "epoch": 3.4579577360721148,
      "grad_norm": 1.1072449684143066,
      "learning_rate": 1.5421900398995123e-05,
      "loss": 0.9167,
      "step": 23400
    },
    {
      "epoch": 3.472735333234816,
      "grad_norm": 1.0513797998428345,
      "learning_rate": 1.527412442736811e-05,
      "loss": 0.92,
      "step": 23500
    },
    {
      "epoch": 3.4875129303975174,
      "grad_norm": 1.0739941596984863,
      "learning_rate": 1.5126348455741096e-05,
      "loss": 0.9041,
      "step": 23600
    },
    {
      "epoch": 3.502290527560219,
      "grad_norm": 1.0277771949768066,
      "learning_rate": 1.4978572484114084e-05,
      "loss": 0.9215,
      "step": 23700
    },
    {
      "epoch": 3.51706812472292,
      "grad_norm": 1.2446588277816772,
      "learning_rate": 1.4830796512487069e-05,
      "loss": 0.9163,
      "step": 23800
    },
    {
      "epoch": 3.531845721885621,
      "grad_norm": 1.251145362854004,
      "learning_rate": 1.4683020540860057e-05,
      "loss": 0.8586,
      "step": 23900
    },
    {
      "epoch": 3.5466233190483227,
      "grad_norm": 1.2295362949371338,
      "learning_rate": 1.4535244569233045e-05,
      "loss": 0.9164,
      "step": 24000
    },
    {
      "epoch": 3.5466233190483227,
      "eval_loss": 0.788703441619873,
      "eval_runtime": 72.4163,
      "eval_samples_per_second": 166.123,
      "eval_steps_per_second": 20.769,
      "step": 24000
    },
    {
      "epoch": 3.5614009162110243,
      "grad_norm": 0.8657997250556946,
      "learning_rate": 1.438746859760603e-05,
      "loss": 0.9019,
      "step": 24100
    },
    {
      "epoch": 3.5761785133737254,
      "grad_norm": 1.1538829803466797,
      "learning_rate": 1.4239692625979017e-05,
      "loss": 0.9452,
      "step": 24200
    },
    {
      "epoch": 3.590956110536427,
      "grad_norm": 0.8012064695358276,
      "learning_rate": 1.4091916654352003e-05,
      "loss": 0.9193,
      "step": 24300
    },
    {
      "epoch": 3.605733707699128,
      "grad_norm": 1.2305548191070557,
      "learning_rate": 1.394414068272499e-05,
      "loss": 0.8999,
      "step": 24400
    },
    {
      "epoch": 3.6205113048618296,
      "grad_norm": 1.4015393257141113,
      "learning_rate": 1.3796364711097975e-05,
      "loss": 0.8951,
      "step": 24500
    },
    {
      "epoch": 3.6352889020245307,
      "grad_norm": 1.1187210083007812,
      "learning_rate": 1.3648588739470963e-05,
      "loss": 0.9002,
      "step": 24600
    },
    {
      "epoch": 3.6500664991872322,
      "grad_norm": 1.2973973751068115,
      "learning_rate": 1.3500812767843948e-05,
      "loss": 0.8965,
      "step": 24700
    },
    {
      "epoch": 3.6648440963499334,
      "grad_norm": 1.0274326801300049,
      "learning_rate": 1.3353036796216936e-05,
      "loss": 0.9204,
      "step": 24800
    },
    {
      "epoch": 3.679621693512635,
      "grad_norm": 1.198315978050232,
      "learning_rate": 1.3205260824589921e-05,
      "loss": 0.9288,
      "step": 24900
    },
    {
      "epoch": 3.694399290675336,
      "grad_norm": 1.0276108980178833,
      "learning_rate": 1.305748485296291e-05,
      "loss": 0.8972,
      "step": 25000
    },
    {
      "epoch": 3.7091768878380376,
      "grad_norm": 0.9221938848495483,
      "learning_rate": 1.2909708881335894e-05,
      "loss": 0.8935,
      "step": 25100
    },
    {
      "epoch": 3.723954485000739,
      "grad_norm": 1.1824347972869873,
      "learning_rate": 1.2761932909708882e-05,
      "loss": 0.9288,
      "step": 25200
    },
    {
      "epoch": 3.73873208216344,
      "grad_norm": 1.1730964183807373,
      "learning_rate": 1.261415693808187e-05,
      "loss": 0.902,
      "step": 25300
    },
    {
      "epoch": 3.7535096793261413,
      "grad_norm": 1.0946401357650757,
      "learning_rate": 1.2466380966454855e-05,
      "loss": 0.902,
      "step": 25400
    },
    {
      "epoch": 3.768287276488843,
      "grad_norm": 1.4348212480545044,
      "learning_rate": 1.2318604994827842e-05,
      "loss": 0.9013,
      "step": 25500
    },
    {
      "epoch": 3.7830648736515444,
      "grad_norm": 1.6366435289382935,
      "learning_rate": 1.2170829023200827e-05,
      "loss": 0.9062,
      "step": 25600
    },
    {
      "epoch": 3.7978424708142455,
      "grad_norm": 0.9022039771080017,
      "learning_rate": 1.2023053051573813e-05,
      "loss": 0.8942,
      "step": 25700
    },
    {
      "epoch": 3.812620067976947,
      "grad_norm": 1.2739439010620117,
      "learning_rate": 1.1875277079946802e-05,
      "loss": 0.8916,
      "step": 25800
    },
    {
      "epoch": 3.827397665139648,
      "grad_norm": 1.2716108560562134,
      "learning_rate": 1.1727501108319788e-05,
      "loss": 0.8843,
      "step": 25900
    },
    {
      "epoch": 3.8421752623023497,
      "grad_norm": 1.1546984910964966,
      "learning_rate": 1.1579725136692775e-05,
      "loss": 0.8994,
      "step": 26000
    },
    {
      "epoch": 3.8421752623023497,
      "eval_loss": 0.7817230224609375,
      "eval_runtime": 72.4211,
      "eval_samples_per_second": 166.112,
      "eval_steps_per_second": 20.767,
      "step": 26000
    },
    {
      "epoch": 3.856952859465051,
      "grad_norm": 0.9304656386375427,
      "learning_rate": 1.1431949165065761e-05,
      "loss": 0.8934,
      "step": 26100
    },
    {
      "epoch": 3.8717304566277524,
      "grad_norm": 0.9781371355056763,
      "learning_rate": 1.1284173193438748e-05,
      "loss": 0.8974,
      "step": 26200
    },
    {
      "epoch": 3.886508053790454,
      "grad_norm": 1.3883702754974365,
      "learning_rate": 1.1136397221811734e-05,
      "loss": 0.8798,
      "step": 26300
    },
    {
      "epoch": 3.901285650953155,
      "grad_norm": 1.1888734102249146,
      "learning_rate": 1.0988621250184721e-05,
      "loss": 0.8963,
      "step": 26400
    },
    {
      "epoch": 3.916063248115856,
      "grad_norm": 1.040829062461853,
      "learning_rate": 1.0840845278557707e-05,
      "loss": 0.9333,
      "step": 26500
    },
    {
      "epoch": 3.9308408452785577,
      "grad_norm": 0.9743462204933167,
      "learning_rate": 1.0693069306930694e-05,
      "loss": 0.897,
      "step": 26600
    },
    {
      "epoch": 3.9456184424412593,
      "grad_norm": 0.9108147025108337,
      "learning_rate": 1.054529333530368e-05,
      "loss": 0.8918,
      "step": 26700
    },
    {
      "epoch": 3.9603960396039604,
      "grad_norm": 1.426581621170044,
      "learning_rate": 1.0397517363676667e-05,
      "loss": 0.8957,
      "step": 26800
    },
    {
      "epoch": 3.9751736367666615,
      "grad_norm": 1.2696062326431274,
      "learning_rate": 1.0249741392049652e-05,
      "loss": 0.8865,
      "step": 26900
    },
    {
      "epoch": 3.989951233929363,
      "grad_norm": 1.1065013408660889,
      "learning_rate": 1.0101965420422638e-05,
      "loss": 0.8989,
      "step": 27000
    },
    {
      "epoch": 4.004728831092065,
      "grad_norm": 1.0660098791122437,
      "learning_rate": 9.954189448795627e-06,
      "loss": 0.8804,
      "step": 27100
    },
    {
      "epoch": 4.019506428254766,
      "grad_norm": 1.0687923431396484,
      "learning_rate": 9.806413477168613e-06,
      "loss": 0.8968,
      "step": 27200
    },
    {
      "epoch": 4.034284025417467,
      "grad_norm": 1.6326539516448975,
      "learning_rate": 9.6586375055416e-06,
      "loss": 0.8797,
      "step": 27300
    },
    {
      "epoch": 4.049061622580169,
      "grad_norm": 1.2591149806976318,
      "learning_rate": 9.510861533914586e-06,
      "loss": 0.9107,
      "step": 27400
    },
    {
      "epoch": 4.06383921974287,
      "grad_norm": 0.9428071975708008,
      "learning_rate": 9.363085562287573e-06,
      "loss": 0.8567,
      "step": 27500
    },
    {
      "epoch": 4.078616816905571,
      "grad_norm": 1.2433983087539673,
      "learning_rate": 9.21530959066056e-06,
      "loss": 0.8836,
      "step": 27600
    },
    {
      "epoch": 4.093394414068272,
      "grad_norm": 0.9615300893783569,
      "learning_rate": 9.067533619033546e-06,
      "loss": 0.8546,
      "step": 27700
    },
    {
      "epoch": 4.108172011230974,
      "grad_norm": 1.3900023698806763,
      "learning_rate": 8.919757647406533e-06,
      "loss": 0.8571,
      "step": 27800
    },
    {
      "epoch": 4.122949608393675,
      "grad_norm": 1.616820216178894,
      "learning_rate": 8.771981675779519e-06,
      "loss": 0.8436,
      "step": 27900
    },
    {
      "epoch": 4.137727205556376,
      "grad_norm": 0.9405859112739563,
      "learning_rate": 8.624205704152506e-06,
      "loss": 0.9286,
      "step": 28000
    },
    {
      "epoch": 4.137727205556376,
      "eval_loss": 0.777459979057312,
      "eval_runtime": 72.4187,
      "eval_samples_per_second": 166.117,
      "eval_steps_per_second": 20.768,
      "step": 28000
    },
    {
      "epoch": 4.152504802719078,
      "grad_norm": 1.0212284326553345,
      "learning_rate": 8.47642973252549e-06,
      "loss": 0.8649,
      "step": 28100
    },
    {
      "epoch": 4.167282399881779,
      "grad_norm": 1.0495051145553589,
      "learning_rate": 8.328653760898477e-06,
      "loss": 0.8968,
      "step": 28200
    },
    {
      "epoch": 4.1820599970444805,
      "grad_norm": 1.3647807836532593,
      "learning_rate": 8.180877789271465e-06,
      "loss": 0.9015,
      "step": 28300
    },
    {
      "epoch": 4.196837594207182,
      "grad_norm": 1.46119225025177,
      "learning_rate": 8.033101817644452e-06,
      "loss": 0.8982,
      "step": 28400
    },
    {
      "epoch": 4.211615191369884,
      "grad_norm": 1.3252617120742798,
      "learning_rate": 7.885325846017438e-06,
      "loss": 0.8815,
      "step": 28500
    },
    {
      "epoch": 4.226392788532585,
      "grad_norm": 0.8765128254890442,
      "learning_rate": 7.737549874390425e-06,
      "loss": 0.8903,
      "step": 28600
    },
    {
      "epoch": 4.241170385695286,
      "grad_norm": 1.0684491395950317,
      "learning_rate": 7.5897739027634115e-06,
      "loss": 0.8813,
      "step": 28700
    },
    {
      "epoch": 4.255947982857987,
      "grad_norm": 1.0353062152862549,
      "learning_rate": 7.441997931136398e-06,
      "loss": 0.882,
      "step": 28800
    },
    {
      "epoch": 4.270725580020689,
      "grad_norm": 1.38593327999115,
      "learning_rate": 7.2942219595093846e-06,
      "loss": 0.8645,
      "step": 28900
    },
    {
      "epoch": 4.28550317718339,
      "grad_norm": 1.3453259468078613,
      "learning_rate": 7.14644598788237e-06,
      "loss": 0.8855,
      "step": 29000
    },
    {
      "epoch": 4.300280774346091,
      "grad_norm": 1.1167036294937134,
      "learning_rate": 6.998670016255357e-06,
      "loss": 0.894,
      "step": 29100
    },
    {
      "epoch": 4.315058371508792,
      "grad_norm": 1.4011156558990479,
      "learning_rate": 6.850894044628343e-06,
      "loss": 0.9246,
      "step": 29200
    },
    {
      "epoch": 4.329835968671494,
      "grad_norm": 1.087908148765564,
      "learning_rate": 6.70311807300133e-06,
      "loss": 0.882,
      "step": 29300
    },
    {
      "epoch": 4.344613565834195,
      "grad_norm": 1.2621804475784302,
      "learning_rate": 6.5553421013743164e-06,
      "loss": 0.8615,
      "step": 29400
    },
    {
      "epoch": 4.359391162996896,
      "grad_norm": 1.4927130937576294,
      "learning_rate": 6.407566129747303e-06,
      "loss": 0.9472,
      "step": 29500
    },
    {
      "epoch": 4.374168760159598,
      "grad_norm": 1.326126217842102,
      "learning_rate": 6.25979015812029e-06,
      "loss": 0.8704,
      "step": 29600
    },
    {
      "epoch": 4.3889463573222995,
      "grad_norm": 1.2137659788131714,
      "learning_rate": 6.112014186493276e-06,
      "loss": 0.9116,
      "step": 29700
    },
    {
      "epoch": 4.403723954485001,
      "grad_norm": 1.1274709701538086,
      "learning_rate": 5.964238214866263e-06,
      "loss": 0.8655,
      "step": 29800
    },
    {
      "epoch": 4.418501551647702,
      "grad_norm": 0.9292647838592529,
      "learning_rate": 5.81646224323925e-06,
      "loss": 0.8667,
      "step": 29900
    },
    {
      "epoch": 4.433279148810404,
      "grad_norm": 1.32888662815094,
      "learning_rate": 5.668686271612237e-06,
      "loss": 0.8704,
      "step": 30000
    },
    {
      "epoch": 4.433279148810404,
      "eval_loss": 0.7724891901016235,
      "eval_runtime": 71.9804,
      "eval_samples_per_second": 167.129,
      "eval_steps_per_second": 20.895,
      "step": 30000
    },
    {
      "epoch": 4.448056745973105,
      "grad_norm": 1.0097945928573608,
      "learning_rate": 5.520910299985223e-06,
      "loss": 0.868,
      "step": 30100
    },
    {
      "epoch": 4.462834343135806,
      "grad_norm": 1.1258947849273682,
      "learning_rate": 5.373134328358209e-06,
      "loss": 0.8748,
      "step": 30200
    },
    {
      "epoch": 4.477611940298507,
      "grad_norm": 1.364837884902954,
      "learning_rate": 5.225358356731195e-06,
      "loss": 0.8793,
      "step": 30300
    },
    {
      "epoch": 4.492389537461209,
      "grad_norm": 1.4305410385131836,
      "learning_rate": 5.077582385104182e-06,
      "loss": 0.8697,
      "step": 30400
    },
    {
      "epoch": 4.50716713462391,
      "grad_norm": 1.8279186487197876,
      "learning_rate": 4.929806413477169e-06,
      "loss": 0.8293,
      "step": 30500
    },
    {
      "epoch": 4.521944731786611,
      "grad_norm": 1.1031122207641602,
      "learning_rate": 4.782030441850156e-06,
      "loss": 0.8414,
      "step": 30600
    },
    {
      "epoch": 4.536722328949313,
      "grad_norm": 0.9475739002227783,
      "learning_rate": 4.634254470223142e-06,
      "loss": 0.853,
      "step": 30700
    },
    {
      "epoch": 4.551499926112014,
      "grad_norm": 1.576987624168396,
      "learning_rate": 4.486478498596128e-06,
      "loss": 0.8644,
      "step": 30800
    },
    {
      "epoch": 4.5662775232747155,
      "grad_norm": 1.2438279390335083,
      "learning_rate": 4.338702526969115e-06,
      "loss": 0.9084,
      "step": 30900
    },
    {
      "epoch": 4.581055120437417,
      "grad_norm": 1.1380743980407715,
      "learning_rate": 4.190926555342101e-06,
      "loss": 0.8555,
      "step": 31000
    },
    {
      "epoch": 4.595832717600119,
      "grad_norm": 1.423694372177124,
      "learning_rate": 4.043150583715088e-06,
      "loss": 0.8135,
      "step": 31100
    },
    {
      "epoch": 4.61061031476282,
      "grad_norm": 1.9756057262420654,
      "learning_rate": 3.895374612088075e-06,
      "loss": 0.8848,
      "step": 31200
    },
    {
      "epoch": 4.625387911925521,
      "grad_norm": 1.2467362880706787,
      "learning_rate": 3.7475986404610613e-06,
      "loss": 0.8444,
      "step": 31300
    },
    {
      "epoch": 4.640165509088222,
      "grad_norm": 1.1764073371887207,
      "learning_rate": 3.599822668834048e-06,
      "loss": 0.88,
      "step": 31400
    },
    {
      "epoch": 4.654943106250924,
      "grad_norm": 1.1241683959960938,
      "learning_rate": 3.4520466972070344e-06,
      "loss": 0.9105,
      "step": 31500
    },
    {
      "epoch": 4.669720703413625,
      "grad_norm": 1.140602946281433,
      "learning_rate": 3.3042707255800205e-06,
      "loss": 0.8791,
      "step": 31600
    },
    {
      "epoch": 4.684498300576326,
      "grad_norm": 1.2846508026123047,
      "learning_rate": 3.156494753953007e-06,
      "loss": 0.8779,
      "step": 31700
    },
    {
      "epoch": 4.699275897739028,
      "grad_norm": 1.2230428457260132,
      "learning_rate": 3.008718782325994e-06,
      "loss": 0.8837,
      "step": 31800
    },
    {
      "epoch": 4.714053494901729,
      "grad_norm": 1.119362235069275,
      "learning_rate": 2.8609428106989806e-06,
      "loss": 0.8747,
      "step": 31900
    },
    {
      "epoch": 4.72883109206443,
      "grad_norm": 1.016284704208374,
      "learning_rate": 2.713166839071967e-06,
      "loss": 0.8672,
      "step": 32000
    },
    {
      "epoch": 4.72883109206443,
      "eval_loss": 0.7697680592536926,
      "eval_runtime": 71.7521,
      "eval_samples_per_second": 167.661,
      "eval_steps_per_second": 20.961,
      "step": 32000
    },
    {
      "epoch": 4.743608689227131,
      "grad_norm": 1.234525203704834,
      "learning_rate": 2.5653908674449537e-06,
      "loss": 0.8943,
      "step": 32100
    },
    {
      "epoch": 4.7583862863898325,
      "grad_norm": 1.5732767581939697,
      "learning_rate": 2.4176148958179398e-06,
      "loss": 0.8876,
      "step": 32200
    },
    {
      "epoch": 4.7731638835525345,
      "grad_norm": 0.999059796333313,
      "learning_rate": 2.2698389241909267e-06,
      "loss": 0.8657,
      "step": 32300
    },
    {
      "epoch": 4.787941480715236,
      "grad_norm": 1.05172860622406,
      "learning_rate": 2.1220629525639133e-06,
      "loss": 0.8614,
      "step": 32400
    },
    {
      "epoch": 4.802719077877937,
      "grad_norm": 1.1320940256118774,
      "learning_rate": 1.9742869809368994e-06,
      "loss": 0.836,
      "step": 32500
    },
    {
      "epoch": 4.817496675040639,
      "grad_norm": 1.204539179801941,
      "learning_rate": 1.8265110093098864e-06,
      "loss": 0.8995,
      "step": 32600
    },
    {
      "epoch": 4.83227427220334,
      "grad_norm": 1.6045246124267578,
      "learning_rate": 1.6787350376828727e-06,
      "loss": 0.8556,
      "step": 32700
    },
    {
      "epoch": 4.847051869366041,
      "grad_norm": 1.5342392921447754,
      "learning_rate": 1.5309590660558595e-06,
      "loss": 0.8788,
      "step": 32800
    },
    {
      "epoch": 4.861829466528742,
      "grad_norm": 1.066981315612793,
      "learning_rate": 1.3831830944288458e-06,
      "loss": 0.8705,
      "step": 32900
    },
    {
      "epoch": 4.876607063691444,
      "grad_norm": 1.3140721321105957,
      "learning_rate": 1.2354071228018326e-06,
      "loss": 0.867,
      "step": 33000
    },
    {
      "epoch": 4.891384660854145,
      "grad_norm": 1.2660177946090698,
      "learning_rate": 1.0876311511748191e-06,
      "loss": 0.8908,
      "step": 33100
    },
    {
      "epoch": 4.906162258016846,
      "grad_norm": 1.0654680728912354,
      "learning_rate": 9.398551795478056e-07,
      "loss": 0.8756,
      "step": 33200
    },
    {
      "epoch": 4.920939855179547,
      "grad_norm": 1.1125162839889526,
      "learning_rate": 7.920792079207921e-07,
      "loss": 0.8419,
      "step": 33300
    },
    {
      "epoch": 4.935717452342249,
      "grad_norm": 1.0036849975585938,
      "learning_rate": 6.443032362937787e-07,
      "loss": 0.8685,
      "step": 33400
    },
    {
      "epoch": 4.9504950495049505,
      "grad_norm": 1.1865416765213013,
      "learning_rate": 4.965272646667652e-07,
      "loss": 0.8665,
      "step": 33500
    },
    {
      "epoch": 4.965272646667652,
      "grad_norm": 1.226455569267273,
      "learning_rate": 3.4875129303975176e-07,
      "loss": 0.8955,
      "step": 33600
    },
    {
      "epoch": 4.9800502438303536,
      "grad_norm": 1.0938405990600586,
      "learning_rate": 2.009753214127383e-07,
      "loss": 0.8921,
      "step": 33700
    },
    {
      "epoch": 4.994827840993055,
      "grad_norm": 1.24551522731781,
      "learning_rate": 5.319934978572484e-08,
      "loss": 0.8654,
      "step": 33800
    },
    {
      "epoch": 5.009605438155756,
      "grad_norm": 1.2577146291732788,
      "learning_rate": 2.4952711689079357e-05,
      "loss": 0.8708,
      "step": 33900
    },
    {
      "epoch": 5.024383035318457,
      "grad_norm": 0.9523239731788635,
      "learning_rate": 2.4878823703265848e-05,
      "loss": 0.8763,
      "step": 34000
    },
    {
      "epoch": 5.024383035318457,
      "eval_loss": 0.768021285533905,
      "eval_runtime": 73.3488,
      "eval_samples_per_second": 164.011,
      "eval_steps_per_second": 20.505,
      "step": 34000
    },
    {
      "epoch": 5.039160632481159,
      "grad_norm": 1.196171522140503,
      "learning_rate": 2.4804935717452342e-05,
      "loss": 0.8684,
      "step": 34100
    },
    {
      "epoch": 5.05393822964386,
      "grad_norm": 0.9675619602203369,
      "learning_rate": 2.4731047731638836e-05,
      "loss": 0.8925,
      "step": 34200
    },
    {
      "epoch": 5.068715826806561,
      "grad_norm": 1.1133108139038086,
      "learning_rate": 2.465715974582533e-05,
      "loss": 0.9108,
      "step": 34300
    },
    {
      "epoch": 5.083493423969262,
      "grad_norm": 1.0612958669662476,
      "learning_rate": 2.458327176001182e-05,
      "loss": 0.869,
      "step": 34400
    },
    {
      "epoch": 5.098271021131964,
      "grad_norm": 1.382619857788086,
      "learning_rate": 2.4509383774198315e-05,
      "loss": 0.8611,
      "step": 34500
    },
    {
      "epoch": 5.113048618294665,
      "grad_norm": 0.9677446484565735,
      "learning_rate": 2.443549578838481e-05,
      "loss": 0.8602,
      "step": 34600
    },
    {
      "epoch": 5.127826215457366,
      "grad_norm": 1.1815754175186157,
      "learning_rate": 2.4361607802571303e-05,
      "loss": 0.8583,
      "step": 34700
    },
    {
      "epoch": 5.142603812620068,
      "grad_norm": 1.1302406787872314,
      "learning_rate": 2.4287719816757794e-05,
      "loss": 0.846,
      "step": 34800
    },
    {
      "epoch": 5.1573814097827695,
      "grad_norm": 1.2724354267120361,
      "learning_rate": 2.4213831830944288e-05,
      "loss": 0.8629,
      "step": 34900
    },
    {
      "epoch": 5.172159006945471,
      "grad_norm": 1.1008046865463257,
      "learning_rate": 2.4139943845130782e-05,
      "loss": 0.8483,
      "step": 35000
    },
    {
      "epoch": 5.186936604108172,
      "grad_norm": 1.2529388666152954,
      "learning_rate": 2.4066055859317276e-05,
      "loss": 0.8642,
      "step": 35100
    },
    {
      "epoch": 5.201714201270874,
      "grad_norm": 1.0253926515579224,
      "learning_rate": 2.399216787350377e-05,
      "loss": 0.8667,
      "step": 35200
    },
    {
      "epoch": 5.216491798433575,
      "grad_norm": 1.2858742475509644,
      "learning_rate": 2.391827988769026e-05,
      "loss": 0.8721,
      "step": 35300
    },
    {
      "epoch": 5.231269395596276,
      "grad_norm": 1.264145851135254,
      "learning_rate": 2.3844391901876755e-05,
      "loss": 0.8537,
      "step": 35400
    },
    {
      "epoch": 5.246046992758977,
      "grad_norm": 1.1545524597167969,
      "learning_rate": 2.377050391606325e-05,
      "loss": 0.8983,
      "step": 35500
    },
    {
      "epoch": 5.260824589921679,
      "grad_norm": 1.0486732721328735,
      "learning_rate": 2.3696615930249744e-05,
      "loss": 0.8411,
      "step": 35600
    },
    {
      "epoch": 5.27560218708438,
      "grad_norm": 1.0643534660339355,
      "learning_rate": 2.3622727944436234e-05,
      "loss": 0.8808,
      "step": 35700
    },
    {
      "epoch": 5.290379784247081,
      "grad_norm": 1.0051188468933105,
      "learning_rate": 2.354883995862273e-05,
      "loss": 0.8277,
      "step": 35800
    },
    {
      "epoch": 5.305157381409783,
      "grad_norm": 1.0107336044311523,
      "learning_rate": 2.3474951972809223e-05,
      "loss": 0.8469,
      "step": 35900
    },
    {
      "epoch": 5.319934978572484,
      "grad_norm": 1.0577834844589233,
      "learning_rate": 2.3401063986995717e-05,
      "loss": 0.8689,
      "step": 36000
    },
    {
      "epoch": 5.319934978572484,
      "eval_loss": 0.7623529434204102,
      "eval_runtime": 73.4466,
      "eval_samples_per_second": 163.792,
      "eval_steps_per_second": 20.477,
      "step": 36000
    },
    {
      "epoch": 5.334712575735185,
      "grad_norm": 1.2195557355880737,
      "learning_rate": 2.3327176001182207e-05,
      "loss": 0.8829,
      "step": 36100
    },
    {
      "epoch": 5.3494901728978865,
      "grad_norm": 0.9773972034454346,
      "learning_rate": 2.32532880153687e-05,
      "loss": 0.8194,
      "step": 36200
    },
    {
      "epoch": 5.3642677700605885,
      "grad_norm": 1.256047010421753,
      "learning_rate": 2.3179400029555196e-05,
      "loss": 0.8393,
      "step": 36300
    },
    {
      "epoch": 5.37904536722329,
      "grad_norm": 0.9086467623710632,
      "learning_rate": 2.310551204374169e-05,
      "loss": 0.8411,
      "step": 36400
    },
    {
      "epoch": 5.393822964385991,
      "grad_norm": 1.101503610610962,
      "learning_rate": 2.3031624057928184e-05,
      "loss": 0.8404,
      "step": 36500
    },
    {
      "epoch": 5.408600561548692,
      "grad_norm": 1.0946028232574463,
      "learning_rate": 2.2957736072114675e-05,
      "loss": 0.8927,
      "step": 36600
    },
    {
      "epoch": 5.423378158711394,
      "grad_norm": 1.253849983215332,
      "learning_rate": 2.288384808630117e-05,
      "loss": 0.8646,
      "step": 36700
    },
    {
      "epoch": 5.438155755874095,
      "grad_norm": 1.2631292343139648,
      "learning_rate": 2.2809960100487663e-05,
      "loss": 0.8581,
      "step": 36800
    },
    {
      "epoch": 5.452933353036796,
      "grad_norm": 0.7580145597457886,
      "learning_rate": 2.2736072114674157e-05,
      "loss": 0.8687,
      "step": 36900
    },
    {
      "epoch": 5.467710950199497,
      "grad_norm": 1.0046567916870117,
      "learning_rate": 2.2662184128860648e-05,
      "loss": 0.8801,
      "step": 37000
    },
    {
      "epoch": 5.482488547362199,
      "grad_norm": 1.1684798002243042,
      "learning_rate": 2.2588296143047142e-05,
      "loss": 0.8537,
      "step": 37100
    },
    {
      "epoch": 5.4972661445249,
      "grad_norm": 1.149670958518982,
      "learning_rate": 2.2514408157233636e-05,
      "loss": 0.8871,
      "step": 37200
    },
    {
      "epoch": 5.512043741687601,
      "grad_norm": 1.1647660732269287,
      "learning_rate": 2.244052017142013e-05,
      "loss": 0.8084,
      "step": 37300
    },
    {
      "epoch": 5.5268213388503025,
      "grad_norm": 1.0900468826293945,
      "learning_rate": 2.236663218560662e-05,
      "loss": 0.8454,
      "step": 37400
    },
    {
      "epoch": 5.5415989360130045,
      "grad_norm": 1.1304272413253784,
      "learning_rate": 2.2292744199793115e-05,
      "loss": 0.834,
      "step": 37500
    },
    {
      "epoch": 5.556376533175706,
      "grad_norm": 0.8535389304161072,
      "learning_rate": 2.221885621397961e-05,
      "loss": 0.8515,
      "step": 37600
    },
    {
      "epoch": 5.571154130338407,
      "grad_norm": 1.224375605583191,
      "learning_rate": 2.2144968228166103e-05,
      "loss": 0.8173,
      "step": 37700
    },
    {
      "epoch": 5.585931727501109,
      "grad_norm": 1.08297598361969,
      "learning_rate": 2.2071080242352597e-05,
      "loss": 0.8729,
      "step": 37800
    },
    {
      "epoch": 5.60070932466381,
      "grad_norm": 0.9069815874099731,
      "learning_rate": 2.1997192256539088e-05,
      "loss": 0.8582,
      "step": 37900
    },
    {
      "epoch": 5.615486921826511,
      "grad_norm": 1.2480193376541138,
      "learning_rate": 2.1923304270725582e-05,
      "loss": 0.8619,
      "step": 38000
    },
    {
      "epoch": 5.615486921826511,
      "eval_loss": 0.7547315955162048,
      "eval_runtime": 73.9691,
      "eval_samples_per_second": 162.636,
      "eval_steps_per_second": 20.333,
      "step": 38000
    },
    {
      "epoch": 5.630264518989212,
      "grad_norm": 1.1185215711593628,
      "learning_rate": 2.1849416284912076e-05,
      "loss": 0.8513,
      "step": 38100
    },
    {
      "epoch": 5.645042116151914,
      "grad_norm": 0.8746647834777832,
      "learning_rate": 2.1775528299098567e-05,
      "loss": 0.8359,
      "step": 38200
    },
    {
      "epoch": 5.659819713314615,
      "grad_norm": 0.9615562558174133,
      "learning_rate": 2.170164031328506e-05,
      "loss": 0.8521,
      "step": 38300
    },
    {
      "epoch": 5.674597310477316,
      "grad_norm": 1.1239737272262573,
      "learning_rate": 2.1627752327471555e-05,
      "loss": 0.8357,
      "step": 38400
    },
    {
      "epoch": 5.689374907640017,
      "grad_norm": 1.2755120992660522,
      "learning_rate": 2.1553864341658046e-05,
      "loss": 0.828,
      "step": 38500
    },
    {
      "epoch": 5.704152504802719,
      "grad_norm": 0.9244624376296997,
      "learning_rate": 2.147997635584454e-05,
      "loss": 0.8503,
      "step": 38600
    },
    {
      "epoch": 5.71893010196542,
      "grad_norm": 1.3098539113998413,
      "learning_rate": 2.1406088370031034e-05,
      "loss": 0.8632,
      "step": 38700
    },
    {
      "epoch": 5.7337076991281215,
      "grad_norm": 0.9496890902519226,
      "learning_rate": 2.1332200384217525e-05,
      "loss": 0.8519,
      "step": 38800
    },
    {
      "epoch": 5.7484852962908235,
      "grad_norm": 0.8030812740325928,
      "learning_rate": 2.125831239840402e-05,
      "loss": 0.8594,
      "step": 38900
    },
    {
      "epoch": 5.763262893453525,
      "grad_norm": 0.8589705228805542,
      "learning_rate": 2.1184424412590513e-05,
      "loss": 0.853,
      "step": 39000
    },
    {
      "epoch": 5.778040490616226,
      "grad_norm": 1.1959925889968872,
      "learning_rate": 2.1110536426777007e-05,
      "loss": 0.84,
      "step": 39100
    },
    {
      "epoch": 5.792818087778927,
      "grad_norm": 1.061894416809082,
      "learning_rate": 2.1036648440963498e-05,
      "loss": 0.8516,
      "step": 39200
    },
    {
      "epoch": 5.807595684941629,
      "grad_norm": 0.9921948909759521,
      "learning_rate": 2.0962760455149992e-05,
      "loss": 0.8329,
      "step": 39300
    },
    {
      "epoch": 5.82237328210433,
      "grad_norm": 1.2263851165771484,
      "learning_rate": 2.0888872469336486e-05,
      "loss": 0.8822,
      "step": 39400
    },
    {
      "epoch": 5.837150879267031,
      "grad_norm": 1.188123106956482,
      "learning_rate": 2.081498448352298e-05,
      "loss": 0.838,
      "step": 39500
    },
    {
      "epoch": 5.851928476429732,
      "grad_norm": 1.256703495979309,
      "learning_rate": 2.074109649770947e-05,
      "loss": 0.8511,
      "step": 39600
    },
    {
      "epoch": 5.866706073592434,
      "grad_norm": 0.6737883687019348,
      "learning_rate": 2.0667208511895965e-05,
      "loss": 0.8486,
      "step": 39700
    },
    {
      "epoch": 5.881483670755135,
      "grad_norm": 1.0785192251205444,
      "learning_rate": 2.059332052608246e-05,
      "loss": 0.8735,
      "step": 39800
    },
    {
      "epoch": 5.896261267917836,
      "grad_norm": 1.5023212432861328,
      "learning_rate": 2.0519432540268954e-05,
      "loss": 0.856,
      "step": 39900
    },
    {
      "epoch": 5.911038865080538,
      "grad_norm": 1.2441891431808472,
      "learning_rate": 2.0445544554455444e-05,
      "loss": 0.8591,
      "step": 40000
    },
    {
      "epoch": 5.911038865080538,
      "eval_loss": 0.7406327128410339,
      "eval_runtime": 74.3106,
      "eval_samples_per_second": 161.888,
      "eval_steps_per_second": 20.239,
      "step": 40000
    },
    {
      "epoch": 5.925816462243239,
      "grad_norm": 1.1299391984939575,
      "learning_rate": 2.037165656864194e-05,
      "loss": 0.8419,
      "step": 40100
    },
    {
      "epoch": 5.9405940594059405,
      "grad_norm": 1.2874855995178223,
      "learning_rate": 2.0297768582828433e-05,
      "loss": 0.7998,
      "step": 40200
    },
    {
      "epoch": 5.955371656568642,
      "grad_norm": 0.8088463544845581,
      "learning_rate": 2.0223880597014927e-05,
      "loss": 0.8845,
      "step": 40300
    },
    {
      "epoch": 5.970149253731344,
      "grad_norm": 1.3085877895355225,
      "learning_rate": 2.014999261120142e-05,
      "loss": 0.865,
      "step": 40400
    },
    {
      "epoch": 5.984926850894045,
      "grad_norm": 1.0041298866271973,
      "learning_rate": 2.007610462538791e-05,
      "loss": 0.8462,
      "step": 40500
    },
    {
      "epoch": 5.999704448056746,
      "grad_norm": 1.1434369087219238,
      "learning_rate": 2.0002216639574406e-05,
      "loss": 0.8288,
      "step": 40600
    },
    {
      "epoch": 6.014482045219447,
      "grad_norm": 0.9769048690795898,
      "learning_rate": 1.99283286537609e-05,
      "loss": 0.8214,
      "step": 40700
    },
    {
      "epoch": 6.029259642382149,
      "grad_norm": 1.0948989391326904,
      "learning_rate": 1.9854440667947394e-05,
      "loss": 0.8236,
      "step": 40800
    },
    {
      "epoch": 6.04403723954485,
      "grad_norm": 1.1169737577438354,
      "learning_rate": 1.9780552682133885e-05,
      "loss": 0.7975,
      "step": 40900
    },
    {
      "epoch": 6.058814836707551,
      "grad_norm": 0.9895215034484863,
      "learning_rate": 1.970666469632038e-05,
      "loss": 0.81,
      "step": 41000
    },
    {
      "epoch": 6.073592433870252,
      "grad_norm": 1.1198062896728516,
      "learning_rate": 1.9632776710506873e-05,
      "loss": 0.8045,
      "step": 41100
    },
    {
      "epoch": 6.088370031032954,
      "grad_norm": 1.1723202466964722,
      "learning_rate": 1.9558888724693367e-05,
      "loss": 0.8365,
      "step": 41200
    },
    {
      "epoch": 6.103147628195655,
      "grad_norm": 1.050209879875183,
      "learning_rate": 1.9485000738879858e-05,
      "loss": 0.8228,
      "step": 41300
    },
    {
      "epoch": 6.1179252253583565,
      "grad_norm": 1.2891385555267334,
      "learning_rate": 1.9411112753066352e-05,
      "loss": 0.8466,
      "step": 41400
    },
    {
      "epoch": 6.1327028225210585,
      "grad_norm": 0.9653822779655457,
      "learning_rate": 1.9337224767252846e-05,
      "loss": 0.8309,
      "step": 41500
    },
    {
      "epoch": 6.14748041968376,
      "grad_norm": 0.9181697964668274,
      "learning_rate": 1.926333678143934e-05,
      "loss": 0.8282,
      "step": 41600
    },
    {
      "epoch": 6.162258016846461,
      "grad_norm": 1.197266936302185,
      "learning_rate": 1.9189448795625834e-05,
      "loss": 0.7986,
      "step": 41700
    },
    {
      "epoch": 6.177035614009162,
      "grad_norm": 1.1372621059417725,
      "learning_rate": 1.9115560809812325e-05,
      "loss": 0.8351,
      "step": 41800
    },
    {
      "epoch": 6.191813211171864,
      "grad_norm": 0.9948418140411377,
      "learning_rate": 1.904167282399882e-05,
      "loss": 0.7717,
      "step": 41900
    },
    {
      "epoch": 6.206590808334565,
      "grad_norm": 1.6077730655670166,
      "learning_rate": 1.8967784838185313e-05,
      "loss": 0.8007,
      "step": 42000
    },
    {
      "epoch": 6.206590808334565,
      "eval_loss": 0.7386361956596375,
      "eval_runtime": 72.4258,
      "eval_samples_per_second": 166.101,
      "eval_steps_per_second": 20.766,
      "step": 42000
    },
    {
      "epoch": 6.221368405497266,
      "grad_norm": 1.1841671466827393,
      "learning_rate": 1.8893896852371807e-05,
      "loss": 0.8159,
      "step": 42100
    },
    {
      "epoch": 6.236146002659967,
      "grad_norm": 1.0084643363952637,
      "learning_rate": 1.8820008866558298e-05,
      "loss": 0.8555,
      "step": 42200
    },
    {
      "epoch": 6.250923599822669,
      "grad_norm": 1.47970712184906,
      "learning_rate": 1.8746120880744792e-05,
      "loss": 0.817,
      "step": 42300
    },
    {
      "epoch": 6.26570119698537,
      "grad_norm": 1.2381446361541748,
      "learning_rate": 1.8672232894931286e-05,
      "loss": 0.8141,
      "step": 42400
    },
    {
      "epoch": 6.280478794148071,
      "grad_norm": 0.9155701398849487,
      "learning_rate": 1.859834490911778e-05,
      "loss": 0.8162,
      "step": 42500
    },
    {
      "epoch": 6.295256391310772,
      "grad_norm": 1.5681486129760742,
      "learning_rate": 1.852445692330427e-05,
      "loss": 0.8008,
      "step": 42600
    },
    {
      "epoch": 6.310033988473474,
      "grad_norm": 1.254838466644287,
      "learning_rate": 1.8450568937490765e-05,
      "loss": 0.8305,
      "step": 42700
    },
    {
      "epoch": 6.3248115856361755,
      "grad_norm": 1.1463549137115479,
      "learning_rate": 1.837668095167726e-05,
      "loss": 0.8533,
      "step": 42800
    },
    {
      "epoch": 6.339589182798877,
      "grad_norm": 1.3144460916519165,
      "learning_rate": 1.8302792965863753e-05,
      "loss": 0.8398,
      "step": 42900
    },
    {
      "epoch": 6.354366779961579,
      "grad_norm": 1.1362299919128418,
      "learning_rate": 1.8228904980050248e-05,
      "loss": 0.8107,
      "step": 43000
    },
    {
      "epoch": 6.36914437712428,
      "grad_norm": 1.529261827468872,
      "learning_rate": 1.815501699423674e-05,
      "loss": 0.8198,
      "step": 43100
    },
    {
      "epoch": 6.383921974286981,
      "grad_norm": 1.240634560585022,
      "learning_rate": 1.8081129008423232e-05,
      "loss": 0.8118,
      "step": 43200
    },
    {
      "epoch": 6.398699571449682,
      "grad_norm": 1.1067168712615967,
      "learning_rate": 1.8007241022609723e-05,
      "loss": 0.8112,
      "step": 43300
    },
    {
      "epoch": 6.413477168612384,
      "grad_norm": 1.0142170190811157,
      "learning_rate": 1.7933353036796217e-05,
      "loss": 0.8214,
      "step": 43400
    },
    {
      "epoch": 6.428254765775085,
      "grad_norm": 1.4841359853744507,
      "learning_rate": 1.785946505098271e-05,
      "loss": 0.8121,
      "step": 43500
    },
    {
      "epoch": 6.443032362937786,
      "grad_norm": 1.3319240808486938,
      "learning_rate": 1.7785577065169202e-05,
      "loss": 0.806,
      "step": 43600
    },
    {
      "epoch": 6.457809960100487,
      "grad_norm": 1.1235730648040771,
      "learning_rate": 1.7711689079355696e-05,
      "loss": 0.8007,
      "step": 43700
    },
    {
      "epoch": 6.472587557263189,
      "grad_norm": 1.7195336818695068,
      "learning_rate": 1.763780109354219e-05,
      "loss": 0.8538,
      "step": 43800
    },
    {
      "epoch": 6.48736515442589,
      "grad_norm": 1.033251166343689,
      "learning_rate": 1.756391310772868e-05,
      "loss": 0.782,
      "step": 43900
    },
    {
      "epoch": 6.5021427515885915,
      "grad_norm": 1.0016850233078003,
      "learning_rate": 1.7490025121915175e-05,
      "loss": 0.7982,
      "step": 44000
    },
    {
      "epoch": 6.5021427515885915,
      "eval_loss": 0.7281588315963745,
      "eval_runtime": 74.8373,
      "eval_samples_per_second": 160.749,
      "eval_steps_per_second": 20.097,
      "step": 44000
    },
    {
      "epoch": 6.5169203487512934,
      "grad_norm": 1.0589277744293213,
      "learning_rate": 1.741613713610167e-05,
      "loss": 0.851,
      "step": 44100
    },
    {
      "epoch": 6.5316979459139946,
      "grad_norm": 1.2036662101745605,
      "learning_rate": 1.7342249150288164e-05,
      "loss": 0.8048,
      "step": 44200
    },
    {
      "epoch": 6.546475543076696,
      "grad_norm": 0.9461020827293396,
      "learning_rate": 1.7268361164474658e-05,
      "loss": 0.8246,
      "step": 44300
    },
    {
      "epoch": 6.561253140239397,
      "grad_norm": 1.5425201654434204,
      "learning_rate": 1.719447317866115e-05,
      "loss": 0.8078,
      "step": 44400
    },
    {
      "epoch": 6.576030737402099,
      "grad_norm": 0.999302864074707,
      "learning_rate": 1.7120585192847642e-05,
      "loss": 0.7996,
      "step": 44500
    },
    {
      "epoch": 6.5908083345648,
      "grad_norm": 0.7974364757537842,
      "learning_rate": 1.7046697207034137e-05,
      "loss": 0.7894,
      "step": 44600
    },
    {
      "epoch": 6.605585931727501,
      "grad_norm": 1.6465458869934082,
      "learning_rate": 1.697280922122063e-05,
      "loss": 0.8036,
      "step": 44700
    },
    {
      "epoch": 6.620363528890202,
      "grad_norm": 1.097476840019226,
      "learning_rate": 1.689892123540712e-05,
      "loss": 0.811,
      "step": 44800
    },
    {
      "epoch": 6.635141126052904,
      "grad_norm": 1.1419247388839722,
      "learning_rate": 1.6825033249593616e-05,
      "loss": 0.846,
      "step": 44900
    },
    {
      "epoch": 6.649918723215605,
      "grad_norm": 1.2973955869674683,
      "learning_rate": 1.675114526378011e-05,
      "loss": 0.7782,
      "step": 45000
    },
    {
      "epoch": 6.664696320378306,
      "grad_norm": 1.0095618963241577,
      "learning_rate": 1.6677257277966604e-05,
      "loss": 0.8332,
      "step": 45100
    },
    {
      "epoch": 6.679473917541008,
      "grad_norm": 0.9655879139900208,
      "learning_rate": 1.6603369292153095e-05,
      "loss": 0.8,
      "step": 45200
    },
    {
      "epoch": 6.694251514703709,
      "grad_norm": 0.97649747133255,
      "learning_rate": 1.652948130633959e-05,
      "loss": 0.833,
      "step": 45300
    },
    {
      "epoch": 6.7090291118664105,
      "grad_norm": 1.1791679859161377,
      "learning_rate": 1.6455593320526083e-05,
      "loss": 0.7924,
      "step": 45400
    },
    {
      "epoch": 6.723806709029112,
      "grad_norm": 0.8489384055137634,
      "learning_rate": 1.6381705334712577e-05,
      "loss": 0.8455,
      "step": 45500
    },
    {
      "epoch": 6.738584306191814,
      "grad_norm": 1.2621809244155884,
      "learning_rate": 1.630781734889907e-05,
      "loss": 0.8205,
      "step": 45600
    },
    {
      "epoch": 6.753361903354515,
      "grad_norm": 1.4490227699279785,
      "learning_rate": 1.6233929363085562e-05,
      "loss": 0.7942,
      "step": 45700
    },
    {
      "epoch": 6.768139500517216,
      "grad_norm": 1.0165139436721802,
      "learning_rate": 1.6160041377272056e-05,
      "loss": 0.8015,
      "step": 45800
    },
    {
      "epoch": 6.782917097679917,
      "grad_norm": 1.335339903831482,
      "learning_rate": 1.608615339145855e-05,
      "loss": 0.8159,
      "step": 45900
    },
    {
      "epoch": 6.797694694842619,
      "grad_norm": 1.125827431678772,
      "learning_rate": 1.6012265405645044e-05,
      "loss": 0.8041,
      "step": 46000
    },
    {
      "epoch": 6.797694694842619,
      "eval_loss": 0.7218738198280334,
      "eval_runtime": 75.1576,
      "eval_samples_per_second": 160.064,
      "eval_steps_per_second": 20.011,
      "step": 46000
    },
    {
      "epoch": 6.81247229200532,
      "grad_norm": 1.047359585762024,
      "learning_rate": 1.5938377419831535e-05,
      "loss": 0.8119,
      "step": 46100
    },
    {
      "epoch": 6.827249889168021,
      "grad_norm": 1.06221342086792,
      "learning_rate": 1.586448943401803e-05,
      "loss": 0.7906,
      "step": 46200
    },
    {
      "epoch": 6.842027486330723,
      "grad_norm": 1.1107245683670044,
      "learning_rate": 1.5790601448204523e-05,
      "loss": 0.8281,
      "step": 46300
    },
    {
      "epoch": 6.856805083493424,
      "grad_norm": 0.946075975894928,
      "learning_rate": 1.5716713462391017e-05,
      "loss": 0.7791,
      "step": 46400
    },
    {
      "epoch": 6.871582680656125,
      "grad_norm": 1.1262515783309937,
      "learning_rate": 1.564282547657751e-05,
      "loss": 0.8138,
      "step": 46500
    },
    {
      "epoch": 6.886360277818826,
      "grad_norm": 1.0643638372421265,
      "learning_rate": 1.5568937490764002e-05,
      "loss": 0.7989,
      "step": 46600
    },
    {
      "epoch": 6.9011378749815275,
      "grad_norm": 1.0690587759017944,
      "learning_rate": 1.5495049504950496e-05,
      "loss": 0.8112,
      "step": 46700
    },
    {
      "epoch": 6.9159154721442295,
      "grad_norm": 1.246631145477295,
      "learning_rate": 1.542116151913699e-05,
      "loss": 0.7983,
      "step": 46800
    },
    {
      "epoch": 6.930693069306931,
      "grad_norm": 2.923959970474243,
      "learning_rate": 1.5347273533323484e-05,
      "loss": 0.8182,
      "step": 46900
    },
    {
      "epoch": 6.945470666469632,
      "grad_norm": 1.3918741941452026,
      "learning_rate": 1.5273385547509975e-05,
      "loss": 0.8062,
      "step": 47000
    },
    {
      "epoch": 6.960248263632334,
      "grad_norm": 1.1089026927947998,
      "learning_rate": 1.519949756169647e-05,
      "loss": 0.7921,
      "step": 47100
    },
    {
      "epoch": 6.975025860795035,
      "grad_norm": 1.1422597169876099,
      "learning_rate": 1.5125609575882962e-05,
      "loss": 0.835,
      "step": 47200
    },
    {
      "epoch": 6.989803457957736,
      "grad_norm": 0.9788558483123779,
      "learning_rate": 1.5051721590069456e-05,
      "loss": 0.7878,
      "step": 47300
    },
    {
      "epoch": 7.004581055120437,
      "grad_norm": 1.442809820175171,
      "learning_rate": 1.4977833604255948e-05,
      "loss": 0.8026,
      "step": 47400
    },
    {
      "epoch": 7.019358652283139,
      "grad_norm": 0.9483773708343506,
      "learning_rate": 1.490394561844244e-05,
      "loss": 0.7906,
      "step": 47500
    },
    {
      "epoch": 7.03413624944584,
      "grad_norm": 0.9939897060394287,
      "learning_rate": 1.4830057632628935e-05,
      "loss": 0.7947,
      "step": 47600
    },
    {
      "epoch": 7.048913846608541,
      "grad_norm": 1.106546401977539,
      "learning_rate": 1.4756169646815429e-05,
      "loss": 0.8253,
      "step": 47700
    },
    {
      "epoch": 7.063691443771242,
      "grad_norm": 1.2794708013534546,
      "learning_rate": 1.4682281661001923e-05,
      "loss": 0.802,
      "step": 47800
    },
    {
      "epoch": 7.078469040933944,
      "grad_norm": 1.146602749824524,
      "learning_rate": 1.4608393675188414e-05,
      "loss": 0.7617,
      "step": 47900
    },
    {
      "epoch": 7.0932466380966455,
      "grad_norm": 0.9436234831809998,
      "learning_rate": 1.4534505689374908e-05,
      "loss": 0.7977,
      "step": 48000
    },
    {
      "epoch": 7.0932466380966455,
      "eval_loss": 0.7208129167556763,
      "eval_runtime": 72.5122,
      "eval_samples_per_second": 165.903,
      "eval_steps_per_second": 20.741,
      "step": 48000
    },
    {
      "epoch": 7.108024235259347,
      "grad_norm": 0.8547183871269226,
      "learning_rate": 1.4460617703561402e-05,
      "loss": 0.7945,
      "step": 48100
    },
    {
      "epoch": 7.122801832422049,
      "grad_norm": 1.197543978691101,
      "learning_rate": 1.4386729717747896e-05,
      "loss": 0.8169,
      "step": 48200
    },
    {
      "epoch": 7.13757942958475,
      "grad_norm": 0.8766186833381653,
      "learning_rate": 1.4312841731934387e-05,
      "loss": 0.8077,
      "step": 48300
    },
    {
      "epoch": 7.152357026747451,
      "grad_norm": 1.3616381883621216,
      "learning_rate": 1.4238953746120881e-05,
      "loss": 0.7988,
      "step": 48400
    },
    {
      "epoch": 7.167134623910152,
      "grad_norm": 0.9114241003990173,
      "learning_rate": 1.4165065760307375e-05,
      "loss": 0.7649,
      "step": 48500
    },
    {
      "epoch": 7.181912221072854,
      "grad_norm": 0.9388317465782166,
      "learning_rate": 1.409117777449387e-05,
      "loss": 0.7702,
      "step": 48600
    },
    {
      "epoch": 7.196689818235555,
      "grad_norm": 1.3361555337905884,
      "learning_rate": 1.401728978868036e-05,
      "loss": 0.8119,
      "step": 48700
    },
    {
      "epoch": 7.211467415398256,
      "grad_norm": 1.1780911684036255,
      "learning_rate": 1.3943401802866854e-05,
      "loss": 0.779,
      "step": 48800
    },
    {
      "epoch": 7.226245012560957,
      "grad_norm": 1.1181269884109497,
      "learning_rate": 1.3869513817053348e-05,
      "loss": 0.7964,
      "step": 48900
    },
    {
      "epoch": 7.241022609723659,
      "grad_norm": 0.8671611547470093,
      "learning_rate": 1.3795625831239842e-05,
      "loss": 0.7487,
      "step": 49000
    },
    {
      "epoch": 7.25580020688636,
      "grad_norm": 1.0266966819763184,
      "learning_rate": 1.3721737845426336e-05,
      "loss": 0.8105,
      "step": 49100
    },
    {
      "epoch": 7.270577804049061,
      "grad_norm": 0.9711130857467651,
      "learning_rate": 1.3647849859612827e-05,
      "loss": 0.7802,
      "step": 49200
    },
    {
      "epoch": 7.285355401211763,
      "grad_norm": 0.874057948589325,
      "learning_rate": 1.3573961873799321e-05,
      "loss": 0.7794,
      "step": 49300
    },
    {
      "epoch": 7.3001329983744645,
      "grad_norm": 1.0853487253189087,
      "learning_rate": 1.3500073887985815e-05,
      "loss": 0.7777,
      "step": 49400
    },
    {
      "epoch": 7.314910595537166,
      "grad_norm": 1.1887248754501343,
      "learning_rate": 1.3426185902172308e-05,
      "loss": 0.7601,
      "step": 49500
    },
    {
      "epoch": 7.329688192699867,
      "grad_norm": 0.7319579720497131,
      "learning_rate": 1.33522979163588e-05,
      "loss": 0.7935,
      "step": 49600
    },
    {
      "epoch": 7.344465789862569,
      "grad_norm": 1.2673819065093994,
      "learning_rate": 1.3278409930545294e-05,
      "loss": 0.7645,
      "step": 49700
    },
    {
      "epoch": 7.35924338702527,
      "grad_norm": 1.183774709701538,
      "learning_rate": 1.3204521944731787e-05,
      "loss": 0.7813,
      "step": 49800
    },
    {
      "epoch": 7.374020984187971,
      "grad_norm": 0.820580780506134,
      "learning_rate": 1.3130633958918281e-05,
      "loss": 0.7827,
      "step": 49900
    },
    {
      "epoch": 7.388798581350672,
      "grad_norm": 1.1409820318222046,
      "learning_rate": 1.3056745973104772e-05,
      "loss": 0.8261,
      "step": 50000
    },
    {
      "epoch": 7.388798581350672,
      "eval_loss": 0.7121010422706604,
      "eval_runtime": 72.4364,
      "eval_samples_per_second": 166.077,
      "eval_steps_per_second": 20.763,
      "step": 50000
    },
    {
      "epoch": 7.403576178513374,
      "grad_norm": 1.100785255432129,
      "learning_rate": 1.2982857987291266e-05,
      "loss": 0.7648,
      "step": 50100
    },
    {
      "epoch": 7.418353775676075,
      "grad_norm": 1.2049427032470703,
      "learning_rate": 1.290897000147776e-05,
      "loss": 0.799,
      "step": 50200
    },
    {
      "epoch": 7.433131372838776,
      "grad_norm": 0.9835713505744934,
      "learning_rate": 1.2835082015664254e-05,
      "loss": 0.7553,
      "step": 50300
    },
    {
      "epoch": 7.447908970001478,
      "grad_norm": 0.971498429775238,
      "learning_rate": 1.2761194029850748e-05,
      "loss": 0.8114,
      "step": 50400
    },
    {
      "epoch": 7.462686567164179,
      "grad_norm": 1.097946286201477,
      "learning_rate": 1.2687306044037239e-05,
      "loss": 0.7806,
      "step": 50500
    },
    {
      "epoch": 7.47746416432688,
      "grad_norm": 1.2505651712417603,
      "learning_rate": 1.2613418058223733e-05,
      "loss": 0.7631,
      "step": 50600
    },
    {
      "epoch": 7.4922417614895815,
      "grad_norm": 1.3169161081314087,
      "learning_rate": 1.2539530072410227e-05,
      "loss": 0.7543,
      "step": 50700
    },
    {
      "epoch": 7.507019358652283,
      "grad_norm": 1.203857421875,
      "learning_rate": 1.246564208659672e-05,
      "loss": 0.7793,
      "step": 50800
    },
    {
      "epoch": 7.521796955814985,
      "grad_norm": 1.2993495464324951,
      "learning_rate": 1.2391754100783214e-05,
      "loss": 0.7823,
      "step": 50900
    },
    {
      "epoch": 7.536574552977686,
      "grad_norm": 1.096360683441162,
      "learning_rate": 1.2317866114969706e-05,
      "loss": 0.8035,
      "step": 51000
    },
    {
      "epoch": 7.551352150140387,
      "grad_norm": 1.1167834997177124,
      "learning_rate": 1.22439781291562e-05,
      "loss": 0.7983,
      "step": 51100
    },
    {
      "epoch": 7.566129747303089,
      "grad_norm": 1.491892695426941,
      "learning_rate": 1.2170090143342693e-05,
      "loss": 0.772,
      "step": 51200
    },
    {
      "epoch": 7.58090734446579,
      "grad_norm": 1.5185718536376953,
      "learning_rate": 1.2096202157529187e-05,
      "loss": 0.8119,
      "step": 51300
    },
    {
      "epoch": 7.595684941628491,
      "grad_norm": 0.8462938070297241,
      "learning_rate": 1.202231417171568e-05,
      "loss": 0.7932,
      "step": 51400
    },
    {
      "epoch": 7.610462538791192,
      "grad_norm": 0.9542213082313538,
      "learning_rate": 1.1948426185902173e-05,
      "loss": 0.8099,
      "step": 51500
    },
    {
      "epoch": 7.625240135953894,
      "grad_norm": 1.3316068649291992,
      "learning_rate": 1.1874538200088667e-05,
      "loss": 0.786,
      "step": 51600
    },
    {
      "epoch": 7.640017733116595,
      "grad_norm": 1.1135494709014893,
      "learning_rate": 1.180065021427516e-05,
      "loss": 0.7913,
      "step": 51700
    },
    {
      "epoch": 7.654795330279296,
      "grad_norm": 1.036075234413147,
      "learning_rate": 1.1726762228461654e-05,
      "loss": 0.7367,
      "step": 51800
    },
    {
      "epoch": 7.6695729274419975,
      "grad_norm": 1.32742178440094,
      "learning_rate": 1.1652874242648146e-05,
      "loss": 0.7893,
      "step": 51900
    },
    {
      "epoch": 7.6843505246046995,
      "grad_norm": 1.062751054763794,
      "learning_rate": 1.1578986256834639e-05,
      "loss": 0.7722,
      "step": 52000
    },
    {
      "epoch": 7.6843505246046995,
      "eval_loss": 0.7084283828735352,
      "eval_runtime": 72.3605,
      "eval_samples_per_second": 166.251,
      "eval_steps_per_second": 20.785,
      "step": 52000
    },
    {
      "epoch": 7.699128121767401,
      "grad_norm": 0.9519420266151428,
      "learning_rate": 1.1505098271021133e-05,
      "loss": 0.8017,
      "step": 52100
    },
    {
      "epoch": 7.713905718930102,
      "grad_norm": 0.8694078922271729,
      "learning_rate": 1.1431210285207625e-05,
      "loss": 0.7915,
      "step": 52200
    },
    {
      "epoch": 7.728683316092804,
      "grad_norm": 1.0062812566757202,
      "learning_rate": 1.1357322299394118e-05,
      "loss": 0.7685,
      "step": 52300
    },
    {
      "epoch": 7.743460913255505,
      "grad_norm": 1.0483721494674683,
      "learning_rate": 1.1283434313580612e-05,
      "loss": 0.8079,
      "step": 52400
    },
    {
      "epoch": 7.758238510418206,
      "grad_norm": 0.9334209561347961,
      "learning_rate": 1.1209546327767104e-05,
      "loss": 0.7768,
      "step": 52500
    },
    {
      "epoch": 7.773016107580907,
      "grad_norm": 1.0387979745864868,
      "learning_rate": 1.1135658341953599e-05,
      "loss": 0.77,
      "step": 52600
    },
    {
      "epoch": 7.787793704743609,
      "grad_norm": 1.088147759437561,
      "learning_rate": 1.1061770356140091e-05,
      "loss": 0.786,
      "step": 52700
    },
    {
      "epoch": 7.80257130190631,
      "grad_norm": 1.1242510080337524,
      "learning_rate": 1.0987882370326585e-05,
      "loss": 0.7634,
      "step": 52800
    },
    {
      "epoch": 7.817348899069011,
      "grad_norm": 1.4115359783172607,
      "learning_rate": 1.091399438451308e-05,
      "loss": 0.7796,
      "step": 52900
    },
    {
      "epoch": 7.832126496231712,
      "grad_norm": 1.251914620399475,
      "learning_rate": 1.0840106398699572e-05,
      "loss": 0.7908,
      "step": 53000
    },
    {
      "epoch": 7.846904093394414,
      "grad_norm": 0.9731045365333557,
      "learning_rate": 1.0766218412886066e-05,
      "loss": 0.7839,
      "step": 53100
    },
    {
      "epoch": 7.861681690557115,
      "grad_norm": 1.2337440252304077,
      "learning_rate": 1.0692330427072558e-05,
      "loss": 0.7895,
      "step": 53200
    },
    {
      "epoch": 7.8764592877198165,
      "grad_norm": 1.087249755859375,
      "learning_rate": 1.0618442441259052e-05,
      "loss": 0.7831,
      "step": 53300
    },
    {
      "epoch": 7.8912368848825185,
      "grad_norm": 1.374959111213684,
      "learning_rate": 1.0544554455445545e-05,
      "loss": 0.7616,
      "step": 53400
    },
    {
      "epoch": 7.90601448204522,
      "grad_norm": 0.7273999452590942,
      "learning_rate": 1.0470666469632039e-05,
      "loss": 0.7851,
      "step": 53500
    },
    {
      "epoch": 7.920792079207921,
      "grad_norm": 1.4148845672607422,
      "learning_rate": 1.0396778483818531e-05,
      "loss": 0.7637,
      "step": 53600
    },
    {
      "epoch": 7.935569676370622,
      "grad_norm": 1.656863808631897,
      "learning_rate": 1.0322890498005025e-05,
      "loss": 0.7536,
      "step": 53700
    },
    {
      "epoch": 7.950347273533324,
      "grad_norm": 1.1790313720703125,
      "learning_rate": 1.0249002512191518e-05,
      "loss": 0.798,
      "step": 53800
    },
    {
      "epoch": 7.965124870696025,
      "grad_norm": 1.0199707746505737,
      "learning_rate": 1.0175114526378012e-05,
      "loss": 0.8305,
      "step": 53900
    },
    {
      "epoch": 7.979902467858726,
      "grad_norm": 1.0051114559173584,
      "learning_rate": 1.0101226540564504e-05,
      "loss": 0.7621,
      "step": 54000
    },
    {
      "epoch": 7.979902467858726,
      "eval_loss": 0.7050982713699341,
      "eval_runtime": 75.0872,
      "eval_samples_per_second": 160.214,
      "eval_steps_per_second": 20.03,
      "step": 54000
    },
    {
      "epoch": 7.994680065021427,
      "grad_norm": 1.251753568649292,
      "learning_rate": 1.0027338554750998e-05,
      "loss": 0.7895,
      "step": 54100
    },
    {
      "epoch": 8.00945766218413,
      "grad_norm": 0.8952770829200745,
      "learning_rate": 9.953450568937493e-06,
      "loss": 0.7886,
      "step": 54200
    },
    {
      "epoch": 8.02423525934683,
      "grad_norm": 1.0824029445648193,
      "learning_rate": 9.879562583123985e-06,
      "loss": 0.7695,
      "step": 54300
    },
    {
      "epoch": 8.039012856509531,
      "grad_norm": 1.142757534980774,
      "learning_rate": 9.805674597310477e-06,
      "loss": 0.7578,
      "step": 54400
    },
    {
      "epoch": 8.053790453672233,
      "grad_norm": 1.550201177597046,
      "learning_rate": 9.731786611496972e-06,
      "loss": 0.7785,
      "step": 54500
    },
    {
      "epoch": 8.068568050834934,
      "grad_norm": 0.9064803123474121,
      "learning_rate": 9.657898625683464e-06,
      "loss": 0.7773,
      "step": 54600
    },
    {
      "epoch": 8.083345647997636,
      "grad_norm": 0.7882472276687622,
      "learning_rate": 9.584010639869956e-06,
      "loss": 0.7698,
      "step": 54700
    },
    {
      "epoch": 8.098123245160338,
      "grad_norm": 0.8124865889549255,
      "learning_rate": 9.51012265405645e-06,
      "loss": 0.7751,
      "step": 54800
    },
    {
      "epoch": 8.112900842323038,
      "grad_norm": 1.0482573509216309,
      "learning_rate": 9.436234668242943e-06,
      "loss": 0.7722,
      "step": 54900
    },
    {
      "epoch": 8.12767843948574,
      "grad_norm": 1.315589189529419,
      "learning_rate": 9.362346682429437e-06,
      "loss": 0.7781,
      "step": 55000
    },
    {
      "epoch": 8.142456036648442,
      "grad_norm": 1.1919496059417725,
      "learning_rate": 9.28845869661593e-06,
      "loss": 0.7519,
      "step": 55100
    },
    {
      "epoch": 8.157233633811142,
      "grad_norm": 0.877117931842804,
      "learning_rate": 9.214570710802424e-06,
      "loss": 0.7996,
      "step": 55200
    },
    {
      "epoch": 8.172011230973844,
      "grad_norm": 1.0323997735977173,
      "learning_rate": 9.140682724988918e-06,
      "loss": 0.7809,
      "step": 55300
    },
    {
      "epoch": 8.186788828136544,
      "grad_norm": 1.2436952590942383,
      "learning_rate": 9.06679473917541e-06,
      "loss": 0.796,
      "step": 55400
    },
    {
      "epoch": 8.201566425299246,
      "grad_norm": 1.1048754453659058,
      "learning_rate": 8.992906753361904e-06,
      "loss": 0.7762,
      "step": 55500
    },
    {
      "epoch": 8.216344022461948,
      "grad_norm": 1.3379461765289307,
      "learning_rate": 8.919018767548397e-06,
      "loss": 0.7511,
      "step": 55600
    },
    {
      "epoch": 8.231121619624648,
      "grad_norm": 1.0962765216827393,
      "learning_rate": 8.845130781734891e-06,
      "loss": 0.7543,
      "step": 55700
    },
    {
      "epoch": 8.24589921678735,
      "grad_norm": 0.834069550037384,
      "learning_rate": 8.771242795921383e-06,
      "loss": 0.7883,
      "step": 55800
    },
    {
      "epoch": 8.260676813950052,
      "grad_norm": 1.203149676322937,
      "learning_rate": 8.697354810107877e-06,
      "loss": 0.7982,
      "step": 55900
    },
    {
      "epoch": 8.275454411112753,
      "grad_norm": 1.1363669633865356,
      "learning_rate": 8.62346682429437e-06,
      "loss": 0.7766,
      "step": 56000
    },
    {
      "epoch": 8.275454411112753,
      "eval_loss": 0.7020719051361084,
      "eval_runtime": 72.4495,
      "eval_samples_per_second": 166.047,
      "eval_steps_per_second": 20.759,
      "step": 56000
    },
    {
      "epoch": 8.290232008275455,
      "grad_norm": 1.0760259628295898,
      "learning_rate": 8.549578838480864e-06,
      "loss": 0.7431,
      "step": 56100
    },
    {
      "epoch": 8.305009605438157,
      "grad_norm": 0.9660651087760925,
      "learning_rate": 8.475690852667356e-06,
      "loss": 0.7766,
      "step": 56200
    },
    {
      "epoch": 8.319787202600857,
      "grad_norm": 1.0129132270812988,
      "learning_rate": 8.40180286685385e-06,
      "loss": 0.7761,
      "step": 56300
    },
    {
      "epoch": 8.334564799763559,
      "grad_norm": 1.0567796230316162,
      "learning_rate": 8.327914881040343e-06,
      "loss": 0.7763,
      "step": 56400
    },
    {
      "epoch": 8.349342396926259,
      "grad_norm": 1.195173978805542,
      "learning_rate": 8.254026895226837e-06,
      "loss": 0.7361,
      "step": 56500
    },
    {
      "epoch": 8.364119994088961,
      "grad_norm": 1.2322102785110474,
      "learning_rate": 8.180138909413331e-06,
      "loss": 0.7854,
      "step": 56600
    },
    {
      "epoch": 8.378897591251663,
      "grad_norm": 1.130596399307251,
      "learning_rate": 8.106250923599824e-06,
      "loss": 0.7907,
      "step": 56700
    },
    {
      "epoch": 8.393675188414363,
      "grad_norm": 0.9165513515472412,
      "learning_rate": 8.032362937786318e-06,
      "loss": 0.7497,
      "step": 56800
    },
    {
      "epoch": 8.408452785577065,
      "grad_norm": 1.0680221319198608,
      "learning_rate": 7.95847495197281e-06,
      "loss": 0.7692,
      "step": 56900
    },
    {
      "epoch": 8.423230382739767,
      "grad_norm": 0.9993378520011902,
      "learning_rate": 7.884586966159303e-06,
      "loss": 0.7634,
      "step": 57000
    },
    {
      "epoch": 8.438007979902467,
      "grad_norm": 1.8354454040527344,
      "learning_rate": 7.810698980345797e-06,
      "loss": 0.7377,
      "step": 57100
    },
    {
      "epoch": 8.45278557706517,
      "grad_norm": 0.8440549969673157,
      "learning_rate": 7.736810994532289e-06,
      "loss": 0.7954,
      "step": 57200
    },
    {
      "epoch": 8.46756317422787,
      "grad_norm": 1.1117643117904663,
      "learning_rate": 7.662923008718782e-06,
      "loss": 0.7475,
      "step": 57300
    },
    {
      "epoch": 8.482340771390572,
      "grad_norm": 1.2579658031463623,
      "learning_rate": 7.5890350229052765e-06,
      "loss": 0.7718,
      "step": 57400
    },
    {
      "epoch": 8.497118368553274,
      "grad_norm": 1.0592985153198242,
      "learning_rate": 7.515147037091769e-06,
      "loss": 0.7555,
      "step": 57500
    },
    {
      "epoch": 8.511895965715974,
      "grad_norm": 1.5115058422088623,
      "learning_rate": 7.441259051278262e-06,
      "loss": 0.762,
      "step": 57600
    },
    {
      "epoch": 8.526673562878676,
      "grad_norm": 1.0235204696655273,
      "learning_rate": 7.3673710654647555e-06,
      "loss": 0.757,
      "step": 57700
    },
    {
      "epoch": 8.541451160041378,
      "grad_norm": 1.0006027221679688,
      "learning_rate": 7.293483079651249e-06,
      "loss": 0.7692,
      "step": 57800
    },
    {
      "epoch": 8.556228757204078,
      "grad_norm": 0.939378023147583,
      "learning_rate": 7.219595093837743e-06,
      "loss": 0.7558,
      "step": 57900
    },
    {
      "epoch": 8.57100635436678,
      "grad_norm": 1.028351902961731,
      "learning_rate": 7.145707108024235e-06,
      "loss": 0.7437,
      "step": 58000
    },
    {
      "epoch": 8.57100635436678,
      "eval_loss": 0.7008432149887085,
      "eval_runtime": 72.4316,
      "eval_samples_per_second": 166.088,
      "eval_steps_per_second": 20.764,
      "step": 58000
    },
    {
      "epoch": 8.585783951529482,
      "grad_norm": 0.9856190085411072,
      "learning_rate": 7.071819122210729e-06,
      "loss": 0.7736,
      "step": 58100
    },
    {
      "epoch": 8.600561548692182,
      "grad_norm": 1.4158014059066772,
      "learning_rate": 6.997931136397222e-06,
      "loss": 0.7846,
      "step": 58200
    },
    {
      "epoch": 8.615339145854884,
      "grad_norm": 0.6742991209030151,
      "learning_rate": 6.924043150583716e-06,
      "loss": 0.759,
      "step": 58300
    },
    {
      "epoch": 8.630116743017584,
      "grad_norm": 0.8981838822364807,
      "learning_rate": 6.850155164770208e-06,
      "loss": 0.7544,
      "step": 58400
    },
    {
      "epoch": 8.644894340180286,
      "grad_norm": 0.989539384841919,
      "learning_rate": 6.7762671789567025e-06,
      "loss": 0.7576,
      "step": 58500
    },
    {
      "epoch": 8.659671937342988,
      "grad_norm": 1.3635807037353516,
      "learning_rate": 6.702379193143195e-06,
      "loss": 0.7611,
      "step": 58600
    },
    {
      "epoch": 8.674449534505689,
      "grad_norm": 0.9892542958259583,
      "learning_rate": 6.628491207329689e-06,
      "loss": 0.7856,
      "step": 58700
    },
    {
      "epoch": 8.68922713166839,
      "grad_norm": 1.8277175426483154,
      "learning_rate": 6.5546032215161815e-06,
      "loss": 0.7435,
      "step": 58800
    },
    {
      "epoch": 8.704004728831093,
      "grad_norm": 1.0886543989181519,
      "learning_rate": 6.480715235702675e-06,
      "loss": 0.7683,
      "step": 58900
    },
    {
      "epoch": 8.718782325993793,
      "grad_norm": 1.4411239624023438,
      "learning_rate": 6.406827249889168e-06,
      "loss": 0.7753,
      "step": 59000
    },
    {
      "epoch": 8.733559923156495,
      "grad_norm": 0.8278260827064514,
      "learning_rate": 6.332939264075661e-06,
      "loss": 0.7675,
      "step": 59100
    },
    {
      "epoch": 8.748337520319197,
      "grad_norm": 1.6789895296096802,
      "learning_rate": 6.2590512782621554e-06,
      "loss": 0.7698,
      "step": 59200
    },
    {
      "epoch": 8.763115117481897,
      "grad_norm": 0.7682338356971741,
      "learning_rate": 6.185163292448648e-06,
      "loss": 0.7601,
      "step": 59300
    },
    {
      "epoch": 8.777892714644599,
      "grad_norm": 1.2123130559921265,
      "learning_rate": 6.111275306635141e-06,
      "loss": 0.7677,
      "step": 59400
    },
    {
      "epoch": 8.7926703118073,
      "grad_norm": 1.2911198139190674,
      "learning_rate": 6.037387320821634e-06,
      "loss": 0.7824,
      "step": 59500
    },
    {
      "epoch": 8.807447908970001,
      "grad_norm": 1.018477439880371,
      "learning_rate": 5.963499335008128e-06,
      "loss": 0.772,
      "step": 59600
    },
    {
      "epoch": 8.822225506132703,
      "grad_norm": 1.0662809610366821,
      "learning_rate": 5.889611349194622e-06,
      "loss": 0.7671,
      "step": 59700
    },
    {
      "epoch": 8.837003103295404,
      "grad_norm": 0.9827556014060974,
      "learning_rate": 5.815723363381115e-06,
      "loss": 0.7636,
      "step": 59800
    },
    {
      "epoch": 8.851780700458105,
      "grad_norm": 1.6825789213180542,
      "learning_rate": 5.741835377567608e-06,
      "loss": 0.7555,
      "step": 59900
    },
    {
      "epoch": 8.866558297620807,
      "grad_norm": 1.0655018091201782,
      "learning_rate": 5.667947391754102e-06,
      "loss": 0.7464,
      "step": 60000
    },
    {
      "epoch": 8.866558297620807,
      "eval_loss": 0.7005761861801147,
      "eval_runtime": 72.5398,
      "eval_samples_per_second": 165.84,
      "eval_steps_per_second": 20.733,
      "step": 60000
    },
    {
      "epoch": 8.881335894783508,
      "grad_norm": 1.029038667678833,
      "learning_rate": 5.594059405940594e-06,
      "loss": 0.7366,
      "step": 60100
    },
    {
      "epoch": 8.89611349194621,
      "grad_norm": 0.8582794666290283,
      "learning_rate": 5.520171420127087e-06,
      "loss": 0.7787,
      "step": 60200
    },
    {
      "epoch": 8.910891089108912,
      "grad_norm": 1.1154043674468994,
      "learning_rate": 5.446283434313581e-06,
      "loss": 0.7512,
      "step": 60300
    },
    {
      "epoch": 8.925668686271612,
      "grad_norm": 1.1245428323745728,
      "learning_rate": 5.372395448500074e-06,
      "loss": 0.7753,
      "step": 60400
    },
    {
      "epoch": 8.940446283434314,
      "grad_norm": 1.2785496711730957,
      "learning_rate": 5.298507462686567e-06,
      "loss": 0.7516,
      "step": 60500
    },
    {
      "epoch": 8.955223880597014,
      "grad_norm": 1.053262710571289,
      "learning_rate": 5.22461947687306e-06,
      "loss": 0.7565,
      "step": 60600
    },
    {
      "epoch": 8.970001477759716,
      "grad_norm": 0.9974027276039124,
      "learning_rate": 5.150731491059554e-06,
      "loss": 0.7461,
      "step": 60700
    },
    {
      "epoch": 8.984779074922418,
      "grad_norm": 1.3934612274169922,
      "learning_rate": 5.076843505246047e-06,
      "loss": 0.7839,
      "step": 60800
    },
    {
      "epoch": 8.999556672085118,
      "grad_norm": 1.1922601461410522,
      "learning_rate": 5.00295551943254e-06,
      "loss": 0.7725,
      "step": 60900
    },
    {
      "epoch": 9.01433426924782,
      "grad_norm": 0.9864547252655029,
      "learning_rate": 4.929067533619034e-06,
      "loss": 0.7547,
      "step": 61000
    },
    {
      "epoch": 9.029111866410522,
      "grad_norm": 0.8302077054977417,
      "learning_rate": 4.855179547805528e-06,
      "loss": 0.7597,
      "step": 61100
    },
    {
      "epoch": 9.043889463573223,
      "grad_norm": 0.8302301168441772,
      "learning_rate": 4.781291561992021e-06,
      "loss": 0.7283,
      "step": 61200
    },
    {
      "epoch": 9.058667060735925,
      "grad_norm": 1.2909250259399414,
      "learning_rate": 4.707403576178513e-06,
      "loss": 0.7592,
      "step": 61300
    },
    {
      "epoch": 9.073444657898627,
      "grad_norm": 1.0657316446304321,
      "learning_rate": 4.633515590365007e-06,
      "loss": 0.7467,
      "step": 61400
    },
    {
      "epoch": 9.088222255061327,
      "grad_norm": 1.2328715324401855,
      "learning_rate": 4.5596276045515e-06,
      "loss": 0.7575,
      "step": 61500
    },
    {
      "epoch": 9.102999852224029,
      "grad_norm": 1.3215532302856445,
      "learning_rate": 4.485739618737993e-06,
      "loss": 0.79,
      "step": 61600
    },
    {
      "epoch": 9.117777449386729,
      "grad_norm": 1.175755500793457,
      "learning_rate": 4.4118516329244864e-06,
      "loss": 0.7455,
      "step": 61700
    },
    {
      "epoch": 9.132555046549431,
      "grad_norm": 0.9539268612861633,
      "learning_rate": 4.33796364711098e-06,
      "loss": 0.7664,
      "step": 61800
    },
    {
      "epoch": 9.147332643712133,
      "grad_norm": 1.0357825756072998,
      "learning_rate": 4.264075661297473e-06,
      "loss": 0.7523,
      "step": 61900
    },
    {
      "epoch": 9.162110240874833,
      "grad_norm": 0.7833012938499451,
      "learning_rate": 4.190187675483966e-06,
      "loss": 0.7578,
      "step": 62000
    },
    {
      "epoch": 9.162110240874833,
      "eval_loss": 0.6973044276237488,
      "eval_runtime": 72.4643,
      "eval_samples_per_second": 166.013,
      "eval_steps_per_second": 20.755,
      "step": 62000
    },
    {
      "epoch": 9.176887838037535,
      "grad_norm": 0.9160245656967163,
      "learning_rate": 4.1162996896704595e-06,
      "loss": 0.739,
      "step": 62100
    },
    {
      "epoch": 9.191665435200237,
      "grad_norm": 0.8298971652984619,
      "learning_rate": 4.042411703856954e-06,
      "loss": 0.7669,
      "step": 62200
    },
    {
      "epoch": 9.206443032362937,
      "grad_norm": 0.9877044558525085,
      "learning_rate": 3.968523718043447e-06,
      "loss": 0.7957,
      "step": 62300
    },
    {
      "epoch": 9.22122062952564,
      "grad_norm": 1.0568028688430786,
      "learning_rate": 3.89463573222994e-06,
      "loss": 0.7361,
      "step": 62400
    },
    {
      "epoch": 9.235998226688341,
      "grad_norm": 0.8714159727096558,
      "learning_rate": 3.8207477464164335e-06,
      "loss": 0.7608,
      "step": 62500
    },
    {
      "epoch": 9.250775823851042,
      "grad_norm": 1.3602731227874756,
      "learning_rate": 3.7468597606029263e-06,
      "loss": 0.7822,
      "step": 62600
    },
    {
      "epoch": 9.265553421013744,
      "grad_norm": 0.7987516522407532,
      "learning_rate": 3.6729717747894196e-06,
      "loss": 0.7353,
      "step": 62700
    },
    {
      "epoch": 9.280331018176444,
      "grad_norm": 0.8805444240570068,
      "learning_rate": 3.599083788975913e-06,
      "loss": 0.7783,
      "step": 62800
    },
    {
      "epoch": 9.295108615339146,
      "grad_norm": 1.1555825471878052,
      "learning_rate": 3.5251958031624057e-06,
      "loss": 0.7731,
      "step": 62900
    },
    {
      "epoch": 9.309886212501848,
      "grad_norm": 3.4929442405700684,
      "learning_rate": 3.451307817348899e-06,
      "loss": 0.7581,
      "step": 63000
    },
    {
      "epoch": 9.324663809664548,
      "grad_norm": 1.1889504194259644,
      "learning_rate": 3.3774198315353923e-06,
      "loss": 0.7373,
      "step": 63100
    },
    {
      "epoch": 9.33944140682725,
      "grad_norm": 1.6927738189697266,
      "learning_rate": 3.3035318457218855e-06,
      "loss": 0.7671,
      "step": 63200
    },
    {
      "epoch": 9.354219003989952,
      "grad_norm": 1.0694615840911865,
      "learning_rate": 3.229643859908379e-06,
      "loss": 0.7499,
      "step": 63300
    },
    {
      "epoch": 9.368996601152652,
      "grad_norm": 1.0204046964645386,
      "learning_rate": 3.155755874094872e-06,
      "loss": 0.7658,
      "step": 63400
    },
    {
      "epoch": 9.383774198315354,
      "grad_norm": 1.1983630657196045,
      "learning_rate": 3.0818678882813654e-06,
      "loss": 0.7639,
      "step": 63500
    },
    {
      "epoch": 9.398551795478054,
      "grad_norm": 1.3255252838134766,
      "learning_rate": 3.0079799024678586e-06,
      "loss": 0.7375,
      "step": 63600
    },
    {
      "epoch": 9.413329392640756,
      "grad_norm": 0.8119337558746338,
      "learning_rate": 2.934091916654352e-06,
      "loss": 0.7451,
      "step": 63700
    },
    {
      "epoch": 9.428106989803458,
      "grad_norm": 1.0828917026519775,
      "learning_rate": 2.8602039308408456e-06,
      "loss": 0.7762,
      "step": 63800
    },
    {
      "epoch": 9.442884586966159,
      "grad_norm": 0.9042173624038696,
      "learning_rate": 2.786315945027339e-06,
      "loss": 0.7332,
      "step": 63900
    },
    {
      "epoch": 9.45766218412886,
      "grad_norm": 1.0344789028167725,
      "learning_rate": 2.712427959213832e-06,
      "loss": 0.7783,
      "step": 64000
    },
    {
      "epoch": 9.45766218412886,
      "eval_loss": 0.6960455775260925,
      "eval_runtime": 73.9821,
      "eval_samples_per_second": 162.607,
      "eval_steps_per_second": 20.329,
      "step": 64000
    },
    {
      "epoch": 9.472439781291563,
      "grad_norm": 1.1710675954818726,
      "learning_rate": 2.6385399734003254e-06,
      "loss": 0.7321,
      "step": 64100
    },
    {
      "epoch": 9.487217378454263,
      "grad_norm": 0.9754855036735535,
      "learning_rate": 2.5646519875868183e-06,
      "loss": 0.7796,
      "step": 64200
    },
    {
      "epoch": 9.501994975616965,
      "grad_norm": 1.0703096389770508,
      "learning_rate": 2.4907640017733115e-06,
      "loss": 0.7662,
      "step": 64300
    },
    {
      "epoch": 9.516772572779667,
      "grad_norm": 0.6832783818244934,
      "learning_rate": 2.4168760159598052e-06,
      "loss": 0.747,
      "step": 64400
    },
    {
      "epoch": 9.531550169942367,
      "grad_norm": 1.0695381164550781,
      "learning_rate": 2.3429880301462985e-06,
      "loss": 0.7193,
      "step": 64500
    },
    {
      "epoch": 9.546327767105069,
      "grad_norm": 1.0288081169128418,
      "learning_rate": 2.269100044332792e-06,
      "loss": 0.7777,
      "step": 64600
    },
    {
      "epoch": 9.56110536426777,
      "grad_norm": 1.1068298816680908,
      "learning_rate": 2.195212058519285e-06,
      "loss": 0.7731,
      "step": 64700
    },
    {
      "epoch": 9.575882961430471,
      "grad_norm": 1.1885273456573486,
      "learning_rate": 2.121324072705778e-06,
      "loss": 0.7625,
      "step": 64800
    },
    {
      "epoch": 9.590660558593173,
      "grad_norm": 1.3579301834106445,
      "learning_rate": 2.047436086892271e-06,
      "loss": 0.7435,
      "step": 64900
    },
    {
      "epoch": 9.605438155755873,
      "grad_norm": 1.252610206604004,
      "learning_rate": 1.9735481010787645e-06,
      "loss": 0.7486,
      "step": 65000
    },
    {
      "epoch": 9.620215752918575,
      "grad_norm": 1.023546576499939,
      "learning_rate": 1.8996601152652582e-06,
      "loss": 0.7396,
      "step": 65100
    },
    {
      "epoch": 9.634993350081277,
      "grad_norm": 1.0296648740768433,
      "learning_rate": 1.8257721294517512e-06,
      "loss": 0.7358,
      "step": 65200
    },
    {
      "epoch": 9.649770947243978,
      "grad_norm": 1.2118301391601562,
      "learning_rate": 1.7518841436382445e-06,
      "loss": 0.7413,
      "step": 65300
    },
    {
      "epoch": 9.66454854440668,
      "grad_norm": 1.071557641029358,
      "learning_rate": 1.6779961578247378e-06,
      "loss": 0.7452,
      "step": 65400
    },
    {
      "epoch": 9.679326141569382,
      "grad_norm": 1.1228612661361694,
      "learning_rate": 1.604108172011231e-06,
      "loss": 0.7456,
      "step": 65500
    },
    {
      "epoch": 9.694103738732082,
      "grad_norm": 1.217137098312378,
      "learning_rate": 1.5302201861977243e-06,
      "loss": 0.7626,
      "step": 65600
    },
    {
      "epoch": 9.708881335894784,
      "grad_norm": 0.7823166251182556,
      "learning_rate": 1.4563322003842176e-06,
      "loss": 0.7842,
      "step": 65700
    },
    {
      "epoch": 9.723658933057484,
      "grad_norm": 1.3990037441253662,
      "learning_rate": 1.3824442145707109e-06,
      "loss": 0.7648,
      "step": 65800
    },
    {
      "epoch": 9.738436530220186,
      "grad_norm": 1.0547791719436646,
      "learning_rate": 1.3085562287572041e-06,
      "loss": 0.7901,
      "step": 65900
    },
    {
      "epoch": 9.753214127382888,
      "grad_norm": 0.7283841371536255,
      "learning_rate": 1.2346682429436974e-06,
      "loss": 0.712,
      "step": 66000
    },
    {
      "epoch": 9.753214127382888,
      "eval_loss": 0.696337103843689,
      "eval_runtime": 72.9919,
      "eval_samples_per_second": 164.813,
      "eval_steps_per_second": 20.605,
      "step": 66000
    },
    {
      "epoch": 9.767991724545588,
      "grad_norm": 0.7546657919883728,
      "learning_rate": 1.1607802571301907e-06,
      "loss": 0.7432,
      "step": 66100
    },
    {
      "epoch": 9.78276932170829,
      "grad_norm": 1.4157286882400513,
      "learning_rate": 1.086892271316684e-06,
      "loss": 0.7613,
      "step": 66200
    },
    {
      "epoch": 9.797546918870992,
      "grad_norm": 0.8676655292510986,
      "learning_rate": 1.0130042855031772e-06,
      "loss": 0.7492,
      "step": 66300
    },
    {
      "epoch": 9.812324516033692,
      "grad_norm": 1.133905291557312,
      "learning_rate": 9.391162996896704e-07,
      "loss": 0.7892,
      "step": 66400
    },
    {
      "epoch": 9.827102113196394,
      "grad_norm": 1.0581029653549194,
      "learning_rate": 8.652283138761638e-07,
      "loss": 0.7383,
      "step": 66500
    },
    {
      "epoch": 9.841879710359095,
      "grad_norm": 0.9542043209075928,
      "learning_rate": 7.913403280626571e-07,
      "loss": 0.7557,
      "step": 66600
    },
    {
      "epoch": 9.856657307521797,
      "grad_norm": 1.234308123588562,
      "learning_rate": 7.174523422491503e-07,
      "loss": 0.7461,
      "step": 66700
    },
    {
      "epoch": 9.871434904684499,
      "grad_norm": 1.1402132511138916,
      "learning_rate": 6.435643564356436e-07,
      "loss": 0.7279,
      "step": 66800
    },
    {
      "epoch": 9.886212501847199,
      "grad_norm": 1.3824496269226074,
      "learning_rate": 5.696763706221369e-07,
      "loss": 0.756,
      "step": 66900
    },
    {
      "epoch": 9.900990099009901,
      "grad_norm": 1.1804386377334595,
      "learning_rate": 4.957883848086301e-07,
      "loss": 0.8012,
      "step": 67000
    },
    {
      "epoch": 9.915767696172603,
      "grad_norm": 1.0919389724731445,
      "learning_rate": 4.2190039899512337e-07,
      "loss": 0.7457,
      "step": 67100
    },
    {
      "epoch": 9.930545293335303,
      "grad_norm": 0.9320476651191711,
      "learning_rate": 3.480124131816167e-07,
      "loss": 0.7442,
      "step": 67200
    },
    {
      "epoch": 9.945322890498005,
      "grad_norm": 0.8968896269798279,
      "learning_rate": 2.7412442736810997e-07,
      "loss": 0.7692,
      "step": 67300
    },
    {
      "epoch": 9.960100487660707,
      "grad_norm": 1.2830569744110107,
      "learning_rate": 2.0023644155460322e-07,
      "loss": 0.7625,
      "step": 67400
    },
    {
      "epoch": 9.974878084823407,
      "grad_norm": 1.3664661645889282,
      "learning_rate": 1.263484557410965e-07,
      "loss": 0.7871,
      "step": 67500
    },
    {
      "epoch": 9.98965568198611,
      "grad_norm": 1.3470678329467773,
      "learning_rate": 5.246046992758977e-08,
      "loss": 0.7357,
      "step": 67600
    },
    {
      "epoch": 10.00443327914881,
      "grad_norm": 0.9677627086639404,
      "learning_rate": 2.4989286242057042e-05,
      "loss": 0.7776,
      "step": 67700
    },
    {
      "epoch": 10.019210876311512,
      "grad_norm": 1.014180064201355,
      "learning_rate": 2.495234224915029e-05,
      "loss": 0.7292,
      "step": 67800
    },
    {
      "epoch": 10.033988473474214,
      "grad_norm": 1.1789072751998901,
      "learning_rate": 2.4915398256243536e-05,
      "loss": 0.7636,
      "step": 67900
    },
    {
      "epoch": 10.048766070636914,
      "grad_norm": 0.9175547361373901,
      "learning_rate": 2.4878454263336783e-05,
      "loss": 0.7776,
      "step": 68000
    },
    {
      "epoch": 10.048766070636914,
      "eval_loss": 0.6971617937088013,
      "eval_runtime": 76.4545,
      "eval_samples_per_second": 157.348,
      "eval_steps_per_second": 19.672,
      "step": 68000
    },
    {
      "epoch": 10.063543667799616,
      "grad_norm": 1.9789247512817383,
      "learning_rate": 2.484151027043003e-05,
      "loss": 0.7702,
      "step": 68100
    },
    {
      "epoch": 10.078321264962318,
      "grad_norm": 0.9365715980529785,
      "learning_rate": 2.4804566277523274e-05,
      "loss": 0.7433,
      "step": 68200
    },
    {
      "epoch": 10.093098862125018,
      "grad_norm": 1.43729829788208,
      "learning_rate": 2.4767622284616525e-05,
      "loss": 0.7899,
      "step": 68300
    },
    {
      "epoch": 10.10787645928772,
      "grad_norm": 1.0377839803695679,
      "learning_rate": 2.4730678291709768e-05,
      "loss": 0.7657,
      "step": 68400
    },
    {
      "epoch": 10.122654056450422,
      "grad_norm": 0.9341148138046265,
      "learning_rate": 2.4693734298803015e-05,
      "loss": 0.7575,
      "step": 68500
    },
    {
      "epoch": 10.137431653613122,
      "grad_norm": 0.9897372126579285,
      "learning_rate": 2.4656790305896262e-05,
      "loss": 0.7691,
      "step": 68600
    },
    {
      "epoch": 10.152209250775824,
      "grad_norm": 1.2059109210968018,
      "learning_rate": 2.461984631298951e-05,
      "loss": 0.7495,
      "step": 68700
    },
    {
      "epoch": 10.166986847938524,
      "grad_norm": 1.1957306861877441,
      "learning_rate": 2.4582902320082756e-05,
      "loss": 0.7512,
      "step": 68800
    },
    {
      "epoch": 10.181764445101226,
      "grad_norm": 1.207107663154602,
      "learning_rate": 2.4545958327176004e-05,
      "loss": 0.7926,
      "step": 68900
    },
    {
      "epoch": 10.196542042263928,
      "grad_norm": 1.140027403831482,
      "learning_rate": 2.4509014334269247e-05,
      "loss": 0.7753,
      "step": 69000
    },
    {
      "epoch": 10.211319639426629,
      "grad_norm": 1.0538620948791504,
      "learning_rate": 2.4472070341362498e-05,
      "loss": 0.7185,
      "step": 69100
    },
    {
      "epoch": 10.22609723658933,
      "grad_norm": 1.0428376197814941,
      "learning_rate": 2.443512634845574e-05,
      "loss": 0.7523,
      "step": 69200
    },
    {
      "epoch": 10.240874833752033,
      "grad_norm": 0.885168731212616,
      "learning_rate": 2.4398182355548992e-05,
      "loss": 0.7537,
      "step": 69300
    },
    {
      "epoch": 10.255652430914733,
      "grad_norm": 1.0774346590042114,
      "learning_rate": 2.4361238362642235e-05,
      "loss": 0.7273,
      "step": 69400
    },
    {
      "epoch": 10.270430028077435,
      "grad_norm": 1.47780179977417,
      "learning_rate": 2.4324294369735483e-05,
      "loss": 0.758,
      "step": 69500
    },
    {
      "epoch": 10.285207625240137,
      "grad_norm": 0.992080569267273,
      "learning_rate": 2.428735037682873e-05,
      "loss": 0.7711,
      "step": 69600
    },
    {
      "epoch": 10.299985222402837,
      "grad_norm": 0.8611345291137695,
      "learning_rate": 2.4250406383921977e-05,
      "loss": 0.7441,
      "step": 69700
    },
    {
      "epoch": 10.314762819565539,
      "grad_norm": 1.203503966331482,
      "learning_rate": 2.421346239101522e-05,
      "loss": 0.7763,
      "step": 69800
    },
    {
      "epoch": 10.32954041672824,
      "grad_norm": 0.9756231904029846,
      "learning_rate": 2.417651839810847e-05,
      "loss": 0.776,
      "step": 69900
    },
    {
      "epoch": 10.344318013890941,
      "grad_norm": 1.2120187282562256,
      "learning_rate": 2.4139574405201714e-05,
      "loss": 0.7706,
      "step": 70000
    },
    {
      "epoch": 10.344318013890941,
      "eval_loss": 0.6929514408111572,
      "eval_runtime": 72.7674,
      "eval_samples_per_second": 165.321,
      "eval_steps_per_second": 20.669,
      "step": 70000
    },
    {
      "epoch": 10.359095611053643,
      "grad_norm": 1.0240036249160767,
      "learning_rate": 2.410263041229496e-05,
      "loss": 0.741,
      "step": 70100
    },
    {
      "epoch": 10.373873208216343,
      "grad_norm": 1.0424760580062866,
      "learning_rate": 2.406568641938821e-05,
      "loss": 0.7669,
      "step": 70200
    },
    {
      "epoch": 10.388650805379045,
      "grad_norm": 0.9934984445571899,
      "learning_rate": 2.4028742426481456e-05,
      "loss": 0.7508,
      "step": 70300
    },
    {
      "epoch": 10.403428402541747,
      "grad_norm": 1.328647494316101,
      "learning_rate": 2.3991798433574703e-05,
      "loss": 0.7616,
      "step": 70400
    },
    {
      "epoch": 10.418205999704448,
      "grad_norm": 0.914870023727417,
      "learning_rate": 2.395485444066795e-05,
      "loss": 0.7197,
      "step": 70500
    },
    {
      "epoch": 10.43298359686715,
      "grad_norm": 1.004202961921692,
      "learning_rate": 2.3917910447761197e-05,
      "loss": 0.7765,
      "step": 70600
    },
    {
      "epoch": 10.447761194029852,
      "grad_norm": 0.8945374488830566,
      "learning_rate": 2.388096645485444e-05,
      "loss": 0.7645,
      "step": 70700
    },
    {
      "epoch": 10.462538791192552,
      "grad_norm": 1.1628128290176392,
      "learning_rate": 2.3844022461947688e-05,
      "loss": 0.7598,
      "step": 70800
    },
    {
      "epoch": 10.477316388355254,
      "grad_norm": 1.1523782014846802,
      "learning_rate": 2.3807078469040935e-05,
      "loss": 0.7687,
      "step": 70900
    },
    {
      "epoch": 10.492093985517954,
      "grad_norm": 0.8852828741073608,
      "learning_rate": 2.377013447613418e-05,
      "loss": 0.7508,
      "step": 71000
    },
    {
      "epoch": 10.506871582680656,
      "grad_norm": 1.3698253631591797,
      "learning_rate": 2.373319048322743e-05,
      "loss": 0.7676,
      "step": 71100
    },
    {
      "epoch": 10.521649179843358,
      "grad_norm": 1.238567590713501,
      "learning_rate": 2.3696246490320676e-05,
      "loss": 0.7204,
      "step": 71200
    },
    {
      "epoch": 10.536426777006058,
      "grad_norm": 0.8999984860420227,
      "learning_rate": 2.365930249741392e-05,
      "loss": 0.7465,
      "step": 71300
    },
    {
      "epoch": 10.55120437416876,
      "grad_norm": 0.895273745059967,
      "learning_rate": 2.362235850450717e-05,
      "loss": 0.746,
      "step": 71400
    },
    {
      "epoch": 10.565981971331462,
      "grad_norm": 0.9955183267593384,
      "learning_rate": 2.3585414511600414e-05,
      "loss": 0.7597,
      "step": 71500
    },
    {
      "epoch": 10.580759568494162,
      "grad_norm": 1.5371778011322021,
      "learning_rate": 2.354847051869366e-05,
      "loss": 0.8098,
      "step": 71600
    },
    {
      "epoch": 10.595537165656864,
      "grad_norm": 1.528185248374939,
      "learning_rate": 2.3511526525786908e-05,
      "loss": 0.7777,
      "step": 71700
    },
    {
      "epoch": 10.610314762819566,
      "grad_norm": 1.0213147401809692,
      "learning_rate": 2.3474582532880155e-05,
      "loss": 0.7425,
      "step": 71800
    },
    {
      "epoch": 10.625092359982267,
      "grad_norm": 0.990750789642334,
      "learning_rate": 2.3437638539973402e-05,
      "loss": 0.7577,
      "step": 71900
    },
    {
      "epoch": 10.639869957144969,
      "grad_norm": 0.9613122344017029,
      "learning_rate": 2.340069454706665e-05,
      "loss": 0.7596,
      "step": 72000
    },
    {
      "epoch": 10.639869957144969,
      "eval_loss": 0.6911147236824036,
      "eval_runtime": 72.533,
      "eval_samples_per_second": 165.856,
      "eval_steps_per_second": 20.735,
      "step": 72000
    },
    {
      "epoch": 10.654647554307669,
      "grad_norm": 1.3602650165557861,
      "learning_rate": 2.3363750554159893e-05,
      "loss": 0.7647,
      "step": 72100
    },
    {
      "epoch": 10.66942515147037,
      "grad_norm": 1.1709059476852417,
      "learning_rate": 2.3326806561253143e-05,
      "loss": 0.7365,
      "step": 72200
    },
    {
      "epoch": 10.684202748633073,
      "grad_norm": 0.7946870923042297,
      "learning_rate": 2.3289862568346387e-05,
      "loss": 0.7691,
      "step": 72300
    },
    {
      "epoch": 10.698980345795773,
      "grad_norm": 0.8707038164138794,
      "learning_rate": 2.3252918575439634e-05,
      "loss": 0.7485,
      "step": 72400
    },
    {
      "epoch": 10.713757942958475,
      "grad_norm": 0.858526349067688,
      "learning_rate": 2.321597458253288e-05,
      "loss": 0.7574,
      "step": 72500
    },
    {
      "epoch": 10.728535540121177,
      "grad_norm": 1.5042260885238647,
      "learning_rate": 2.3179030589626128e-05,
      "loss": 0.7863,
      "step": 72600
    },
    {
      "epoch": 10.743313137283877,
      "grad_norm": 1.1514171361923218,
      "learning_rate": 2.3142086596719375e-05,
      "loss": 0.7531,
      "step": 72700
    },
    {
      "epoch": 10.75809073444658,
      "grad_norm": 0.9215624332427979,
      "learning_rate": 2.3105142603812622e-05,
      "loss": 0.7508,
      "step": 72800
    },
    {
      "epoch": 10.772868331609281,
      "grad_norm": 0.8880457878112793,
      "learning_rate": 2.3068198610905866e-05,
      "loss": 0.741,
      "step": 72900
    },
    {
      "epoch": 10.787645928771981,
      "grad_norm": 1.2393039464950562,
      "learning_rate": 2.3031254617999116e-05,
      "loss": 0.731,
      "step": 73000
    },
    {
      "epoch": 10.802423525934683,
      "grad_norm": 1.2915916442871094,
      "learning_rate": 2.299431062509236e-05,
      "loss": 0.742,
      "step": 73100
    },
    {
      "epoch": 10.817201123097384,
      "grad_norm": 1.0504487752914429,
      "learning_rate": 2.295736663218561e-05,
      "loss": 0.7743,
      "step": 73200
    },
    {
      "epoch": 10.831978720260086,
      "grad_norm": 1.137994647026062,
      "learning_rate": 2.2920422639278854e-05,
      "loss": 0.7279,
      "step": 73300
    },
    {
      "epoch": 10.846756317422788,
      "grad_norm": 1.093125343322754,
      "learning_rate": 2.28834786463721e-05,
      "loss": 0.7379,
      "step": 73400
    },
    {
      "epoch": 10.861533914585488,
      "grad_norm": 1.0755581855773926,
      "learning_rate": 2.2846534653465348e-05,
      "loss": 0.7415,
      "step": 73500
    },
    {
      "epoch": 10.87631151174819,
      "grad_norm": 1.0780351161956787,
      "learning_rate": 2.2809590660558595e-05,
      "loss": 0.7477,
      "step": 73600
    },
    {
      "epoch": 10.891089108910892,
      "grad_norm": 1.1643911600112915,
      "learning_rate": 2.277264666765184e-05,
      "loss": 0.7216,
      "step": 73700
    },
    {
      "epoch": 10.905866706073592,
      "grad_norm": 1.1898903846740723,
      "learning_rate": 2.273570267474509e-05,
      "loss": 0.7183,
      "step": 73800
    },
    {
      "epoch": 10.920644303236294,
      "grad_norm": 1.3530192375183105,
      "learning_rate": 2.2698758681838333e-05,
      "loss": 0.7325,
      "step": 73900
    },
    {
      "epoch": 10.935421900398994,
      "grad_norm": 1.2107070684432983,
      "learning_rate": 2.2661814688931583e-05,
      "loss": 0.7406,
      "step": 74000
    },
    {
      "epoch": 10.935421900398994,
      "eval_loss": 0.6815153360366821,
      "eval_runtime": 73.4915,
      "eval_samples_per_second": 163.692,
      "eval_steps_per_second": 20.465,
      "step": 74000
    },
    {
      "epoch": 10.950199497561696,
      "grad_norm": 1.0926299095153809,
      "learning_rate": 2.2624870696024827e-05,
      "loss": 0.761,
      "step": 74100
    },
    {
      "epoch": 10.964977094724398,
      "grad_norm": 1.0043312311172485,
      "learning_rate": 2.2587926703118074e-05,
      "loss": 0.7337,
      "step": 74200
    },
    {
      "epoch": 10.979754691887099,
      "grad_norm": 0.9177083969116211,
      "learning_rate": 2.255098271021132e-05,
      "loss": 0.7284,
      "step": 74300
    },
    {
      "epoch": 10.9945322890498,
      "grad_norm": 1.3748128414154053,
      "learning_rate": 2.2514038717304568e-05,
      "loss": 0.7158,
      "step": 74400
    },
    {
      "epoch": 11.009309886212503,
      "grad_norm": 0.9958263039588928,
      "learning_rate": 2.2477094724397815e-05,
      "loss": 0.7327,
      "step": 74500
    },
    {
      "epoch": 11.024087483375203,
      "grad_norm": 0.9144322872161865,
      "learning_rate": 2.2440150731491062e-05,
      "loss": 0.7348,
      "step": 74600
    },
    {
      "epoch": 11.038865080537905,
      "grad_norm": 1.1867402791976929,
      "learning_rate": 2.2403206738584306e-05,
      "loss": 0.7329,
      "step": 74700
    },
    {
      "epoch": 11.053642677700607,
      "grad_norm": 1.548082709312439,
      "learning_rate": 2.2366262745677556e-05,
      "loss": 0.7223,
      "step": 74800
    },
    {
      "epoch": 11.068420274863307,
      "grad_norm": 1.0940332412719727,
      "learning_rate": 2.23293187527708e-05,
      "loss": 0.7367,
      "step": 74900
    },
    {
      "epoch": 11.083197872026009,
      "grad_norm": 1.2561343908309937,
      "learning_rate": 2.2292374759864047e-05,
      "loss": 0.7456,
      "step": 75000
    },
    {
      "epoch": 11.09797546918871,
      "grad_norm": 0.7995582222938538,
      "learning_rate": 2.2255430766957294e-05,
      "loss": 0.7143,
      "step": 75100
    },
    {
      "epoch": 11.112753066351411,
      "grad_norm": 0.9451131820678711,
      "learning_rate": 2.221848677405054e-05,
      "loss": 0.7217,
      "step": 75200
    },
    {
      "epoch": 11.127530663514113,
      "grad_norm": 0.9939725399017334,
      "learning_rate": 2.2181542781143788e-05,
      "loss": 0.7386,
      "step": 75300
    },
    {
      "epoch": 11.142308260676813,
      "grad_norm": 1.0289243459701538,
      "learning_rate": 2.2144598788237035e-05,
      "loss": 0.7373,
      "step": 75400
    },
    {
      "epoch": 11.157085857839515,
      "grad_norm": 1.0224981307983398,
      "learning_rate": 2.210765479533028e-05,
      "loss": 0.6856,
      "step": 75500
    },
    {
      "epoch": 11.171863455002217,
      "grad_norm": 1.1292089223861694,
      "learning_rate": 2.2070710802423526e-05,
      "loss": 0.7158,
      "step": 75600
    },
    {
      "epoch": 11.186641052164918,
      "grad_norm": 1.2097374200820923,
      "learning_rate": 2.2033766809516773e-05,
      "loss": 0.7769,
      "step": 75700
    },
    {
      "epoch": 11.20141864932762,
      "grad_norm": 1.0093810558319092,
      "learning_rate": 2.199682281661002e-05,
      "loss": 0.7307,
      "step": 75800
    },
    {
      "epoch": 11.216196246490322,
      "grad_norm": 0.8507819771766663,
      "learning_rate": 2.1959878823703267e-05,
      "loss": 0.7074,
      "step": 75900
    },
    {
      "epoch": 11.230973843653022,
      "grad_norm": 0.994472861289978,
      "learning_rate": 2.1922934830796514e-05,
      "loss": 0.7319,
      "step": 76000
    },
    {
      "epoch": 11.230973843653022,
      "eval_loss": 0.6802543997764587,
      "eval_runtime": 75.1978,
      "eval_samples_per_second": 159.978,
      "eval_steps_per_second": 20.001,
      "step": 76000
    },
    {
      "epoch": 11.245751440815724,
      "grad_norm": 1.5791146755218506,
      "learning_rate": 2.188599083788976e-05,
      "loss": 0.728,
      "step": 76100
    },
    {
      "epoch": 11.260529037978424,
      "grad_norm": 1.3178682327270508,
      "learning_rate": 2.1849046844983005e-05,
      "loss": 0.7267,
      "step": 76200
    },
    {
      "epoch": 11.275306635141126,
      "grad_norm": 1.0439220666885376,
      "learning_rate": 2.1812102852076252e-05,
      "loss": 0.728,
      "step": 76300
    },
    {
      "epoch": 11.290084232303828,
      "grad_norm": 1.1144925355911255,
      "learning_rate": 2.17751588591695e-05,
      "loss": 0.7356,
      "step": 76400
    },
    {
      "epoch": 11.304861829466528,
      "grad_norm": 1.2526518106460571,
      "learning_rate": 2.1738214866262746e-05,
      "loss": 0.7238,
      "step": 76500
    },
    {
      "epoch": 11.31963942662923,
      "grad_norm": 0.8951583504676819,
      "learning_rate": 2.1701270873355993e-05,
      "loss": 0.7182,
      "step": 76600
    },
    {
      "epoch": 11.334417023791932,
      "grad_norm": 1.135000467300415,
      "learning_rate": 2.166432688044924e-05,
      "loss": 0.7227,
      "step": 76700
    },
    {
      "epoch": 11.349194620954632,
      "grad_norm": 1.0264428853988647,
      "learning_rate": 2.1627382887542484e-05,
      "loss": 0.7299,
      "step": 76800
    },
    {
      "epoch": 11.363972218117334,
      "grad_norm": 0.9247434139251709,
      "learning_rate": 2.1590438894635734e-05,
      "loss": 0.7213,
      "step": 76900
    },
    {
      "epoch": 11.378749815280035,
      "grad_norm": 1.122887372970581,
      "learning_rate": 2.1553494901728978e-05,
      "loss": 0.6959,
      "step": 77000
    },
    {
      "epoch": 11.393527412442737,
      "grad_norm": 0.6967537999153137,
      "learning_rate": 2.151655090882223e-05,
      "loss": 0.7269,
      "step": 77100
    },
    {
      "epoch": 11.408305009605439,
      "grad_norm": 1.15496027469635,
      "learning_rate": 2.1479606915915472e-05,
      "loss": 0.7254,
      "step": 77200
    },
    {
      "epoch": 11.423082606768139,
      "grad_norm": 1.0119768381118774,
      "learning_rate": 2.144266292300872e-05,
      "loss": 0.7275,
      "step": 77300
    },
    {
      "epoch": 11.43786020393084,
      "grad_norm": 0.9593588709831238,
      "learning_rate": 2.1405718930101966e-05,
      "loss": 0.7638,
      "step": 77400
    },
    {
      "epoch": 11.452637801093543,
      "grad_norm": 0.8950539827346802,
      "learning_rate": 2.1368774937195213e-05,
      "loss": 0.7593,
      "step": 77500
    },
    {
      "epoch": 11.467415398256243,
      "grad_norm": 1.077501654624939,
      "learning_rate": 2.1331830944288457e-05,
      "loss": 0.7446,
      "step": 77600
    },
    {
      "epoch": 11.482192995418945,
      "grad_norm": 1.3824208974838257,
      "learning_rate": 2.1294886951381708e-05,
      "loss": 0.6982,
      "step": 77700
    },
    {
      "epoch": 11.496970592581647,
      "grad_norm": 0.840596616268158,
      "learning_rate": 2.125794295847495e-05,
      "loss": 0.7466,
      "step": 77800
    },
    {
      "epoch": 11.511748189744347,
      "grad_norm": 0.7791270017623901,
      "learning_rate": 2.1220998965568202e-05,
      "loss": 0.6957,
      "step": 77900
    },
    {
      "epoch": 11.52652578690705,
      "grad_norm": 1.2889982461929321,
      "learning_rate": 2.1184054972661445e-05,
      "loss": 0.7341,
      "step": 78000
    },
    {
      "epoch": 11.52652578690705,
      "eval_loss": 0.6762268543243408,
      "eval_runtime": 73.7505,
      "eval_samples_per_second": 163.118,
      "eval_steps_per_second": 20.393,
      "step": 78000
    },
    {
      "epoch": 11.54130338406975,
      "grad_norm": 1.0066224336624146,
      "learning_rate": 2.1147110979754692e-05,
      "loss": 0.7208,
      "step": 78100
    },
    {
      "epoch": 11.556080981232451,
      "grad_norm": 1.1529289484024048,
      "learning_rate": 2.111016698684794e-05,
      "loss": 0.7147,
      "step": 78200
    },
    {
      "epoch": 11.570858578395153,
      "grad_norm": 1.0413442850112915,
      "learning_rate": 2.1073222993941187e-05,
      "loss": 0.7265,
      "step": 78300
    },
    {
      "epoch": 11.585636175557854,
      "grad_norm": 1.1057734489440918,
      "learning_rate": 2.1036279001034434e-05,
      "loss": 0.7221,
      "step": 78400
    },
    {
      "epoch": 11.600413772720556,
      "grad_norm": 1.0394264459609985,
      "learning_rate": 2.099933500812768e-05,
      "loss": 0.74,
      "step": 78500
    },
    {
      "epoch": 11.615191369883258,
      "grad_norm": 0.8790578842163086,
      "learning_rate": 2.0962391015220924e-05,
      "loss": 0.767,
      "step": 78600
    },
    {
      "epoch": 11.629968967045958,
      "grad_norm": 1.2934049367904663,
      "learning_rate": 2.0925447022314175e-05,
      "loss": 0.7311,
      "step": 78700
    },
    {
      "epoch": 11.64474656420866,
      "grad_norm": 1.098351240158081,
      "learning_rate": 2.088850302940742e-05,
      "loss": 0.7353,
      "step": 78800
    },
    {
      "epoch": 11.659524161371362,
      "grad_norm": 1.1968203783035278,
      "learning_rate": 2.0851559036500666e-05,
      "loss": 0.7339,
      "step": 78900
    },
    {
      "epoch": 11.674301758534062,
      "grad_norm": 1.1924853324890137,
      "learning_rate": 2.0814615043593913e-05,
      "loss": 0.6947,
      "step": 79000
    },
    {
      "epoch": 11.689079355696764,
      "grad_norm": 1.2374720573425293,
      "learning_rate": 2.077767105068716e-05,
      "loss": 0.7136,
      "step": 79100
    },
    {
      "epoch": 11.703856952859464,
      "grad_norm": 0.780519962310791,
      "learning_rate": 2.0740727057780407e-05,
      "loss": 0.7317,
      "step": 79200
    },
    {
      "epoch": 11.718634550022166,
      "grad_norm": 0.8821130990982056,
      "learning_rate": 2.0703783064873654e-05,
      "loss": 0.7169,
      "step": 79300
    },
    {
      "epoch": 11.733412147184868,
      "grad_norm": 1.1055469512939453,
      "learning_rate": 2.0666839071966897e-05,
      "loss": 0.7329,
      "step": 79400
    },
    {
      "epoch": 11.748189744347568,
      "grad_norm": 1.1191205978393555,
      "learning_rate": 2.0629895079060148e-05,
      "loss": 0.7141,
      "step": 79500
    },
    {
      "epoch": 11.76296734151027,
      "grad_norm": 0.9835250973701477,
      "learning_rate": 2.059295108615339e-05,
      "loss": 0.7357,
      "step": 79600
    },
    {
      "epoch": 11.777744938672972,
      "grad_norm": 0.908359944820404,
      "learning_rate": 2.055600709324664e-05,
      "loss": 0.712,
      "step": 79700
    },
    {
      "epoch": 11.792522535835673,
      "grad_norm": 0.9868008494377136,
      "learning_rate": 2.0519063100339886e-05,
      "loss": 0.6967,
      "step": 79800
    },
    {
      "epoch": 11.807300132998375,
      "grad_norm": 1.113813042640686,
      "learning_rate": 2.0482119107433133e-05,
      "loss": 0.714,
      "step": 79900
    },
    {
      "epoch": 11.822077730161077,
      "grad_norm": 1.0996086597442627,
      "learning_rate": 2.044517511452638e-05,
      "loss": 0.7341,
      "step": 80000
    },
    {
      "epoch": 11.822077730161077,
      "eval_loss": 0.6670944094657898,
      "eval_runtime": 75.2573,
      "eval_samples_per_second": 159.852,
      "eval_steps_per_second": 19.985,
      "step": 80000
    },
    {
      "epoch": 11.836855327323777,
      "grad_norm": 1.1082333326339722,
      "learning_rate": 2.0408231121619627e-05,
      "loss": 0.7103,
      "step": 80100
    },
    {
      "epoch": 11.851632924486479,
      "grad_norm": 1.1867586374282837,
      "learning_rate": 2.037128712871287e-05,
      "loss": 0.6979,
      "step": 80200
    },
    {
      "epoch": 11.866410521649179,
      "grad_norm": 0.8638510704040527,
      "learning_rate": 2.0334343135806118e-05,
      "loss": 0.6914,
      "step": 80300
    },
    {
      "epoch": 11.881188118811881,
      "grad_norm": 1.3953218460083008,
      "learning_rate": 2.0297399142899365e-05,
      "loss": 0.6967,
      "step": 80400
    },
    {
      "epoch": 11.895965715974583,
      "grad_norm": 0.9926997423171997,
      "learning_rate": 2.0260455149992612e-05,
      "loss": 0.7251,
      "step": 80500
    },
    {
      "epoch": 11.910743313137283,
      "grad_norm": 0.9632355570793152,
      "learning_rate": 2.022351115708586e-05,
      "loss": 0.6993,
      "step": 80600
    },
    {
      "epoch": 11.925520910299985,
      "grad_norm": 1.152909755706787,
      "learning_rate": 2.0186567164179106e-05,
      "loss": 0.7345,
      "step": 80700
    },
    {
      "epoch": 11.940298507462687,
      "grad_norm": 1.1502898931503296,
      "learning_rate": 2.0149623171272353e-05,
      "loss": 0.7099,
      "step": 80800
    },
    {
      "epoch": 11.955076104625388,
      "grad_norm": 0.901545524597168,
      "learning_rate": 2.0112679178365597e-05,
      "loss": 0.7091,
      "step": 80900
    },
    {
      "epoch": 11.96985370178809,
      "grad_norm": 1.1136658191680908,
      "learning_rate": 2.0075735185458847e-05,
      "loss": 0.7201,
      "step": 81000
    },
    {
      "epoch": 11.984631298950791,
      "grad_norm": 1.000943899154663,
      "learning_rate": 2.003879119255209e-05,
      "loss": 0.7201,
      "step": 81100
    },
    {
      "epoch": 11.999408896113492,
      "grad_norm": 1.0160515308380127,
      "learning_rate": 2.0001847199645338e-05,
      "loss": 0.7375,
      "step": 81200
    },
    {
      "epoch": 12.014186493276194,
      "grad_norm": 0.7008199691772461,
      "learning_rate": 1.9964903206738585e-05,
      "loss": 0.6812,
      "step": 81300
    },
    {
      "epoch": 12.028964090438894,
      "grad_norm": 2.4224748611450195,
      "learning_rate": 1.9927959213831832e-05,
      "loss": 0.7282,
      "step": 81400
    },
    {
      "epoch": 12.043741687601596,
      "grad_norm": 1.0564160346984863,
      "learning_rate": 1.9891015220925076e-05,
      "loss": 0.7448,
      "step": 81500
    },
    {
      "epoch": 12.058519284764298,
      "grad_norm": 0.9627124667167664,
      "learning_rate": 1.9854071228018326e-05,
      "loss": 0.7244,
      "step": 81600
    },
    {
      "epoch": 12.073296881926998,
      "grad_norm": 1.074235200881958,
      "learning_rate": 1.981712723511157e-05,
      "loss": 0.7024,
      "step": 81700
    },
    {
      "epoch": 12.0880744790897,
      "grad_norm": 1.0285313129425049,
      "learning_rate": 1.978018324220482e-05,
      "loss": 0.7018,
      "step": 81800
    },
    {
      "epoch": 12.102852076252402,
      "grad_norm": 1.0447629690170288,
      "learning_rate": 1.9743239249298064e-05,
      "loss": 0.7117,
      "step": 81900
    },
    {
      "epoch": 12.117629673415102,
      "grad_norm": 1.1911261081695557,
      "learning_rate": 1.970629525639131e-05,
      "loss": 0.6989,
      "step": 82000
    },
    {
      "epoch": 12.117629673415102,
      "eval_loss": 0.6684519052505493,
      "eval_runtime": 73.7922,
      "eval_samples_per_second": 163.025,
      "eval_steps_per_second": 20.382,
      "step": 82000
    },
    {
      "epoch": 12.132407270577804,
      "grad_norm": 0.9985661506652832,
      "learning_rate": 1.9669351263484558e-05,
      "loss": 0.7024,
      "step": 82100
    },
    {
      "epoch": 12.147184867740505,
      "grad_norm": 1.1076277494430542,
      "learning_rate": 1.9632407270577805e-05,
      "loss": 0.7292,
      "step": 82200
    },
    {
      "epoch": 12.161962464903207,
      "grad_norm": 0.9517275094985962,
      "learning_rate": 1.9595463277671052e-05,
      "loss": 0.6962,
      "step": 82300
    },
    {
      "epoch": 12.176740062065909,
      "grad_norm": 1.0052402019500732,
      "learning_rate": 1.95585192847643e-05,
      "loss": 0.6931,
      "step": 82400
    },
    {
      "epoch": 12.191517659228609,
      "grad_norm": 1.1064965724945068,
      "learning_rate": 1.9521575291857543e-05,
      "loss": 0.678,
      "step": 82500
    },
    {
      "epoch": 12.20629525639131,
      "grad_norm": 0.8757889270782471,
      "learning_rate": 1.9484631298950793e-05,
      "loss": 0.7176,
      "step": 82600
    },
    {
      "epoch": 12.221072853554013,
      "grad_norm": 0.8454016447067261,
      "learning_rate": 1.9447687306044037e-05,
      "loss": 0.6853,
      "step": 82700
    },
    {
      "epoch": 12.235850450716713,
      "grad_norm": 1.437031865119934,
      "learning_rate": 1.9410743313137284e-05,
      "loss": 0.739,
      "step": 82800
    },
    {
      "epoch": 12.250628047879415,
      "grad_norm": 1.0425796508789062,
      "learning_rate": 1.937379932023053e-05,
      "loss": 0.6916,
      "step": 82900
    },
    {
      "epoch": 12.265405645042117,
      "grad_norm": 0.9458098411560059,
      "learning_rate": 1.9336855327323778e-05,
      "loss": 0.6981,
      "step": 83000
    },
    {
      "epoch": 12.280183242204817,
      "grad_norm": 1.1125667095184326,
      "learning_rate": 1.9299911334417025e-05,
      "loss": 0.7204,
      "step": 83100
    },
    {
      "epoch": 12.29496083936752,
      "grad_norm": 0.7522534728050232,
      "learning_rate": 1.9262967341510272e-05,
      "loss": 0.6812,
      "step": 83200
    },
    {
      "epoch": 12.30973843653022,
      "grad_norm": 1.1660047769546509,
      "learning_rate": 1.9226023348603516e-05,
      "loss": 0.7091,
      "step": 83300
    },
    {
      "epoch": 12.324516033692921,
      "grad_norm": 0.805862307548523,
      "learning_rate": 1.9189079355696766e-05,
      "loss": 0.6998,
      "step": 83400
    },
    {
      "epoch": 12.339293630855623,
      "grad_norm": 1.3066062927246094,
      "learning_rate": 1.915213536279001e-05,
      "loss": 0.7374,
      "step": 83500
    },
    {
      "epoch": 12.354071228018324,
      "grad_norm": 0.9903029203414917,
      "learning_rate": 1.911519136988326e-05,
      "loss": 0.7045,
      "step": 83600
    },
    {
      "epoch": 12.368848825181026,
      "grad_norm": 0.9040297865867615,
      "learning_rate": 1.9078247376976504e-05,
      "loss": 0.6876,
      "step": 83700
    },
    {
      "epoch": 12.383626422343728,
      "grad_norm": 1.3442554473876953,
      "learning_rate": 1.904130338406975e-05,
      "loss": 0.7105,
      "step": 83800
    },
    {
      "epoch": 12.398404019506428,
      "grad_norm": 1.1145894527435303,
      "learning_rate": 1.9004359391162998e-05,
      "loss": 0.7256,
      "step": 83900
    },
    {
      "epoch": 12.41318161666913,
      "grad_norm": 1.2026535272598267,
      "learning_rate": 1.8967415398256245e-05,
      "loss": 0.7166,
      "step": 84000
    },
    {
      "epoch": 12.41318161666913,
      "eval_loss": 0.6632270812988281,
      "eval_runtime": 72.4258,
      "eval_samples_per_second": 166.101,
      "eval_steps_per_second": 20.766,
      "step": 84000
    },
    {
      "epoch": 12.427959213831832,
      "grad_norm": 0.9242472648620605,
      "learning_rate": 1.893047140534949e-05,
      "loss": 0.7028,
      "step": 84100
    },
    {
      "epoch": 12.442736810994532,
      "grad_norm": 1.1135374307632446,
      "learning_rate": 1.889352741244274e-05,
      "loss": 0.7335,
      "step": 84200
    },
    {
      "epoch": 12.457514408157234,
      "grad_norm": 1.0034747123718262,
      "learning_rate": 1.8856583419535983e-05,
      "loss": 0.7011,
      "step": 84300
    },
    {
      "epoch": 12.472292005319934,
      "grad_norm": 1.0521416664123535,
      "learning_rate": 1.8819639426629234e-05,
      "loss": 0.6866,
      "step": 84400
    },
    {
      "epoch": 12.487069602482636,
      "grad_norm": 0.9632725119590759,
      "learning_rate": 1.8782695433722477e-05,
      "loss": 0.6906,
      "step": 84500
    },
    {
      "epoch": 12.501847199645338,
      "grad_norm": 0.9044872522354126,
      "learning_rate": 1.8745751440815724e-05,
      "loss": 0.6912,
      "step": 84600
    },
    {
      "epoch": 12.516624796808038,
      "grad_norm": 1.118507981300354,
      "learning_rate": 1.870880744790897e-05,
      "loss": 0.6825,
      "step": 84700
    },
    {
      "epoch": 12.53140239397074,
      "grad_norm": 0.9919484257698059,
      "learning_rate": 1.867186345500222e-05,
      "loss": 0.7056,
      "step": 84800
    },
    {
      "epoch": 12.546179991133442,
      "grad_norm": 1.1194788217544556,
      "learning_rate": 1.8634919462095465e-05,
      "loss": 0.6982,
      "step": 84900
    },
    {
      "epoch": 12.560957588296143,
      "grad_norm": 0.971808910369873,
      "learning_rate": 1.8597975469188713e-05,
      "loss": 0.6964,
      "step": 85000
    },
    {
      "epoch": 12.575735185458845,
      "grad_norm": 1.1013058423995972,
      "learning_rate": 1.8561031476281956e-05,
      "loss": 0.7129,
      "step": 85100
    },
    {
      "epoch": 12.590512782621545,
      "grad_norm": 1.0183684825897217,
      "learning_rate": 1.8524087483375203e-05,
      "loss": 0.7155,
      "step": 85200
    },
    {
      "epoch": 12.605290379784247,
      "grad_norm": 1.3255631923675537,
      "learning_rate": 1.848714349046845e-05,
      "loss": 0.6692,
      "step": 85300
    },
    {
      "epoch": 12.620067976946949,
      "grad_norm": 1.0785541534423828,
      "learning_rate": 1.8450199497561697e-05,
      "loss": 0.6881,
      "step": 85400
    },
    {
      "epoch": 12.634845574109649,
      "grad_norm": 1.099770188331604,
      "learning_rate": 1.8413255504654944e-05,
      "loss": 0.7229,
      "step": 85500
    },
    {
      "epoch": 12.649623171272351,
      "grad_norm": 0.9257359504699707,
      "learning_rate": 1.837631151174819e-05,
      "loss": 0.7113,
      "step": 85600
    },
    {
      "epoch": 12.664400768435053,
      "grad_norm": 1.1892368793487549,
      "learning_rate": 1.833936751884144e-05,
      "loss": 0.6946,
      "step": 85700
    },
    {
      "epoch": 12.679178365597753,
      "grad_norm": 1.151781678199768,
      "learning_rate": 1.8302423525934682e-05,
      "loss": 0.6943,
      "step": 85800
    },
    {
      "epoch": 12.693955962760455,
      "grad_norm": 0.9042637348175049,
      "learning_rate": 1.826547953302793e-05,
      "loss": 0.723,
      "step": 85900
    },
    {
      "epoch": 12.708733559923157,
      "grad_norm": 0.9778671264648438,
      "learning_rate": 1.8228535540121176e-05,
      "loss": 0.6955,
      "step": 86000
    },
    {
      "epoch": 12.708733559923157,
      "eval_loss": 0.6595451831817627,
      "eval_runtime": 72.4283,
      "eval_samples_per_second": 166.095,
      "eval_steps_per_second": 20.765,
      "step": 86000
    },
    {
      "epoch": 12.723511157085857,
      "grad_norm": 1.1203275918960571,
      "learning_rate": 1.8191591547214423e-05,
      "loss": 0.7036,
      "step": 86100
    },
    {
      "epoch": 12.73828875424856,
      "grad_norm": 0.7403334975242615,
      "learning_rate": 1.815464755430767e-05,
      "loss": 0.6864,
      "step": 86200
    },
    {
      "epoch": 12.75306635141126,
      "grad_norm": 1.212448239326477,
      "learning_rate": 1.8117703561400918e-05,
      "loss": 0.6989,
      "step": 86300
    },
    {
      "epoch": 12.767843948573962,
      "grad_norm": 1.1011589765548706,
      "learning_rate": 1.808075956849416e-05,
      "loss": 0.6925,
      "step": 86400
    },
    {
      "epoch": 12.782621545736664,
      "grad_norm": 1.205620527267456,
      "learning_rate": 1.804381557558741e-05,
      "loss": 0.7109,
      "step": 86500
    },
    {
      "epoch": 12.797399142899364,
      "grad_norm": 1.2076324224472046,
      "learning_rate": 1.8006871582680655e-05,
      "loss": 0.6985,
      "step": 86600
    },
    {
      "epoch": 12.812176740062066,
      "grad_norm": 0.9100780487060547,
      "learning_rate": 1.7969927589773906e-05,
      "loss": 0.6899,
      "step": 86700
    },
    {
      "epoch": 12.826954337224768,
      "grad_norm": 1.0221627950668335,
      "learning_rate": 1.793298359686715e-05,
      "loss": 0.6802,
      "step": 86800
    },
    {
      "epoch": 12.841731934387468,
      "grad_norm": 0.8899125456809998,
      "learning_rate": 1.7896039603960396e-05,
      "loss": 0.7231,
      "step": 86900
    },
    {
      "epoch": 12.85650953155017,
      "grad_norm": 0.9159532189369202,
      "learning_rate": 1.7859095611053644e-05,
      "loss": 0.6692,
      "step": 87000
    },
    {
      "epoch": 12.871287128712872,
      "grad_norm": 1.2702643871307373,
      "learning_rate": 1.782215161814689e-05,
      "loss": 0.7124,
      "step": 87100
    },
    {
      "epoch": 12.886064725875572,
      "grad_norm": 0.879112720489502,
      "learning_rate": 1.7785207625240134e-05,
      "loss": 0.7384,
      "step": 87200
    },
    {
      "epoch": 12.900842323038274,
      "grad_norm": 0.9396902322769165,
      "learning_rate": 1.7748263632333385e-05,
      "loss": 0.7005,
      "step": 87300
    },
    {
      "epoch": 12.915619920200974,
      "grad_norm": 1.1784329414367676,
      "learning_rate": 1.771131963942663e-05,
      "loss": 0.693,
      "step": 87400
    },
    {
      "epoch": 12.930397517363676,
      "grad_norm": 0.746954619884491,
      "learning_rate": 1.767437564651988e-05,
      "loss": 0.6905,
      "step": 87500
    },
    {
      "epoch": 12.945175114526378,
      "grad_norm": 1.3380306959152222,
      "learning_rate": 1.7637431653613123e-05,
      "loss": 0.7152,
      "step": 87600
    },
    {
      "epoch": 12.959952711689079,
      "grad_norm": 1.0598081350326538,
      "learning_rate": 1.760048766070637e-05,
      "loss": 0.6726,
      "step": 87700
    },
    {
      "epoch": 12.97473030885178,
      "grad_norm": 0.8919087648391724,
      "learning_rate": 1.7563543667799617e-05,
      "loss": 0.7054,
      "step": 87800
    },
    {
      "epoch": 12.989507906014483,
      "grad_norm": 1.2141177654266357,
      "learning_rate": 1.7526599674892864e-05,
      "loss": 0.724,
      "step": 87900
    },
    {
      "epoch": 13.004285503177183,
      "grad_norm": 0.9148221611976624,
      "learning_rate": 1.748965568198611e-05,
      "loss": 0.7112,
      "step": 88000
    },
    {
      "epoch": 13.004285503177183,
      "eval_loss": 0.6602754592895508,
      "eval_runtime": 72.6181,
      "eval_samples_per_second": 165.661,
      "eval_steps_per_second": 20.711,
      "step": 88000
    },
    {
      "epoch": 13.019063100339885,
      "grad_norm": 0.868414044380188,
      "learning_rate": 1.7452711689079358e-05,
      "loss": 0.6743,
      "step": 88100
    },
    {
      "epoch": 13.033840697502587,
      "grad_norm": 1.1606416702270508,
      "learning_rate": 1.74157676961726e-05,
      "loss": 0.7057,
      "step": 88200
    },
    {
      "epoch": 13.048618294665287,
      "grad_norm": 1.0632493495941162,
      "learning_rate": 1.7378823703265852e-05,
      "loss": 0.6795,
      "step": 88300
    },
    {
      "epoch": 13.063395891827989,
      "grad_norm": 1.2327766418457031,
      "learning_rate": 1.7341879710359096e-05,
      "loss": 0.6852,
      "step": 88400
    },
    {
      "epoch": 13.07817348899069,
      "grad_norm": 1.4654676914215088,
      "learning_rate": 1.7304935717452343e-05,
      "loss": 0.7093,
      "step": 88500
    },
    {
      "epoch": 13.092951086153391,
      "grad_norm": 0.9902232885360718,
      "learning_rate": 1.726799172454559e-05,
      "loss": 0.6891,
      "step": 88600
    },
    {
      "epoch": 13.107728683316093,
      "grad_norm": 1.100732445716858,
      "learning_rate": 1.7231047731638837e-05,
      "loss": 0.7021,
      "step": 88700
    },
    {
      "epoch": 13.122506280478794,
      "grad_norm": 0.9743509888648987,
      "learning_rate": 1.7194103738732084e-05,
      "loss": 0.7001,
      "step": 88800
    },
    {
      "epoch": 13.137283877641496,
      "grad_norm": 0.933016836643219,
      "learning_rate": 1.715715974582533e-05,
      "loss": 0.6837,
      "step": 88900
    },
    {
      "epoch": 13.152061474804198,
      "grad_norm": 0.8884352445602417,
      "learning_rate": 1.7120215752918575e-05,
      "loss": 0.7172,
      "step": 89000
    },
    {
      "epoch": 13.166839071966898,
      "grad_norm": 1.2121707201004028,
      "learning_rate": 1.7083271760011825e-05,
      "loss": 0.6977,
      "step": 89100
    },
    {
      "epoch": 13.1816166691296,
      "grad_norm": 0.7884231805801392,
      "learning_rate": 1.704632776710507e-05,
      "loss": 0.6656,
      "step": 89200
    },
    {
      "epoch": 13.196394266292302,
      "grad_norm": 0.9799410700798035,
      "learning_rate": 1.700938377419832e-05,
      "loss": 0.7199,
      "step": 89300
    },
    {
      "epoch": 13.211171863455002,
      "grad_norm": 1.170253872871399,
      "learning_rate": 1.6972439781291563e-05,
      "loss": 0.6907,
      "step": 89400
    },
    {
      "epoch": 13.225949460617704,
      "grad_norm": 1.5317445993423462,
      "learning_rate": 1.693549578838481e-05,
      "loss": 0.6971,
      "step": 89500
    },
    {
      "epoch": 13.240727057780404,
      "grad_norm": 1.0606968402862549,
      "learning_rate": 1.6898551795478057e-05,
      "loss": 0.698,
      "step": 89600
    },
    {
      "epoch": 13.255504654943106,
      "grad_norm": 1.0039607286453247,
      "learning_rate": 1.6861607802571304e-05,
      "loss": 0.6921,
      "step": 89700
    },
    {
      "epoch": 13.270282252105808,
      "grad_norm": 1.12453293800354,
      "learning_rate": 1.6824663809664548e-05,
      "loss": 0.707,
      "step": 89800
    },
    {
      "epoch": 13.285059849268508,
      "grad_norm": 1.582393765449524,
      "learning_rate": 1.6787719816757798e-05,
      "loss": 0.6463,
      "step": 89900
    },
    {
      "epoch": 13.29983744643121,
      "grad_norm": 1.0809446573257446,
      "learning_rate": 1.6750775823851042e-05,
      "loss": 0.6528,
      "step": 90000
    },
    {
      "epoch": 13.29983744643121,
      "eval_loss": 0.6584380269050598,
      "eval_runtime": 74.1488,
      "eval_samples_per_second": 162.241,
      "eval_steps_per_second": 20.284,
      "step": 90000
    },
    {
      "epoch": 13.314615043593912,
      "grad_norm": 0.9266636967658997,
      "learning_rate": 1.671383183094429e-05,
      "loss": 0.7032,
      "step": 90100
    },
    {
      "epoch": 13.329392640756613,
      "grad_norm": 1.1051337718963623,
      "learning_rate": 1.6676887838037536e-05,
      "loss": 0.7278,
      "step": 90200
    },
    {
      "epoch": 13.344170237919315,
      "grad_norm": 0.8317976593971252,
      "learning_rate": 1.6639943845130783e-05,
      "loss": 0.6651,
      "step": 90300
    },
    {
      "epoch": 13.358947835082017,
      "grad_norm": 0.8702987432479858,
      "learning_rate": 1.660299985222403e-05,
      "loss": 0.6789,
      "step": 90400
    },
    {
      "epoch": 13.373725432244717,
      "grad_norm": 1.0268951654434204,
      "learning_rate": 1.6566055859317277e-05,
      "loss": 0.6798,
      "step": 90500
    },
    {
      "epoch": 13.388503029407419,
      "grad_norm": 1.056894302368164,
      "learning_rate": 1.6529111866410524e-05,
      "loss": 0.6743,
      "step": 90600
    },
    {
      "epoch": 13.403280626570119,
      "grad_norm": 0.8301112055778503,
      "learning_rate": 1.6492167873503768e-05,
      "loss": 0.6837,
      "step": 90700
    },
    {
      "epoch": 13.418058223732821,
      "grad_norm": 1.0076147317886353,
      "learning_rate": 1.6455223880597015e-05,
      "loss": 0.6881,
      "step": 90800
    },
    {
      "epoch": 13.432835820895523,
      "grad_norm": 0.8106651902198792,
      "learning_rate": 1.6418279887690262e-05,
      "loss": 0.675,
      "step": 90900
    },
    {
      "epoch": 13.447613418058223,
      "grad_norm": 0.9313141107559204,
      "learning_rate": 1.638133589478351e-05,
      "loss": 0.6882,
      "step": 91000
    },
    {
      "epoch": 13.462391015220925,
      "grad_norm": 1.1260849237442017,
      "learning_rate": 1.6344391901876756e-05,
      "loss": 0.6729,
      "step": 91100
    },
    {
      "epoch": 13.477168612383627,
      "grad_norm": 0.9594779014587402,
      "learning_rate": 1.6307447908970003e-05,
      "loss": 0.6932,
      "step": 91200
    },
    {
      "epoch": 13.491946209546327,
      "grad_norm": 1.4070804119110107,
      "learning_rate": 1.6270503916063247e-05,
      "loss": 0.6985,
      "step": 91300
    },
    {
      "epoch": 13.50672380670903,
      "grad_norm": 0.9084337949752808,
      "learning_rate": 1.6233559923156497e-05,
      "loss": 0.7028,
      "step": 91400
    },
    {
      "epoch": 13.521501403871731,
      "grad_norm": 0.8820614218711853,
      "learning_rate": 1.619661593024974e-05,
      "loss": 0.6669,
      "step": 91500
    },
    {
      "epoch": 13.536279001034432,
      "grad_norm": 1.0595887899398804,
      "learning_rate": 1.6159671937342988e-05,
      "loss": 0.6972,
      "step": 91600
    },
    {
      "epoch": 13.551056598197134,
      "grad_norm": 1.1522698402404785,
      "learning_rate": 1.6122727944436235e-05,
      "loss": 0.6978,
      "step": 91700
    },
    {
      "epoch": 13.565834195359834,
      "grad_norm": 0.7938214540481567,
      "learning_rate": 1.6085783951529482e-05,
      "loss": 0.6985,
      "step": 91800
    },
    {
      "epoch": 13.580611792522536,
      "grad_norm": 1.4144272804260254,
      "learning_rate": 1.604883995862273e-05,
      "loss": 0.6885,
      "step": 91900
    },
    {
      "epoch": 13.595389389685238,
      "grad_norm": 0.7653403282165527,
      "learning_rate": 1.6011895965715976e-05,
      "loss": 0.6507,
      "step": 92000
    },
    {
      "epoch": 13.595389389685238,
      "eval_loss": 0.655167281627655,
      "eval_runtime": 73.9824,
      "eval_samples_per_second": 162.606,
      "eval_steps_per_second": 20.329,
      "step": 92000
    },
    {
      "epoch": 13.610166986847938,
      "grad_norm": 0.9331390857696533,
      "learning_rate": 1.597495197280922e-05,
      "loss": 0.688,
      "step": 92100
    },
    {
      "epoch": 13.62494458401064,
      "grad_norm": 0.7155753970146179,
      "learning_rate": 1.593800797990247e-05,
      "loss": 0.6886,
      "step": 92200
    },
    {
      "epoch": 13.639722181173342,
      "grad_norm": 0.8345404267311096,
      "learning_rate": 1.5901063986995714e-05,
      "loss": 0.6884,
      "step": 92300
    },
    {
      "epoch": 13.654499778336042,
      "grad_norm": 1.1266709566116333,
      "learning_rate": 1.586411999408896e-05,
      "loss": 0.6933,
      "step": 92400
    },
    {
      "epoch": 13.669277375498744,
      "grad_norm": 1.2893558740615845,
      "learning_rate": 1.5827176001182208e-05,
      "loss": 0.6802,
      "step": 92500
    },
    {
      "epoch": 13.684054972661444,
      "grad_norm": 1.0804396867752075,
      "learning_rate": 1.5790232008275455e-05,
      "loss": 0.6688,
      "step": 92600
    },
    {
      "epoch": 13.698832569824146,
      "grad_norm": 1.3968195915222168,
      "learning_rate": 1.5753288015368702e-05,
      "loss": 0.6805,
      "step": 92700
    },
    {
      "epoch": 13.713610166986848,
      "grad_norm": 1.1877120733261108,
      "learning_rate": 1.571634402246195e-05,
      "loss": 0.6667,
      "step": 92800
    },
    {
      "epoch": 13.728387764149549,
      "grad_norm": 0.7773404121398926,
      "learning_rate": 1.5679400029555193e-05,
      "loss": 0.678,
      "step": 92900
    },
    {
      "epoch": 13.74316536131225,
      "grad_norm": 0.7917542457580566,
      "learning_rate": 1.5642456036648443e-05,
      "loss": 0.6768,
      "step": 93000
    },
    {
      "epoch": 13.757942958474953,
      "grad_norm": 1.1213325262069702,
      "learning_rate": 1.5605512043741687e-05,
      "loss": 0.6439,
      "step": 93100
    },
    {
      "epoch": 13.772720555637653,
      "grad_norm": 0.7639795541763306,
      "learning_rate": 1.5568568050834938e-05,
      "loss": 0.6934,
      "step": 93200
    },
    {
      "epoch": 13.787498152800355,
      "grad_norm": 1.1365656852722168,
      "learning_rate": 1.553162405792818e-05,
      "loss": 0.6656,
      "step": 93300
    },
    {
      "epoch": 13.802275749963057,
      "grad_norm": 1.1450722217559814,
      "learning_rate": 1.5494680065021428e-05,
      "loss": 0.6911,
      "step": 93400
    },
    {
      "epoch": 13.817053347125757,
      "grad_norm": 0.9236533045768738,
      "learning_rate": 1.5457736072114675e-05,
      "loss": 0.6865,
      "step": 93500
    },
    {
      "epoch": 13.831830944288459,
      "grad_norm": 1.0463619232177734,
      "learning_rate": 1.5420792079207922e-05,
      "loss": 0.6642,
      "step": 93600
    },
    {
      "epoch": 13.84660854145116,
      "grad_norm": 1.0600844621658325,
      "learning_rate": 1.5383848086301166e-05,
      "loss": 0.7065,
      "step": 93700
    },
    {
      "epoch": 13.861386138613861,
      "grad_norm": 1.0540003776550293,
      "learning_rate": 1.5346904093394417e-05,
      "loss": 0.6802,
      "step": 93800
    },
    {
      "epoch": 13.876163735776563,
      "grad_norm": 0.7892475724220276,
      "learning_rate": 1.530996010048766e-05,
      "loss": 0.689,
      "step": 93900
    },
    {
      "epoch": 13.890941332939263,
      "grad_norm": 0.9101499915122986,
      "learning_rate": 1.527301610758091e-05,
      "loss": 0.6467,
      "step": 94000
    },
    {
      "epoch": 13.890941332939263,
      "eval_loss": 0.653953492641449,
      "eval_runtime": 73.8048,
      "eval_samples_per_second": 162.998,
      "eval_steps_per_second": 20.378,
      "step": 94000
    },
    {
      "epoch": 13.905718930101965,
      "grad_norm": 1.1824525594711304,
      "learning_rate": 1.5236072114674154e-05,
      "loss": 0.6647,
      "step": 94100
    },
    {
      "epoch": 13.920496527264667,
      "grad_norm": 1.0474684238433838,
      "learning_rate": 1.51991281217674e-05,
      "loss": 0.7073,
      "step": 94200
    },
    {
      "epoch": 13.935274124427368,
      "grad_norm": 1.1123645305633545,
      "learning_rate": 1.5162184128860648e-05,
      "loss": 0.6768,
      "step": 94300
    },
    {
      "epoch": 13.95005172159007,
      "grad_norm": 1.0992532968521118,
      "learning_rate": 1.5125240135953894e-05,
      "loss": 0.6738,
      "step": 94400
    },
    {
      "epoch": 13.96482931875277,
      "grad_norm": 0.9169654250144958,
      "learning_rate": 1.5088296143047143e-05,
      "loss": 0.683,
      "step": 94500
    },
    {
      "epoch": 13.979606915915472,
      "grad_norm": 1.0813758373260498,
      "learning_rate": 1.5051352150140388e-05,
      "loss": 0.6966,
      "step": 94600
    },
    {
      "epoch": 13.994384513078174,
      "grad_norm": 1.3213834762573242,
      "learning_rate": 1.5014408157233633e-05,
      "loss": 0.6588,
      "step": 94700
    },
    {
      "epoch": 14.009162110240874,
      "grad_norm": 0.8099653124809265,
      "learning_rate": 1.4977464164326882e-05,
      "loss": 0.6956,
      "step": 94800
    },
    {
      "epoch": 14.023939707403576,
      "grad_norm": 0.7479217052459717,
      "learning_rate": 1.4940520171420127e-05,
      "loss": 0.6665,
      "step": 94900
    },
    {
      "epoch": 14.038717304566278,
      "grad_norm": 1.291208267211914,
      "learning_rate": 1.4903576178513373e-05,
      "loss": 0.6634,
      "step": 95000
    },
    {
      "epoch": 14.053494901728978,
      "grad_norm": 1.0990172624588013,
      "learning_rate": 1.4866632185606622e-05,
      "loss": 0.6422,
      "step": 95100
    },
    {
      "epoch": 14.06827249889168,
      "grad_norm": 1.4282171726226807,
      "learning_rate": 1.4829688192699867e-05,
      "loss": 0.6543,
      "step": 95200
    },
    {
      "epoch": 14.083050096054382,
      "grad_norm": 0.8365142345428467,
      "learning_rate": 1.4792744199793116e-05,
      "loss": 0.6509,
      "step": 95300
    },
    {
      "epoch": 14.097827693217083,
      "grad_norm": 0.7071831226348877,
      "learning_rate": 1.4755800206886361e-05,
      "loss": 0.6744,
      "step": 95400
    },
    {
      "epoch": 14.112605290379785,
      "grad_norm": 0.9505817890167236,
      "learning_rate": 1.4718856213979606e-05,
      "loss": 0.684,
      "step": 95500
    },
    {
      "epoch": 14.127382887542485,
      "grad_norm": 0.9245626330375671,
      "learning_rate": 1.4681912221072855e-05,
      "loss": 0.6607,
      "step": 95600
    },
    {
      "epoch": 14.142160484705187,
      "grad_norm": 1.0052685737609863,
      "learning_rate": 1.46449682281661e-05,
      "loss": 0.6782,
      "step": 95700
    },
    {
      "epoch": 14.156938081867889,
      "grad_norm": 0.8353855609893799,
      "learning_rate": 1.460802423525935e-05,
      "loss": 0.6885,
      "step": 95800
    },
    {
      "epoch": 14.171715679030589,
      "grad_norm": 1.2204034328460693,
      "learning_rate": 1.4571080242352595e-05,
      "loss": 0.6616,
      "step": 95900
    },
    {
      "epoch": 14.186493276193291,
      "grad_norm": 0.9320843815803528,
      "learning_rate": 1.453413624944584e-05,
      "loss": 0.6882,
      "step": 96000
    },
    {
      "epoch": 14.186493276193291,
      "eval_loss": 0.6486310958862305,
      "eval_runtime": 73.1802,
      "eval_samples_per_second": 164.389,
      "eval_steps_per_second": 20.552,
      "step": 96000
    },
    {
      "epoch": 14.201270873355993,
      "grad_norm": 1.0977272987365723,
      "learning_rate": 1.4497192256539089e-05,
      "loss": 0.6726,
      "step": 96100
    },
    {
      "epoch": 14.216048470518693,
      "grad_norm": 1.025484323501587,
      "learning_rate": 1.4460248263632334e-05,
      "loss": 0.6967,
      "step": 96200
    },
    {
      "epoch": 14.230826067681395,
      "grad_norm": 1.4673000574111938,
      "learning_rate": 1.442330427072558e-05,
      "loss": 0.6856,
      "step": 96300
    },
    {
      "epoch": 14.245603664844097,
      "grad_norm": 1.1234508752822876,
      "learning_rate": 1.4386360277818828e-05,
      "loss": 0.6893,
      "step": 96400
    },
    {
      "epoch": 14.260381262006797,
      "grad_norm": 1.0146249532699585,
      "learning_rate": 1.4349416284912074e-05,
      "loss": 0.6536,
      "step": 96500
    },
    {
      "epoch": 14.2751588591695,
      "grad_norm": 1.6488474607467651,
      "learning_rate": 1.4312472292005322e-05,
      "loss": 0.6928,
      "step": 96600
    },
    {
      "epoch": 14.2899364563322,
      "grad_norm": 0.9286467432975769,
      "learning_rate": 1.4275528299098568e-05,
      "loss": 0.6666,
      "step": 96700
    },
    {
      "epoch": 14.304714053494902,
      "grad_norm": 1.151430368423462,
      "learning_rate": 1.4238584306191813e-05,
      "loss": 0.6793,
      "step": 96800
    },
    {
      "epoch": 14.319491650657604,
      "grad_norm": 1.2908105850219727,
      "learning_rate": 1.4201640313285062e-05,
      "loss": 0.6904,
      "step": 96900
    },
    {
      "epoch": 14.334269247820304,
      "grad_norm": 1.150412678718567,
      "learning_rate": 1.4164696320378307e-05,
      "loss": 0.6482,
      "step": 97000
    },
    {
      "epoch": 14.349046844983006,
      "grad_norm": 0.953312873840332,
      "learning_rate": 1.4127752327471554e-05,
      "loss": 0.6982,
      "step": 97100
    },
    {
      "epoch": 14.363824442145708,
      "grad_norm": 1.3589262962341309,
      "learning_rate": 1.4090808334564801e-05,
      "loss": 0.6575,
      "step": 97200
    },
    {
      "epoch": 14.378602039308408,
      "grad_norm": 1.0445914268493652,
      "learning_rate": 1.4053864341658047e-05,
      "loss": 0.6724,
      "step": 97300
    },
    {
      "epoch": 14.39337963647111,
      "grad_norm": 1.270666241645813,
      "learning_rate": 1.4016920348751294e-05,
      "loss": 0.6947,
      "step": 97400
    },
    {
      "epoch": 14.408157233633812,
      "grad_norm": 1.166537880897522,
      "learning_rate": 1.397997635584454e-05,
      "loss": 0.673,
      "step": 97500
    },
    {
      "epoch": 14.422934830796512,
      "grad_norm": 1.1274240016937256,
      "learning_rate": 1.3943032362937786e-05,
      "loss": 0.6748,
      "step": 97600
    },
    {
      "epoch": 14.437712427959214,
      "grad_norm": 0.810762345790863,
      "learning_rate": 1.3906088370031033e-05,
      "loss": 0.6756,
      "step": 97700
    },
    {
      "epoch": 14.452490025121914,
      "grad_norm": 1.2702494859695435,
      "learning_rate": 1.386914437712428e-05,
      "loss": 0.66,
      "step": 97800
    },
    {
      "epoch": 14.467267622284616,
      "grad_norm": 0.7615957260131836,
      "learning_rate": 1.3832200384217527e-05,
      "loss": 0.6354,
      "step": 97900
    },
    {
      "epoch": 14.482045219447318,
      "grad_norm": 0.838482677936554,
      "learning_rate": 1.3795256391310773e-05,
      "loss": 0.6664,
      "step": 98000
    },
    {
      "epoch": 14.482045219447318,
      "eval_loss": 0.6472692489624023,
      "eval_runtime": 77.0233,
      "eval_samples_per_second": 156.187,
      "eval_steps_per_second": 19.527,
      "step": 98000
    },
    {
      "epoch": 14.496822816610019,
      "grad_norm": 1.2597075700759888,
      "learning_rate": 1.375831239840402e-05,
      "loss": 0.6403,
      "step": 98100
    },
    {
      "epoch": 14.51160041377272,
      "grad_norm": 1.2678691148757935,
      "learning_rate": 1.3721368405497267e-05,
      "loss": 0.6438,
      "step": 98200
    },
    {
      "epoch": 14.526378010935423,
      "grad_norm": 1.0803767442703247,
      "learning_rate": 1.3684424412590512e-05,
      "loss": 0.6721,
      "step": 98300
    },
    {
      "epoch": 14.541155608098123,
      "grad_norm": 1.0176036357879639,
      "learning_rate": 1.3647480419683761e-05,
      "loss": 0.6481,
      "step": 98400
    },
    {
      "epoch": 14.555933205260825,
      "grad_norm": 1.5501383543014526,
      "learning_rate": 1.3610536426777006e-05,
      "loss": 0.6602,
      "step": 98500
    },
    {
      "epoch": 14.570710802423527,
      "grad_norm": 0.7574765086174011,
      "learning_rate": 1.3573592433870252e-05,
      "loss": 0.649,
      "step": 98600
    },
    {
      "epoch": 14.585488399586227,
      "grad_norm": 1.2411330938339233,
      "learning_rate": 1.35366484409635e-05,
      "loss": 0.6788,
      "step": 98700
    },
    {
      "epoch": 14.600265996748929,
      "grad_norm": 1.1874653100967407,
      "learning_rate": 1.3499704448056746e-05,
      "loss": 0.6838,
      "step": 98800
    },
    {
      "epoch": 14.61504359391163,
      "grad_norm": 0.9698812365531921,
      "learning_rate": 1.3462760455149991e-05,
      "loss": 0.7005,
      "step": 98900
    },
    {
      "epoch": 14.629821191074331,
      "grad_norm": 0.9002095460891724,
      "learning_rate": 1.342581646224324e-05,
      "loss": 0.6676,
      "step": 99000
    },
    {
      "epoch": 14.644598788237033,
      "grad_norm": 1.4271575212478638,
      "learning_rate": 1.3388872469336485e-05,
      "loss": 0.6445,
      "step": 99100
    },
    {
      "epoch": 14.659376385399733,
      "grad_norm": 1.5193614959716797,
      "learning_rate": 1.3351928476429734e-05,
      "loss": 0.6925,
      "step": 99200
    },
    {
      "epoch": 14.674153982562435,
      "grad_norm": 0.7651877403259277,
      "learning_rate": 1.331498448352298e-05,
      "loss": 0.6743,
      "step": 99300
    },
    {
      "epoch": 14.688931579725137,
      "grad_norm": 1.1567795276641846,
      "learning_rate": 1.3278040490616225e-05,
      "loss": 0.6935,
      "step": 99400
    },
    {
      "epoch": 14.703709176887838,
      "grad_norm": 1.0096805095672607,
      "learning_rate": 1.3241096497709474e-05,
      "loss": 0.6632,
      "step": 99500
    },
    {
      "epoch": 14.71848677405054,
      "grad_norm": 1.071057915687561,
      "learning_rate": 1.3204152504802719e-05,
      "loss": 0.6748,
      "step": 99600
    },
    {
      "epoch": 14.733264371213242,
      "grad_norm": 0.9943912625312805,
      "learning_rate": 1.3167208511895968e-05,
      "loss": 0.6696,
      "step": 99700
    },
    {
      "epoch": 14.748041968375942,
      "grad_norm": 0.9173223972320557,
      "learning_rate": 1.3130264518989213e-05,
      "loss": 0.6807,
      "step": 99800
    },
    {
      "epoch": 14.762819565538644,
      "grad_norm": 0.888197660446167,
      "learning_rate": 1.3093320526082458e-05,
      "loss": 0.6573,
      "step": 99900
    },
    {
      "epoch": 14.777597162701344,
      "grad_norm": 1.0184731483459473,
      "learning_rate": 1.3056376533175707e-05,
      "loss": 0.6596,
      "step": 100000
    },
    {
      "epoch": 14.777597162701344,
      "eval_loss": 0.6455420851707458,
      "eval_runtime": 73.0202,
      "eval_samples_per_second": 164.749,
      "eval_steps_per_second": 20.597,
      "step": 100000
    },
    {
      "epoch": 14.792374759864046,
      "grad_norm": 1.2973523139953613,
      "learning_rate": 1.3019432540268953e-05,
      "loss": 0.6752,
      "step": 100100
    },
    {
      "epoch": 14.807152357026748,
      "grad_norm": 0.960716962814331,
      "learning_rate": 1.2982488547362198e-05,
      "loss": 0.6722,
      "step": 100200
    },
    {
      "epoch": 14.821929954189448,
      "grad_norm": 0.9998403787612915,
      "learning_rate": 1.2945544554455447e-05,
      "loss": 0.6731,
      "step": 100300
    },
    {
      "epoch": 14.83670755135215,
      "grad_norm": 1.075630784034729,
      "learning_rate": 1.2908600561548692e-05,
      "loss": 0.658,
      "step": 100400
    },
    {
      "epoch": 14.851485148514852,
      "grad_norm": 1.173027753829956,
      "learning_rate": 1.287165656864194e-05,
      "loss": 0.7014,
      "step": 100500
    },
    {
      "epoch": 14.866262745677552,
      "grad_norm": 1.0667436122894287,
      "learning_rate": 1.2834712575735186e-05,
      "loss": 0.6637,
      "step": 100600
    },
    {
      "epoch": 14.881040342840254,
      "grad_norm": 0.8635653853416443,
      "learning_rate": 1.2797768582828432e-05,
      "loss": 0.6859,
      "step": 100700
    },
    {
      "epoch": 14.895817940002956,
      "grad_norm": 1.1947437524795532,
      "learning_rate": 1.276082458992168e-05,
      "loss": 0.6717,
      "step": 100800
    },
    {
      "epoch": 14.910595537165657,
      "grad_norm": 1.4255092144012451,
      "learning_rate": 1.2723880597014926e-05,
      "loss": 0.6647,
      "step": 100900
    },
    {
      "epoch": 14.925373134328359,
      "grad_norm": 0.8349853754043579,
      "learning_rate": 1.2686936604108174e-05,
      "loss": 0.6552,
      "step": 101000
    },
    {
      "epoch": 14.940150731491059,
      "grad_norm": 1.128580927848816,
      "learning_rate": 1.264999261120142e-05,
      "loss": 0.6769,
      "step": 101100
    },
    {
      "epoch": 14.95492832865376,
      "grad_norm": 1.1174442768096924,
      "learning_rate": 1.2613048618294665e-05,
      "loss": 0.6463,
      "step": 101200
    },
    {
      "epoch": 14.969705925816463,
      "grad_norm": 1.192617416381836,
      "learning_rate": 1.2576104625387914e-05,
      "loss": 0.6561,
      "step": 101300
    },
    {
      "epoch": 14.984483522979163,
      "grad_norm": 1.2592500448226929,
      "learning_rate": 1.253916063248116e-05,
      "loss": 0.6789,
      "step": 101400
    },
    {
      "epoch": 14.999261120141865,
      "grad_norm": 0.8922826647758484,
      "learning_rate": 1.2502216639574405e-05,
      "loss": 0.6471,
      "step": 101500
    },
    {
      "epoch": 15.014038717304567,
      "grad_norm": 1.2102266550064087,
      "learning_rate": 1.2465272646667653e-05,
      "loss": 0.6667,
      "step": 101600
    },
    {
      "epoch": 15.028816314467267,
      "grad_norm": 1.1261674165725708,
      "learning_rate": 1.2428328653760899e-05,
      "loss": 0.6773,
      "step": 101700
    },
    {
      "epoch": 15.04359391162997,
      "grad_norm": 1.3934179544448853,
      "learning_rate": 1.2391384660854146e-05,
      "loss": 0.6544,
      "step": 101800
    },
    {
      "epoch": 15.05837150879267,
      "grad_norm": 1.013023853302002,
      "learning_rate": 1.2354440667947393e-05,
      "loss": 0.6505,
      "step": 101900
    },
    {
      "epoch": 15.073149105955371,
      "grad_norm": 1.388055443763733,
      "learning_rate": 1.231749667504064e-05,
      "loss": 0.6702,
      "step": 102000
    },
    {
      "epoch": 15.073149105955371,
      "eval_loss": 0.6457504034042358,
      "eval_runtime": 73.051,
      "eval_samples_per_second": 164.679,
      "eval_steps_per_second": 20.588,
      "step": 102000
    },
    {
      "epoch": 15.087926703118073,
      "grad_norm": 0.9007826447486877,
      "learning_rate": 1.2280552682133887e-05,
      "loss": 0.6139,
      "step": 102100
    },
    {
      "epoch": 15.102704300280774,
      "grad_norm": 0.8934223055839539,
      "learning_rate": 1.2243608689227132e-05,
      "loss": 0.6666,
      "step": 102200
    },
    {
      "epoch": 15.117481897443476,
      "grad_norm": 0.8211014866828918,
      "learning_rate": 1.220666469632038e-05,
      "loss": 0.6732,
      "step": 102300
    },
    {
      "epoch": 15.132259494606178,
      "grad_norm": 1.1160913705825806,
      "learning_rate": 1.2169720703413626e-05,
      "loss": 0.6433,
      "step": 102400
    },
    {
      "epoch": 15.147037091768878,
      "grad_norm": 0.9866610169410706,
      "learning_rate": 1.2132776710506872e-05,
      "loss": 0.6473,
      "step": 102500
    },
    {
      "epoch": 15.16181468893158,
      "grad_norm": 1.1354626417160034,
      "learning_rate": 1.2095832717600119e-05,
      "loss": 0.6344,
      "step": 102600
    },
    {
      "epoch": 15.176592286094282,
      "grad_norm": 0.7237865924835205,
      "learning_rate": 1.2058888724693366e-05,
      "loss": 0.6524,
      "step": 102700
    },
    {
      "epoch": 15.191369883256982,
      "grad_norm": 0.9748188853263855,
      "learning_rate": 1.2021944731786611e-05,
      "loss": 0.6487,
      "step": 102800
    },
    {
      "epoch": 15.206147480419684,
      "grad_norm": 1.0154788494110107,
      "learning_rate": 1.1985000738879858e-05,
      "loss": 0.63,
      "step": 102900
    },
    {
      "epoch": 15.220925077582384,
      "grad_norm": 0.882749617099762,
      "learning_rate": 1.1948056745973105e-05,
      "loss": 0.6661,
      "step": 103000
    },
    {
      "epoch": 15.235702674745086,
      "grad_norm": 1.168882131576538,
      "learning_rate": 1.191111275306635e-05,
      "loss": 0.6568,
      "step": 103100
    },
    {
      "epoch": 15.250480271907788,
      "grad_norm": 0.9606198668479919,
      "learning_rate": 1.1874168760159598e-05,
      "loss": 0.6366,
      "step": 103200
    },
    {
      "epoch": 15.265257869070489,
      "grad_norm": 0.7921802997589111,
      "learning_rate": 1.1837224767252845e-05,
      "loss": 0.6655,
      "step": 103300
    },
    {
      "epoch": 15.28003546623319,
      "grad_norm": 1.269914150238037,
      "learning_rate": 1.1800280774346092e-05,
      "loss": 0.6692,
      "step": 103400
    },
    {
      "epoch": 15.294813063395893,
      "grad_norm": 0.8794090151786804,
      "learning_rate": 1.1763336781439337e-05,
      "loss": 0.6538,
      "step": 103500
    },
    {
      "epoch": 15.309590660558593,
      "grad_norm": 1.3002068996429443,
      "learning_rate": 1.1726392788532584e-05,
      "loss": 0.6793,
      "step": 103600
    },
    {
      "epoch": 15.324368257721295,
      "grad_norm": 0.9572145342826843,
      "learning_rate": 1.1689448795625831e-05,
      "loss": 0.6765,
      "step": 103700
    },
    {
      "epoch": 15.339145854883995,
      "grad_norm": 0.9985772967338562,
      "learning_rate": 1.1652504802719079e-05,
      "loss": 0.6893,
      "step": 103800
    },
    {
      "epoch": 15.353923452046697,
      "grad_norm": 0.8060305714607239,
      "learning_rate": 1.1615560809812324e-05,
      "loss": 0.667,
      "step": 103900
    },
    {
      "epoch": 15.368701049209399,
      "grad_norm": 1.2226009368896484,
      "learning_rate": 1.1578616816905571e-05,
      "loss": 0.6436,
      "step": 104000
    },
    {
      "epoch": 15.368701049209399,
      "eval_loss": 0.6432155966758728,
      "eval_runtime": 71.9411,
      "eval_samples_per_second": 167.22,
      "eval_steps_per_second": 20.906,
      "step": 104000
    },
    {
      "epoch": 15.3834786463721,
      "grad_norm": 1.18367600440979,
      "learning_rate": 1.1541672823998818e-05,
      "loss": 0.6624,
      "step": 104100
    },
    {
      "epoch": 15.398256243534801,
      "grad_norm": 1.1838651895523071,
      "learning_rate": 1.1504728831092065e-05,
      "loss": 0.6631,
      "step": 104200
    },
    {
      "epoch": 15.413033840697503,
      "grad_norm": 0.895575225353241,
      "learning_rate": 1.1467784838185312e-05,
      "loss": 0.6713,
      "step": 104300
    },
    {
      "epoch": 15.427811437860203,
      "grad_norm": 0.9390720725059509,
      "learning_rate": 1.1430840845278558e-05,
      "loss": 0.6785,
      "step": 104400
    },
    {
      "epoch": 15.442589035022905,
      "grad_norm": 1.2333223819732666,
      "learning_rate": 1.1393896852371805e-05,
      "loss": 0.6606,
      "step": 104500
    },
    {
      "epoch": 15.457366632185607,
      "grad_norm": 1.0195252895355225,
      "learning_rate": 1.1356952859465052e-05,
      "loss": 0.653,
      "step": 104600
    },
    {
      "epoch": 15.472144229348308,
      "grad_norm": 1.0237059593200684,
      "learning_rate": 1.1320008866558299e-05,
      "loss": 0.6548,
      "step": 104700
    },
    {
      "epoch": 15.48692182651101,
      "grad_norm": 0.8073248863220215,
      "learning_rate": 1.1283064873651544e-05,
      "loss": 0.6817,
      "step": 104800
    },
    {
      "epoch": 15.50169942367371,
      "grad_norm": 1.0267595052719116,
      "learning_rate": 1.1246120880744791e-05,
      "loss": 0.6604,
      "step": 104900
    },
    {
      "epoch": 15.516477020836412,
      "grad_norm": 0.9435787796974182,
      "learning_rate": 1.1209176887838038e-05,
      "loss": 0.6581,
      "step": 105000
    },
    {
      "epoch": 15.531254617999114,
      "grad_norm": 1.280347228050232,
      "learning_rate": 1.1172232894931285e-05,
      "loss": 0.6616,
      "step": 105100
    },
    {
      "epoch": 15.546032215161814,
      "grad_norm": 1.1889525651931763,
      "learning_rate": 1.113528890202453e-05,
      "loss": 0.632,
      "step": 105200
    },
    {
      "epoch": 15.560809812324516,
      "grad_norm": 1.2797421216964722,
      "learning_rate": 1.1098344909117778e-05,
      "loss": 0.6405,
      "step": 105300
    },
    {
      "epoch": 15.575587409487218,
      "grad_norm": 1.2201236486434937,
      "learning_rate": 1.1061400916211025e-05,
      "loss": 0.6831,
      "step": 105400
    },
    {
      "epoch": 15.590365006649918,
      "grad_norm": 0.8531402945518494,
      "learning_rate": 1.1024456923304272e-05,
      "loss": 0.6505,
      "step": 105500
    },
    {
      "epoch": 15.60514260381262,
      "grad_norm": 1.0496643781661987,
      "learning_rate": 1.0987512930397519e-05,
      "loss": 0.673,
      "step": 105600
    },
    {
      "epoch": 15.619920200975322,
      "grad_norm": 1.1250848770141602,
      "learning_rate": 1.0950568937490764e-05,
      "loss": 0.6728,
      "step": 105700
    },
    {
      "epoch": 15.634697798138022,
      "grad_norm": 1.1391541957855225,
      "learning_rate": 1.0913624944584011e-05,
      "loss": 0.6426,
      "step": 105800
    },
    {
      "epoch": 15.649475395300724,
      "grad_norm": 1.3102085590362549,
      "learning_rate": 1.0876680951677258e-05,
      "loss": 0.6726,
      "step": 105900
    },
    {
      "epoch": 15.664252992463425,
      "grad_norm": 1.2501466274261475,
      "learning_rate": 1.0839736958770505e-05,
      "loss": 0.6697,
      "step": 106000
    },
    {
      "epoch": 15.664252992463425,
      "eval_loss": 0.6428406834602356,
      "eval_runtime": 78.0065,
      "eval_samples_per_second": 154.218,
      "eval_steps_per_second": 19.28,
      "step": 106000
    },
    {
      "epoch": 15.679030589626127,
      "grad_norm": 0.816891610622406,
      "learning_rate": 1.080279296586375e-05,
      "loss": 0.6686,
      "step": 106100
    },
    {
      "epoch": 15.693808186788829,
      "grad_norm": 1.5934269428253174,
      "learning_rate": 1.0765848972956998e-05,
      "loss": 0.662,
      "step": 106200
    },
    {
      "epoch": 15.708585783951529,
      "grad_norm": 1.2002370357513428,
      "learning_rate": 1.0728904980050245e-05,
      "loss": 0.6307,
      "step": 106300
    },
    {
      "epoch": 15.72336338111423,
      "grad_norm": 0.985430121421814,
      "learning_rate": 1.0691960987143492e-05,
      "loss": 0.6655,
      "step": 106400
    },
    {
      "epoch": 15.738140978276933,
      "grad_norm": 1.002516508102417,
      "learning_rate": 1.0655016994236737e-05,
      "loss": 0.6509,
      "step": 106500
    },
    {
      "epoch": 15.752918575439633,
      "grad_norm": 1.1462314128875732,
      "learning_rate": 1.0618073001329984e-05,
      "loss": 0.639,
      "step": 106600
    },
    {
      "epoch": 15.767696172602335,
      "grad_norm": 1.2473591566085815,
      "learning_rate": 1.0581129008423231e-05,
      "loss": 0.6521,
      "step": 106700
    },
    {
      "epoch": 15.782473769765037,
      "grad_norm": 0.8752203583717346,
      "learning_rate": 1.0544185015516478e-05,
      "loss": 0.6692,
      "step": 106800
    },
    {
      "epoch": 15.797251366927737,
      "grad_norm": 0.8359528183937073,
      "learning_rate": 1.0507241022609726e-05,
      "loss": 0.641,
      "step": 106900
    },
    {
      "epoch": 15.81202896409044,
      "grad_norm": 1.3582048416137695,
      "learning_rate": 1.0470297029702971e-05,
      "loss": 0.6471,
      "step": 107000
    },
    {
      "epoch": 15.82680656125314,
      "grad_norm": 0.8919937014579773,
      "learning_rate": 1.0433353036796218e-05,
      "loss": 0.6253,
      "step": 107100
    },
    {
      "epoch": 15.841584158415841,
      "grad_norm": 1.0836553573608398,
      "learning_rate": 1.0396409043889465e-05,
      "loss": 0.6663,
      "step": 107200
    },
    {
      "epoch": 15.856361755578543,
      "grad_norm": 1.084720492362976,
      "learning_rate": 1.035946505098271e-05,
      "loss": 0.6493,
      "step": 107300
    },
    {
      "epoch": 15.871139352741244,
      "grad_norm": 1.138200283050537,
      "learning_rate": 1.0322521058075957e-05,
      "loss": 0.6504,
      "step": 107400
    },
    {
      "epoch": 15.885916949903946,
      "grad_norm": 1.253187894821167,
      "learning_rate": 1.0285577065169205e-05,
      "loss": 0.656,
      "step": 107500
    },
    {
      "epoch": 15.900694547066648,
      "grad_norm": 1.3272185325622559,
      "learning_rate": 1.024863307226245e-05,
      "loss": 0.6374,
      "step": 107600
    },
    {
      "epoch": 15.915472144229348,
      "grad_norm": 1.1696805953979492,
      "learning_rate": 1.0211689079355697e-05,
      "loss": 0.6761,
      "step": 107700
    },
    {
      "epoch": 15.93024974139205,
      "grad_norm": 0.8434779644012451,
      "learning_rate": 1.0174745086448944e-05,
      "loss": 0.6612,
      "step": 107800
    },
    {
      "epoch": 15.945027338554752,
      "grad_norm": 1.0184563398361206,
      "learning_rate": 1.013780109354219e-05,
      "loss": 0.6734,
      "step": 107900
    },
    {
      "epoch": 15.959804935717452,
      "grad_norm": 0.9920335412025452,
      "learning_rate": 1.0100857100635436e-05,
      "loss": 0.6388,
      "step": 108000
    },
    {
      "epoch": 15.959804935717452,
      "eval_loss": 0.6434669494628906,
      "eval_runtime": 74.9022,
      "eval_samples_per_second": 160.61,
      "eval_steps_per_second": 20.08,
      "step": 108000
    },
    {
      "epoch": 15.974582532880154,
      "grad_norm": 0.9830268621444702,
      "learning_rate": 1.0063913107728684e-05,
      "loss": 0.6392,
      "step": 108100
    },
    {
      "epoch": 15.989360130042854,
      "grad_norm": 1.1439255475997925,
      "learning_rate": 1.002696911482193e-05,
      "loss": 0.6555,
      "step": 108200
    },
    {
      "epoch": 16.004137727205556,
      "grad_norm": 0.8784761428833008,
      "learning_rate": 9.990025121915176e-06,
      "loss": 0.6611,
      "step": 108300
    },
    {
      "epoch": 16.01891532436826,
      "grad_norm": 0.9967791438102722,
      "learning_rate": 9.953081129008423e-06,
      "loss": 0.6527,
      "step": 108400
    },
    {
      "epoch": 16.03369292153096,
      "grad_norm": 1.1224321126937866,
      "learning_rate": 9.91613713610167e-06,
      "loss": 0.6717,
      "step": 108500
    },
    {
      "epoch": 16.04847051869366,
      "grad_norm": 1.2320295572280884,
      "learning_rate": 9.879193143194917e-06,
      "loss": 0.6405,
      "step": 108600
    },
    {
      "epoch": 16.06324811585636,
      "grad_norm": 0.9814786314964294,
      "learning_rate": 9.842249150288162e-06,
      "loss": 0.6638,
      "step": 108700
    },
    {
      "epoch": 16.078025713019063,
      "grad_norm": 0.8934978246688843,
      "learning_rate": 9.80530515738141e-06,
      "loss": 0.6546,
      "step": 108800
    },
    {
      "epoch": 16.092803310181765,
      "grad_norm": 0.9925720691680908,
      "learning_rate": 9.768361164474657e-06,
      "loss": 0.6473,
      "step": 108900
    },
    {
      "epoch": 16.107580907344467,
      "grad_norm": 1.150923252105713,
      "learning_rate": 9.731417171567904e-06,
      "loss": 0.6393,
      "step": 109000
    },
    {
      "epoch": 16.12235850450717,
      "grad_norm": 0.8887282013893127,
      "learning_rate": 9.694473178661149e-06,
      "loss": 0.6364,
      "step": 109100
    },
    {
      "epoch": 16.137136101669867,
      "grad_norm": 0.9553865790367126,
      "learning_rate": 9.657529185754396e-06,
      "loss": 0.659,
      "step": 109200
    },
    {
      "epoch": 16.15191369883257,
      "grad_norm": 1.1606794595718384,
      "learning_rate": 9.620585192847643e-06,
      "loss": 0.6393,
      "step": 109300
    },
    {
      "epoch": 16.16669129599527,
      "grad_norm": 1.0795485973358154,
      "learning_rate": 9.58364119994089e-06,
      "loss": 0.6344,
      "step": 109400
    },
    {
      "epoch": 16.181468893157973,
      "grad_norm": 1.1172174215316772,
      "learning_rate": 9.546697207034137e-06,
      "loss": 0.6443,
      "step": 109500
    },
    {
      "epoch": 16.196246490320675,
      "grad_norm": 0.8731644153594971,
      "learning_rate": 9.509753214127383e-06,
      "loss": 0.651,
      "step": 109600
    },
    {
      "epoch": 16.211024087483374,
      "grad_norm": 1.1041953563690186,
      "learning_rate": 9.47280922122063e-06,
      "loss": 0.6637,
      "step": 109700
    },
    {
      "epoch": 16.225801684646076,
      "grad_norm": 1.1215054988861084,
      "learning_rate": 9.435865228313877e-06,
      "loss": 0.6556,
      "step": 109800
    },
    {
      "epoch": 16.240579281808778,
      "grad_norm": 0.9795853495597839,
      "learning_rate": 9.398921235407124e-06,
      "loss": 0.6454,
      "step": 109900
    },
    {
      "epoch": 16.25535687897148,
      "grad_norm": 1.0162864923477173,
      "learning_rate": 9.36197724250037e-06,
      "loss": 0.6656,
      "step": 110000
    },
    {
      "epoch": 16.25535687897148,
      "eval_loss": 0.6402543783187866,
      "eval_runtime": 77.1458,
      "eval_samples_per_second": 155.939,
      "eval_steps_per_second": 19.496,
      "step": 110000
    },
    {
      "epoch": 16.27013447613418,
      "grad_norm": 0.9097892045974731,
      "learning_rate": 9.325033249593616e-06,
      "loss": 0.6283,
      "step": 110100
    },
    {
      "epoch": 16.284912073296884,
      "grad_norm": 0.7424737811088562,
      "learning_rate": 9.288089256686863e-06,
      "loss": 0.6569,
      "step": 110200
    },
    {
      "epoch": 16.299689670459582,
      "grad_norm": 1.1081472635269165,
      "learning_rate": 9.25114526378011e-06,
      "loss": 0.6634,
      "step": 110300
    },
    {
      "epoch": 16.314467267622284,
      "grad_norm": 0.859665036201477,
      "learning_rate": 9.214201270873356e-06,
      "loss": 0.6457,
      "step": 110400
    },
    {
      "epoch": 16.329244864784986,
      "grad_norm": 0.7625141143798828,
      "learning_rate": 9.177257277966603e-06,
      "loss": 0.6746,
      "step": 110500
    },
    {
      "epoch": 16.344022461947688,
      "grad_norm": 1.0860724449157715,
      "learning_rate": 9.14031328505985e-06,
      "loss": 0.6255,
      "step": 110600
    },
    {
      "epoch": 16.35880005911039,
      "grad_norm": 1.003368616104126,
      "learning_rate": 9.103369292153097e-06,
      "loss": 0.6533,
      "step": 110700
    },
    {
      "epoch": 16.37357765627309,
      "grad_norm": 1.2987687587738037,
      "learning_rate": 9.066425299246344e-06,
      "loss": 0.6402,
      "step": 110800
    },
    {
      "epoch": 16.38835525343579,
      "grad_norm": 1.3438324928283691,
      "learning_rate": 9.02948130633959e-06,
      "loss": 0.6416,
      "step": 110900
    },
    {
      "epoch": 16.403132850598492,
      "grad_norm": 0.9161988496780396,
      "learning_rate": 8.992537313432836e-06,
      "loss": 0.6067,
      "step": 111000
    },
    {
      "epoch": 16.417910447761194,
      "grad_norm": 1.2037044763565063,
      "learning_rate": 8.955593320526083e-06,
      "loss": 0.609,
      "step": 111100
    },
    {
      "epoch": 16.432688044923896,
      "grad_norm": 1.0108846426010132,
      "learning_rate": 8.91864932761933e-06,
      "loss": 0.6492,
      "step": 111200
    },
    {
      "epoch": 16.4474656420866,
      "grad_norm": 1.1155284643173218,
      "learning_rate": 8.881705334712576e-06,
      "loss": 0.649,
      "step": 111300
    },
    {
      "epoch": 16.462243239249297,
      "grad_norm": 1.248159646987915,
      "learning_rate": 8.844761341805823e-06,
      "loss": 0.6612,
      "step": 111400
    },
    {
      "epoch": 16.477020836412,
      "grad_norm": 0.9180518984794617,
      "learning_rate": 8.80781734889907e-06,
      "loss": 0.6278,
      "step": 111500
    },
    {
      "epoch": 16.4917984335747,
      "grad_norm": 0.8325663208961487,
      "learning_rate": 8.770873355992317e-06,
      "loss": 0.6676,
      "step": 111600
    },
    {
      "epoch": 16.506576030737403,
      "grad_norm": 1.4253629446029663,
      "learning_rate": 8.733929363085562e-06,
      "loss": 0.6603,
      "step": 111700
    },
    {
      "epoch": 16.521353627900105,
      "grad_norm": 1.300703525543213,
      "learning_rate": 8.69698537017881e-06,
      "loss": 0.6495,
      "step": 111800
    },
    {
      "epoch": 16.536131225062803,
      "grad_norm": 1.5597137212753296,
      "learning_rate": 8.660041377272057e-06,
      "loss": 0.6233,
      "step": 111900
    },
    {
      "epoch": 16.550908822225505,
      "grad_norm": 1.044168472290039,
      "learning_rate": 8.623097384365304e-06,
      "loss": 0.6472,
      "step": 112000
    },
    {
      "epoch": 16.550908822225505,
      "eval_loss": 0.6368250250816345,
      "eval_runtime": 75.272,
      "eval_samples_per_second": 159.82,
      "eval_steps_per_second": 19.981,
      "step": 112000
    },
    {
      "epoch": 16.565686419388207,
      "grad_norm": 0.8322375416755676,
      "learning_rate": 8.58615339145855e-06,
      "loss": 0.6404,
      "step": 112100
    },
    {
      "epoch": 16.58046401655091,
      "grad_norm": 0.7611870765686035,
      "learning_rate": 8.549209398551796e-06,
      "loss": 0.6392,
      "step": 112200
    },
    {
      "epoch": 16.59524161371361,
      "grad_norm": 1.1770496368408203,
      "learning_rate": 8.512265405645043e-06,
      "loss": 0.6696,
      "step": 112300
    },
    {
      "epoch": 16.610019210876313,
      "grad_norm": 0.7712723612785339,
      "learning_rate": 8.47532141273829e-06,
      "loss": 0.6355,
      "step": 112400
    },
    {
      "epoch": 16.62479680803901,
      "grad_norm": 1.1193760633468628,
      "learning_rate": 8.438377419831536e-06,
      "loss": 0.6477,
      "step": 112500
    },
    {
      "epoch": 16.639574405201714,
      "grad_norm": 0.9246242642402649,
      "learning_rate": 8.401433426924783e-06,
      "loss": 0.6242,
      "step": 112600
    },
    {
      "epoch": 16.654352002364416,
      "grad_norm": 1.2202982902526855,
      "learning_rate": 8.36448943401803e-06,
      "loss": 0.651,
      "step": 112700
    },
    {
      "epoch": 16.669129599527118,
      "grad_norm": 0.9914655685424805,
      "learning_rate": 8.327545441111275e-06,
      "loss": 0.6491,
      "step": 112800
    },
    {
      "epoch": 16.68390719668982,
      "grad_norm": 1.2211405038833618,
      "learning_rate": 8.290601448204522e-06,
      "loss": 0.6743,
      "step": 112900
    },
    {
      "epoch": 16.698684793852518,
      "grad_norm": 0.8243391513824463,
      "learning_rate": 8.253657455297769e-06,
      "loss": 0.6401,
      "step": 113000
    },
    {
      "epoch": 16.71346239101522,
      "grad_norm": 1.0546472072601318,
      "learning_rate": 8.216713462391015e-06,
      "loss": 0.622,
      "step": 113100
    },
    {
      "epoch": 16.728239988177922,
      "grad_norm": 1.2236030101776123,
      "learning_rate": 8.179769469484262e-06,
      "loss": 0.6485,
      "step": 113200
    },
    {
      "epoch": 16.743017585340624,
      "grad_norm": 0.9211701154708862,
      "learning_rate": 8.142825476577509e-06,
      "loss": 0.6695,
      "step": 113300
    },
    {
      "epoch": 16.757795182503326,
      "grad_norm": 1.0149118900299072,
      "learning_rate": 8.105881483670756e-06,
      "loss": 0.6431,
      "step": 113400
    },
    {
      "epoch": 16.772572779666028,
      "grad_norm": 1.1124719381332397,
      "learning_rate": 8.068937490764001e-06,
      "loss": 0.6619,
      "step": 113500
    },
    {
      "epoch": 16.787350376828726,
      "grad_norm": 1.533900499343872,
      "learning_rate": 8.031993497857248e-06,
      "loss": 0.6574,
      "step": 113600
    },
    {
      "epoch": 16.80212797399143,
      "grad_norm": 1.3272560834884644,
      "learning_rate": 7.995049504950495e-06,
      "loss": 0.6287,
      "step": 113700
    },
    {
      "epoch": 16.81690557115413,
      "grad_norm": 0.891067385673523,
      "learning_rate": 7.958105512043742e-06,
      "loss": 0.6177,
      "step": 113800
    },
    {
      "epoch": 16.831683168316832,
      "grad_norm": 0.9139746427536011,
      "learning_rate": 7.921161519136988e-06,
      "loss": 0.6707,
      "step": 113900
    },
    {
      "epoch": 16.846460765479534,
      "grad_norm": 1.0487879514694214,
      "learning_rate": 7.884217526230235e-06,
      "loss": 0.6398,
      "step": 114000
    },
    {
      "epoch": 16.846460765479534,
      "eval_loss": 0.6397036910057068,
      "eval_runtime": 71.9452,
      "eval_samples_per_second": 167.211,
      "eval_steps_per_second": 20.905,
      "step": 114000
    },
    {
      "epoch": 16.861238362642233,
      "grad_norm": 1.1913973093032837,
      "learning_rate": 7.847273533323482e-06,
      "loss": 0.6536,
      "step": 114100
    },
    {
      "epoch": 16.876015959804935,
      "grad_norm": 1.0558212995529175,
      "learning_rate": 7.810329540416729e-06,
      "loss": 0.6586,
      "step": 114200
    },
    {
      "epoch": 16.890793556967637,
      "grad_norm": 1.3992843627929688,
      "learning_rate": 7.773385547509976e-06,
      "loss": 0.6576,
      "step": 114300
    },
    {
      "epoch": 16.90557115413034,
      "grad_norm": 0.9202985167503357,
      "learning_rate": 7.736441554603221e-06,
      "loss": 0.6493,
      "step": 114400
    },
    {
      "epoch": 16.92034875129304,
      "grad_norm": 0.6877183318138123,
      "learning_rate": 7.699497561696468e-06,
      "loss": 0.6543,
      "step": 114500
    },
    {
      "epoch": 16.93512634845574,
      "grad_norm": 0.8959941267967224,
      "learning_rate": 7.662553568789715e-06,
      "loss": 0.6386,
      "step": 114600
    },
    {
      "epoch": 16.94990394561844,
      "grad_norm": 0.9729301333427429,
      "learning_rate": 7.625609575882962e-06,
      "loss": 0.6275,
      "step": 114700
    },
    {
      "epoch": 16.964681542781143,
      "grad_norm": 1.1122559309005737,
      "learning_rate": 7.588665582976208e-06,
      "loss": 0.6417,
      "step": 114800
    },
    {
      "epoch": 16.979459139943845,
      "grad_norm": 1.3454537391662598,
      "learning_rate": 7.551721590069455e-06,
      "loss": 0.6699,
      "step": 114900
    },
    {
      "epoch": 16.994236737106547,
      "grad_norm": 0.9774417877197266,
      "learning_rate": 7.514777597162702e-06,
      "loss": 0.6284,
      "step": 115000
    },
    {
      "epoch": 17.00901433426925,
      "grad_norm": 1.2625668048858643,
      "learning_rate": 7.477833604255949e-06,
      "loss": 0.6707,
      "step": 115100
    },
    {
      "epoch": 17.023791931431948,
      "grad_norm": 1.0181334018707275,
      "learning_rate": 7.440889611349194e-06,
      "loss": 0.6393,
      "step": 115200
    },
    {
      "epoch": 17.03856952859465,
      "grad_norm": 1.0627385377883911,
      "learning_rate": 7.403945618442441e-06,
      "loss": 0.6661,
      "step": 115300
    },
    {
      "epoch": 17.05334712575735,
      "grad_norm": 1.0944210290908813,
      "learning_rate": 7.367001625535688e-06,
      "loss": 0.6367,
      "step": 115400
    },
    {
      "epoch": 17.068124722920054,
      "grad_norm": 1.067641019821167,
      "learning_rate": 7.3300576326289355e-06,
      "loss": 0.6766,
      "step": 115500
    },
    {
      "epoch": 17.082902320082756,
      "grad_norm": 1.198936104774475,
      "learning_rate": 7.2931136397221825e-06,
      "loss": 0.6476,
      "step": 115600
    },
    {
      "epoch": 17.097679917245458,
      "grad_norm": 1.0219557285308838,
      "learning_rate": 7.256169646815428e-06,
      "loss": 0.6424,
      "step": 115700
    },
    {
      "epoch": 17.112457514408156,
      "grad_norm": 1.114729642868042,
      "learning_rate": 7.219225653908675e-06,
      "loss": 0.6168,
      "step": 115800
    },
    {
      "epoch": 17.127235111570858,
      "grad_norm": 1.1806442737579346,
      "learning_rate": 7.182281661001922e-06,
      "loss": 0.6533,
      "step": 115900
    },
    {
      "epoch": 17.14201270873356,
      "grad_norm": 1.2793338298797607,
      "learning_rate": 7.145337668095168e-06,
      "loss": 0.6406,
      "step": 116000
    },
    {
      "epoch": 17.14201270873356,
      "eval_loss": 0.6374136209487915,
      "eval_runtime": 75.1696,
      "eval_samples_per_second": 160.038,
      "eval_steps_per_second": 20.008,
      "step": 116000
    },
    {
      "epoch": 17.156790305896262,
      "grad_norm": 0.9766043424606323,
      "learning_rate": 7.1083936751884145e-06,
      "loss": 0.6391,
      "step": 116100
    },
    {
      "epoch": 17.171567903058964,
      "grad_norm": 1.1517508029937744,
      "learning_rate": 7.0714496822816615e-06,
      "loss": 0.6537,
      "step": 116200
    },
    {
      "epoch": 17.186345500221663,
      "grad_norm": 1.053453803062439,
      "learning_rate": 7.034505689374908e-06,
      "loss": 0.6403,
      "step": 116300
    },
    {
      "epoch": 17.201123097384365,
      "grad_norm": 0.7436836957931519,
      "learning_rate": 6.997561696468155e-06,
      "loss": 0.624,
      "step": 116400
    },
    {
      "epoch": 17.215900694547067,
      "grad_norm": 1.0293967723846436,
      "learning_rate": 6.960617703561401e-06,
      "loss": 0.6439,
      "step": 116500
    },
    {
      "epoch": 17.23067829170977,
      "grad_norm": 1.392637848854065,
      "learning_rate": 6.923673710654647e-06,
      "loss": 0.6509,
      "step": 116600
    },
    {
      "epoch": 17.24545588887247,
      "grad_norm": 1.2619218826293945,
      "learning_rate": 6.886729717747894e-06,
      "loss": 0.6256,
      "step": 116700
    },
    {
      "epoch": 17.26023348603517,
      "grad_norm": 1.2116444110870361,
      "learning_rate": 6.849785724841141e-06,
      "loss": 0.6215,
      "step": 116800
    },
    {
      "epoch": 17.27501108319787,
      "grad_norm": 1.465928316116333,
      "learning_rate": 6.812841731934388e-06,
      "loss": 0.6315,
      "step": 116900
    },
    {
      "epoch": 17.289788680360573,
      "grad_norm": 1.1879576444625854,
      "learning_rate": 6.775897739027634e-06,
      "loss": 0.663,
      "step": 117000
    },
    {
      "epoch": 17.304566277523275,
      "grad_norm": 1.1232891082763672,
      "learning_rate": 6.738953746120881e-06,
      "loss": 0.6553,
      "step": 117100
    },
    {
      "epoch": 17.319343874685977,
      "grad_norm": 1.0246933698654175,
      "learning_rate": 6.702009753214128e-06,
      "loss": 0.6483,
      "step": 117200
    },
    {
      "epoch": 17.33412147184868,
      "grad_norm": 1.3287198543548584,
      "learning_rate": 6.665065760307375e-06,
      "loss": 0.6438,
      "step": 117300
    },
    {
      "epoch": 17.348899069011377,
      "grad_norm": 1.0852999687194824,
      "learning_rate": 6.62812176740062e-06,
      "loss": 0.6215,
      "step": 117400
    },
    {
      "epoch": 17.36367666617408,
      "grad_norm": 1.5611902475357056,
      "learning_rate": 6.591177774493867e-06,
      "loss": 0.6681,
      "step": 117500
    },
    {
      "epoch": 17.37845426333678,
      "grad_norm": 1.333229422569275,
      "learning_rate": 6.5542337815871144e-06,
      "loss": 0.6532,
      "step": 117600
    },
    {
      "epoch": 17.393231860499483,
      "grad_norm": 1.0756841897964478,
      "learning_rate": 6.5172897886803615e-06,
      "loss": 0.6268,
      "step": 117700
    },
    {
      "epoch": 17.408009457662185,
      "grad_norm": 0.8677040934562683,
      "learning_rate": 6.480345795773607e-06,
      "loss": 0.6317,
      "step": 117800
    },
    {
      "epoch": 17.422787054824884,
      "grad_norm": 1.1451975107192993,
      "learning_rate": 6.443401802866854e-06,
      "loss": 0.6502,
      "step": 117900
    },
    {
      "epoch": 17.437564651987586,
      "grad_norm": 0.8959707617759705,
      "learning_rate": 6.406457809960101e-06,
      "loss": 0.629,
      "step": 118000
    },
    {
      "epoch": 17.437564651987586,
      "eval_loss": 0.6369349956512451,
      "eval_runtime": 75.431,
      "eval_samples_per_second": 159.483,
      "eval_steps_per_second": 19.939,
      "step": 118000
    },
    {
      "epoch": 17.452342249150288,
      "grad_norm": 1.1437245607376099,
      "learning_rate": 2.09130092113689e-05,
      "loss": 0.6376,
      "step": 118100
    },
    {
      "epoch": 17.46711984631299,
      "grad_norm": 1.0323675870895386,
      "learning_rate": 2.0888379882764396e-05,
      "loss": 0.629,
      "step": 118200
    },
    {
      "epoch": 17.48189744347569,
      "grad_norm": 1.7063288688659668,
      "learning_rate": 2.0863750554159893e-05,
      "loss": 0.6472,
      "step": 118300
    },
    {
      "epoch": 17.496675040638394,
      "grad_norm": 0.9529716372489929,
      "learning_rate": 2.0839121225555393e-05,
      "loss": 0.6358,
      "step": 118400
    },
    {
      "epoch": 17.511452637801092,
      "grad_norm": 1.0816396474838257,
      "learning_rate": 2.081449189695089e-05,
      "loss": 0.6321,
      "step": 118500
    },
    {
      "epoch": 17.526230234963794,
      "grad_norm": 0.7799069881439209,
      "learning_rate": 2.0789862568346387e-05,
      "loss": 0.65,
      "step": 118600
    },
    {
      "epoch": 17.541007832126496,
      "grad_norm": 0.6665335893630981,
      "learning_rate": 2.0765233239741884e-05,
      "loss": 0.6281,
      "step": 118700
    },
    {
      "epoch": 17.555785429289198,
      "grad_norm": 1.158642292022705,
      "learning_rate": 2.074060391113738e-05,
      "loss": 0.6387,
      "step": 118800
    },
    {
      "epoch": 17.5705630264519,
      "grad_norm": 1.0780025720596313,
      "learning_rate": 2.071597458253288e-05,
      "loss": 0.6082,
      "step": 118900
    },
    {
      "epoch": 17.5853406236146,
      "grad_norm": 1.0887185335159302,
      "learning_rate": 2.0691345253928378e-05,
      "loss": 0.657,
      "step": 119000
    },
    {
      "epoch": 17.6001182207773,
      "grad_norm": 0.8352630138397217,
      "learning_rate": 2.0666715925323875e-05,
      "loss": 0.6378,
      "step": 119100
    },
    {
      "epoch": 17.614895817940003,
      "grad_norm": 0.7933911085128784,
      "learning_rate": 2.0642086596719372e-05,
      "loss": 0.6378,
      "step": 119200
    },
    {
      "epoch": 17.629673415102705,
      "grad_norm": 1.101252794265747,
      "learning_rate": 2.0617457268114872e-05,
      "loss": 0.6552,
      "step": 119300
    },
    {
      "epoch": 17.644451012265407,
      "grad_norm": 0.9456673860549927,
      "learning_rate": 2.059282793951037e-05,
      "loss": 0.6525,
      "step": 119400
    },
    {
      "epoch": 17.65922860942811,
      "grad_norm": 1.010768175125122,
      "learning_rate": 2.0568198610905866e-05,
      "loss": 0.6325,
      "step": 119500
    },
    {
      "epoch": 17.674006206590807,
      "grad_norm": 0.9472885727882385,
      "learning_rate": 2.0543569282301363e-05,
      "loss": 0.6567,
      "step": 119600
    },
    {
      "epoch": 17.68878380375351,
      "grad_norm": 0.9218549728393555,
      "learning_rate": 2.0518939953696863e-05,
      "loss": 0.6609,
      "step": 119700
    },
    {
      "epoch": 17.70356140091621,
      "grad_norm": 1.1878597736358643,
      "learning_rate": 2.049431062509236e-05,
      "loss": 0.639,
      "step": 119800
    },
    {
      "epoch": 17.718338998078913,
      "grad_norm": 1.0938290357589722,
      "learning_rate": 2.0469681296487857e-05,
      "loss": 0.6241,
      "step": 119900
    },
    {
      "epoch": 17.733116595241615,
      "grad_norm": 1.1724640130996704,
      "learning_rate": 2.0445051967883354e-05,
      "loss": 0.6676,
      "step": 120000
    },
    {
      "epoch": 17.733116595241615,
      "eval_loss": 0.6369110345840454,
      "eval_runtime": 77.0434,
      "eval_samples_per_second": 156.146,
      "eval_steps_per_second": 19.521,
      "step": 120000
    },
    {
      "epoch": 17.747894192404313,
      "grad_norm": 1.283416986465454,
      "learning_rate": 2.0420422639278854e-05,
      "loss": 0.6531,
      "step": 120100
    },
    {
      "epoch": 17.762671789567015,
      "grad_norm": 1.2649716138839722,
      "learning_rate": 2.039579331067435e-05,
      "loss": 0.6484,
      "step": 120200
    },
    {
      "epoch": 17.777449386729717,
      "grad_norm": 0.8684986233711243,
      "learning_rate": 2.0371163982069848e-05,
      "loss": 0.6304,
      "step": 120300
    },
    {
      "epoch": 17.79222698389242,
      "grad_norm": 1.2059320211410522,
      "learning_rate": 2.0346534653465345e-05,
      "loss": 0.6268,
      "step": 120400
    },
    {
      "epoch": 17.80700458105512,
      "grad_norm": 0.8317687511444092,
      "learning_rate": 2.0321905324860845e-05,
      "loss": 0.6455,
      "step": 120500
    },
    {
      "epoch": 17.821782178217823,
      "grad_norm": 0.9495176076889038,
      "learning_rate": 2.0297275996256342e-05,
      "loss": 0.6378,
      "step": 120600
    },
    {
      "epoch": 17.836559775380522,
      "grad_norm": 1.1910525560379028,
      "learning_rate": 2.027264666765184e-05,
      "loss": 0.6331,
      "step": 120700
    },
    {
      "epoch": 17.851337372543224,
      "grad_norm": 1.0578669309616089,
      "learning_rate": 2.024801733904734e-05,
      "loss": 0.6123,
      "step": 120800
    },
    {
      "epoch": 17.866114969705926,
      "grad_norm": 0.8161836862564087,
      "learning_rate": 2.0223388010442836e-05,
      "loss": 0.6211,
      "step": 120900
    },
    {
      "epoch": 17.880892566868628,
      "grad_norm": 0.8231093287467957,
      "learning_rate": 2.0198758681838333e-05,
      "loss": 0.6228,
      "step": 121000
    },
    {
      "epoch": 17.89567016403133,
      "grad_norm": 1.282691478729248,
      "learning_rate": 2.017412935323383e-05,
      "loss": 0.6512,
      "step": 121100
    },
    {
      "epoch": 17.91044776119403,
      "grad_norm": 0.9583901762962341,
      "learning_rate": 2.014950002462933e-05,
      "loss": 0.6302,
      "step": 121200
    },
    {
      "epoch": 17.92522535835673,
      "grad_norm": 1.0703709125518799,
      "learning_rate": 2.0124870696024827e-05,
      "loss": 0.6557,
      "step": 121300
    },
    {
      "epoch": 17.940002955519432,
      "grad_norm": 1.4016971588134766,
      "learning_rate": 2.0100241367420324e-05,
      "loss": 0.6295,
      "step": 121400
    },
    {
      "epoch": 17.954780552682134,
      "grad_norm": 1.4476304054260254,
      "learning_rate": 2.007561203881582e-05,
      "loss": 0.6372,
      "step": 121500
    },
    {
      "epoch": 17.969558149844836,
      "grad_norm": 1.4620282649993896,
      "learning_rate": 2.005098271021132e-05,
      "loss": 0.6373,
      "step": 121600
    },
    {
      "epoch": 17.98433574700754,
      "grad_norm": 1.2443292140960693,
      "learning_rate": 2.0026353381606818e-05,
      "loss": 0.6412,
      "step": 121700
    },
    {
      "epoch": 17.999113344170237,
      "grad_norm": 1.309985637664795,
      "learning_rate": 2.0001724053002315e-05,
      "loss": 0.6634,
      "step": 121800
    },
    {
      "epoch": 18.01389094133294,
      "grad_norm": 0.8934074640274048,
      "learning_rate": 1.9977094724397812e-05,
      "loss": 0.6511,
      "step": 121900
    },
    {
      "epoch": 18.02866853849564,
      "grad_norm": 0.9690625071525574,
      "learning_rate": 1.9952465395793312e-05,
      "loss": 0.6416,
      "step": 122000
    },
    {
      "epoch": 18.02866853849564,
      "eval_loss": 0.6348153948783875,
      "eval_runtime": 75.2205,
      "eval_samples_per_second": 159.93,
      "eval_steps_per_second": 19.995,
      "step": 122000
    },
    {
      "epoch": 18.043446135658343,
      "grad_norm": 1.1812664270401,
      "learning_rate": 1.992783606718881e-05,
      "loss": 0.6192,
      "step": 122100
    },
    {
      "epoch": 18.058223732821045,
      "grad_norm": 0.8939969539642334,
      "learning_rate": 1.9903206738584306e-05,
      "loss": 0.6289,
      "step": 122200
    },
    {
      "epoch": 18.073001329983743,
      "grad_norm": 0.8672378063201904,
      "learning_rate": 1.9878577409979803e-05,
      "loss": 0.6236,
      "step": 122300
    },
    {
      "epoch": 18.087778927146445,
      "grad_norm": 1.1338547468185425,
      "learning_rate": 1.9853948081375303e-05,
      "loss": 0.6287,
      "step": 122400
    },
    {
      "epoch": 18.102556524309147,
      "grad_norm": 1.246146321296692,
      "learning_rate": 1.98293187527708e-05,
      "loss": 0.6545,
      "step": 122500
    },
    {
      "epoch": 18.11733412147185,
      "grad_norm": 1.5004076957702637,
      "learning_rate": 1.9804689424166297e-05,
      "loss": 0.6223,
      "step": 122600
    },
    {
      "epoch": 18.13211171863455,
      "grad_norm": 0.675942599773407,
      "learning_rate": 1.9780060095561794e-05,
      "loss": 0.6305,
      "step": 122700
    },
    {
      "epoch": 18.146889315797253,
      "grad_norm": 1.0479176044464111,
      "learning_rate": 1.9755430766957294e-05,
      "loss": 0.6173,
      "step": 122800
    },
    {
      "epoch": 18.16166691295995,
      "grad_norm": 0.9385790824890137,
      "learning_rate": 1.973080143835279e-05,
      "loss": 0.6389,
      "step": 122900
    },
    {
      "epoch": 18.176444510122653,
      "grad_norm": 1.1210592985153198,
      "learning_rate": 1.9706172109748288e-05,
      "loss": 0.619,
      "step": 123000
    },
    {
      "epoch": 18.191222107285355,
      "grad_norm": 0.9318525791168213,
      "learning_rate": 1.9681542781143785e-05,
      "loss": 0.6522,
      "step": 123100
    },
    {
      "epoch": 18.205999704448057,
      "grad_norm": 1.0324429273605347,
      "learning_rate": 1.9656913452539285e-05,
      "loss": 0.6163,
      "step": 123200
    },
    {
      "epoch": 18.22077730161076,
      "grad_norm": 1.0814834833145142,
      "learning_rate": 1.9632284123934782e-05,
      "loss": 0.6188,
      "step": 123300
    },
    {
      "epoch": 18.235554898773458,
      "grad_norm": 1.0656158924102783,
      "learning_rate": 1.960765479533028e-05,
      "loss": 0.6574,
      "step": 123400
    },
    {
      "epoch": 18.25033249593616,
      "grad_norm": 0.9966025948524475,
      "learning_rate": 1.9583025466725776e-05,
      "loss": 0.6405,
      "step": 123500
    },
    {
      "epoch": 18.265110093098862,
      "grad_norm": 0.9862622022628784,
      "learning_rate": 1.9558396138121276e-05,
      "loss": 0.6123,
      "step": 123600
    },
    {
      "epoch": 18.279887690261564,
      "grad_norm": 1.025415062904358,
      "learning_rate": 1.9533766809516773e-05,
      "loss": 0.64,
      "step": 123700
    },
    {
      "epoch": 18.294665287424266,
      "grad_norm": 1.3140411376953125,
      "learning_rate": 1.950913748091227e-05,
      "loss": 0.6499,
      "step": 123800
    },
    {
      "epoch": 18.309442884586968,
      "grad_norm": 0.9387514591217041,
      "learning_rate": 1.9484508152307767e-05,
      "loss": 0.6162,
      "step": 123900
    },
    {
      "epoch": 18.324220481749666,
      "grad_norm": 0.9912253618240356,
      "learning_rate": 1.9459878823703267e-05,
      "loss": 0.633,
      "step": 124000
    },
    {
      "epoch": 18.324220481749666,
      "eval_loss": 0.6355635523796082,
      "eval_runtime": 75.8396,
      "eval_samples_per_second": 158.624,
      "eval_steps_per_second": 19.831,
      "step": 124000
    },
    {
      "epoch": 18.33899807891237,
      "grad_norm": 1.0236140489578247,
      "learning_rate": 1.9435249495098764e-05,
      "loss": 0.6232,
      "step": 124100
    },
    {
      "epoch": 18.35377567607507,
      "grad_norm": 0.9846132397651672,
      "learning_rate": 1.941062016649426e-05,
      "loss": 0.6318,
      "step": 124200
    },
    {
      "epoch": 18.368553273237772,
      "grad_norm": 1.2334895133972168,
      "learning_rate": 1.9385990837889758e-05,
      "loss": 0.6431,
      "step": 124300
    },
    {
      "epoch": 18.383330870400474,
      "grad_norm": 1.189368486404419,
      "learning_rate": 1.936136150928526e-05,
      "loss": 0.6416,
      "step": 124400
    },
    {
      "epoch": 18.398108467563173,
      "grad_norm": 0.8041225075721741,
      "learning_rate": 1.9336732180680755e-05,
      "loss": 0.6308,
      "step": 124500
    },
    {
      "epoch": 18.412886064725875,
      "grad_norm": 0.8218746185302734,
      "learning_rate": 1.9312102852076252e-05,
      "loss": 0.6415,
      "step": 124600
    },
    {
      "epoch": 18.427663661888577,
      "grad_norm": 0.7795408964157104,
      "learning_rate": 1.9287473523471753e-05,
      "loss": 0.6321,
      "step": 124700
    },
    {
      "epoch": 18.44244125905128,
      "grad_norm": 1.285056471824646,
      "learning_rate": 1.926284419486725e-05,
      "loss": 0.6282,
      "step": 124800
    },
    {
      "epoch": 18.45721885621398,
      "grad_norm": 1.2458332777023315,
      "learning_rate": 1.9238214866262746e-05,
      "loss": 0.6297,
      "step": 124900
    },
    {
      "epoch": 18.471996453376683,
      "grad_norm": 1.051910638809204,
      "learning_rate": 1.9213585537658243e-05,
      "loss": 0.631,
      "step": 125000
    },
    {
      "epoch": 18.48677405053938,
      "grad_norm": 1.1586565971374512,
      "learning_rate": 1.9188956209053744e-05,
      "loss": 0.6356,
      "step": 125100
    },
    {
      "epoch": 18.501551647702083,
      "grad_norm": 1.1101418733596802,
      "learning_rate": 1.916432688044924e-05,
      "loss": 0.6387,
      "step": 125200
    },
    {
      "epoch": 18.516329244864785,
      "grad_norm": 0.7449996471405029,
      "learning_rate": 1.9139697551844737e-05,
      "loss": 0.6487,
      "step": 125300
    },
    {
      "epoch": 18.531106842027487,
      "grad_norm": 0.7728233933448792,
      "learning_rate": 1.9115068223240234e-05,
      "loss": 0.617,
      "step": 125400
    },
    {
      "epoch": 18.54588443919019,
      "grad_norm": 1.1261509656906128,
      "learning_rate": 1.9090438894635735e-05,
      "loss": 0.6662,
      "step": 125500
    },
    {
      "epoch": 18.560662036352888,
      "grad_norm": 1.0513843297958374,
      "learning_rate": 1.906580956603123e-05,
      "loss": 0.6392,
      "step": 125600
    },
    {
      "epoch": 18.57543963351559,
      "grad_norm": 1.0877691507339478,
      "learning_rate": 1.904118023742673e-05,
      "loss": 0.6149,
      "step": 125700
    },
    {
      "epoch": 18.59021723067829,
      "grad_norm": 0.9230731725692749,
      "learning_rate": 1.9016550908822225e-05,
      "loss": 0.6125,
      "step": 125800
    },
    {
      "epoch": 18.604994827840994,
      "grad_norm": 1.001188039779663,
      "learning_rate": 1.8991921580217726e-05,
      "loss": 0.6045,
      "step": 125900
    },
    {
      "epoch": 18.619772425003696,
      "grad_norm": 1.6999684572219849,
      "learning_rate": 1.8967292251613223e-05,
      "loss": 0.6305,
      "step": 126000
    },
    {
      "epoch": 18.619772425003696,
      "eval_loss": 0.6341928839683533,
      "eval_runtime": 75.3257,
      "eval_samples_per_second": 159.706,
      "eval_steps_per_second": 19.967,
      "step": 126000
    },
    {
      "epoch": 18.634550022166394,
      "grad_norm": 1.3028099536895752,
      "learning_rate": 1.894266292300872e-05,
      "loss": 0.6498,
      "step": 126100
    },
    {
      "epoch": 18.649327619329096,
      "grad_norm": 1.3011833429336548,
      "learning_rate": 1.8918033594404216e-05,
      "loss": 0.6332,
      "step": 126200
    },
    {
      "epoch": 18.664105216491798,
      "grad_norm": 0.9347653985023499,
      "learning_rate": 1.8893404265799717e-05,
      "loss": 0.6623,
      "step": 126300
    },
    {
      "epoch": 18.6788828136545,
      "grad_norm": 1.1843725442886353,
      "learning_rate": 1.8868774937195214e-05,
      "loss": 0.6539,
      "step": 126400
    },
    {
      "epoch": 18.693660410817202,
      "grad_norm": 1.085433006286621,
      "learning_rate": 1.884414560859071e-05,
      "loss": 0.6283,
      "step": 126500
    },
    {
      "epoch": 18.708438007979904,
      "grad_norm": 1.1340519189834595,
      "learning_rate": 1.8819516279986208e-05,
      "loss": 0.6342,
      "step": 126600
    },
    {
      "epoch": 18.723215605142602,
      "grad_norm": 0.9117011427879333,
      "learning_rate": 1.8794886951381708e-05,
      "loss": 0.6416,
      "step": 126700
    },
    {
      "epoch": 18.737993202305304,
      "grad_norm": 0.8786781430244446,
      "learning_rate": 1.8770257622777205e-05,
      "loss": 0.6106,
      "step": 126800
    },
    {
      "epoch": 18.752770799468006,
      "grad_norm": 0.9378005862236023,
      "learning_rate": 1.87456282941727e-05,
      "loss": 0.6294,
      "step": 126900
    },
    {
      "epoch": 18.76754839663071,
      "grad_norm": 1.000476360321045,
      "learning_rate": 1.87209989655682e-05,
      "loss": 0.628,
      "step": 127000
    },
    {
      "epoch": 18.78232599379341,
      "grad_norm": 1.1032764911651611,
      "learning_rate": 1.86963696369637e-05,
      "loss": 0.6435,
      "step": 127100
    },
    {
      "epoch": 18.79710359095611,
      "grad_norm": 1.183156967163086,
      "learning_rate": 1.8671740308359196e-05,
      "loss": 0.6118,
      "step": 127200
    },
    {
      "epoch": 18.81188118811881,
      "grad_norm": 0.7150366902351379,
      "learning_rate": 1.8647110979754693e-05,
      "loss": 0.6223,
      "step": 127300
    },
    {
      "epoch": 18.826658785281513,
      "grad_norm": 1.1325488090515137,
      "learning_rate": 1.862248165115019e-05,
      "loss": 0.6221,
      "step": 127400
    },
    {
      "epoch": 18.841436382444215,
      "grad_norm": 0.7215073704719543,
      "learning_rate": 1.859785232254569e-05,
      "loss": 0.6292,
      "step": 127500
    },
    {
      "epoch": 18.856213979606917,
      "grad_norm": 1.103716254234314,
      "learning_rate": 1.8573222993941187e-05,
      "loss": 0.6383,
      "step": 127600
    },
    {
      "epoch": 18.87099157676962,
      "grad_norm": 1.233559489250183,
      "learning_rate": 1.8548593665336684e-05,
      "loss": 0.6316,
      "step": 127700
    },
    {
      "epoch": 18.885769173932317,
      "grad_norm": 1.787363052368164,
      "learning_rate": 1.852396433673218e-05,
      "loss": 0.6359,
      "step": 127800
    },
    {
      "epoch": 18.90054677109502,
      "grad_norm": 1.0536264181137085,
      "learning_rate": 1.849933500812768e-05,
      "loss": 0.6374,
      "step": 127900
    },
    {
      "epoch": 18.91532436825772,
      "grad_norm": 1.2787009477615356,
      "learning_rate": 1.8474705679523178e-05,
      "loss": 0.647,
      "step": 128000
    },
    {
      "epoch": 18.91532436825772,
      "eval_loss": 0.6278188228607178,
      "eval_runtime": 75.6931,
      "eval_samples_per_second": 158.931,
      "eval_steps_per_second": 19.87,
      "step": 128000
    },
    {
      "epoch": 18.930101965420423,
      "grad_norm": 0.8123847842216492,
      "learning_rate": 1.8450076350918675e-05,
      "loss": 0.6698,
      "step": 128100
    },
    {
      "epoch": 18.944879562583125,
      "grad_norm": 1.426937460899353,
      "learning_rate": 1.8425447022314175e-05,
      "loss": 0.648,
      "step": 128200
    },
    {
      "epoch": 18.959657159745824,
      "grad_norm": 1.0507967472076416,
      "learning_rate": 1.8400817693709672e-05,
      "loss": 0.6353,
      "step": 128300
    },
    {
      "epoch": 18.974434756908526,
      "grad_norm": 0.9898267984390259,
      "learning_rate": 1.837618836510517e-05,
      "loss": 0.6204,
      "step": 128400
    },
    {
      "epoch": 18.989212354071228,
      "grad_norm": 1.0614479780197144,
      "learning_rate": 1.8351559036500666e-05,
      "loss": 0.6192,
      "step": 128500
    },
    {
      "epoch": 19.00398995123393,
      "grad_norm": 1.2281490564346313,
      "learning_rate": 1.8326929707896166e-05,
      "loss": 0.6067,
      "step": 128600
    },
    {
      "epoch": 19.01876754839663,
      "grad_norm": 1.428240180015564,
      "learning_rate": 1.8302300379291663e-05,
      "loss": 0.6378,
      "step": 128700
    },
    {
      "epoch": 19.033545145559334,
      "grad_norm": 1.3064014911651611,
      "learning_rate": 1.827767105068716e-05,
      "loss": 0.6206,
      "step": 128800
    },
    {
      "epoch": 19.048322742722032,
      "grad_norm": 1.0739799737930298,
      "learning_rate": 1.8253041722082657e-05,
      "loss": 0.6366,
      "step": 128900
    },
    {
      "epoch": 19.063100339884734,
      "grad_norm": 0.8706842064857483,
      "learning_rate": 1.8228412393478157e-05,
      "loss": 0.6038,
      "step": 129000
    },
    {
      "epoch": 19.077877937047436,
      "grad_norm": 1.1905800104141235,
      "learning_rate": 1.8203783064873654e-05,
      "loss": 0.658,
      "step": 129100
    },
    {
      "epoch": 19.092655534210138,
      "grad_norm": 0.9460486769676208,
      "learning_rate": 1.817915373626915e-05,
      "loss": 0.6204,
      "step": 129200
    },
    {
      "epoch": 19.10743313137284,
      "grad_norm": 1.0806339979171753,
      "learning_rate": 1.8154524407664648e-05,
      "loss": 0.6236,
      "step": 129300
    },
    {
      "epoch": 19.12221072853554,
      "grad_norm": 0.9712991714477539,
      "learning_rate": 1.8129895079060148e-05,
      "loss": 0.6311,
      "step": 129400
    },
    {
      "epoch": 19.13698832569824,
      "grad_norm": 1.2348144054412842,
      "learning_rate": 1.8105265750455645e-05,
      "loss": 0.6119,
      "step": 129500
    },
    {
      "epoch": 19.151765922860942,
      "grad_norm": 1.2999327182769775,
      "learning_rate": 1.8080636421851142e-05,
      "loss": 0.6171,
      "step": 129600
    },
    {
      "epoch": 19.166543520023644,
      "grad_norm": 0.896817147731781,
      "learning_rate": 1.805600709324664e-05,
      "loss": 0.609,
      "step": 129700
    },
    {
      "epoch": 19.181321117186346,
      "grad_norm": 1.430932641029358,
      "learning_rate": 1.803137776464214e-05,
      "loss": 0.615,
      "step": 129800
    },
    {
      "epoch": 19.19609871434905,
      "grad_norm": 0.9883264899253845,
      "learning_rate": 1.8006748436037636e-05,
      "loss": 0.6332,
      "step": 129900
    },
    {
      "epoch": 19.210876311511747,
      "grad_norm": 1.009542465209961,
      "learning_rate": 1.7982119107433133e-05,
      "loss": 0.6237,
      "step": 130000
    },
    {
      "epoch": 19.210876311511747,
      "eval_loss": 0.6296155452728271,
      "eval_runtime": 75.263,
      "eval_samples_per_second": 159.84,
      "eval_steps_per_second": 19.983,
      "step": 130000
    },
    {
      "epoch": 19.22565390867445,
      "grad_norm": 0.8756978511810303,
      "learning_rate": 1.795748977882863e-05,
      "loss": 0.617,
      "step": 130100
    },
    {
      "epoch": 19.24043150583715,
      "grad_norm": 1.2993886470794678,
      "learning_rate": 1.793286045022413e-05,
      "loss": 0.6274,
      "step": 130200
    },
    {
      "epoch": 19.255209102999853,
      "grad_norm": 0.8134912848472595,
      "learning_rate": 1.7908231121619627e-05,
      "loss": 0.6267,
      "step": 130300
    },
    {
      "epoch": 19.269986700162555,
      "grad_norm": 1.2259427309036255,
      "learning_rate": 1.7883601793015124e-05,
      "loss": 0.6037,
      "step": 130400
    },
    {
      "epoch": 19.284764297325253,
      "grad_norm": 0.8195787072181702,
      "learning_rate": 1.785897246441062e-05,
      "loss": 0.6203,
      "step": 130500
    },
    {
      "epoch": 19.299541894487955,
      "grad_norm": 1.468687653541565,
      "learning_rate": 1.783434313580612e-05,
      "loss": 0.6327,
      "step": 130600
    },
    {
      "epoch": 19.314319491650657,
      "grad_norm": 1.068482756614685,
      "learning_rate": 1.7809713807201618e-05,
      "loss": 0.6118,
      "step": 130700
    },
    {
      "epoch": 19.32909708881336,
      "grad_norm": 1.2220370769500732,
      "learning_rate": 1.7785084478597115e-05,
      "loss": 0.5958,
      "step": 130800
    },
    {
      "epoch": 19.34387468597606,
      "grad_norm": 0.9830862879753113,
      "learning_rate": 1.7760455149992612e-05,
      "loss": 0.6053,
      "step": 130900
    },
    {
      "epoch": 19.358652283138763,
      "grad_norm": 1.205005407333374,
      "learning_rate": 1.7735825821388112e-05,
      "loss": 0.6169,
      "step": 131000
    },
    {
      "epoch": 19.37342988030146,
      "grad_norm": 1.1261435747146606,
      "learning_rate": 1.771119649278361e-05,
      "loss": 0.6005,
      "step": 131100
    },
    {
      "epoch": 19.388207477464164,
      "grad_norm": 1.2352224588394165,
      "learning_rate": 1.7686567164179106e-05,
      "loss": 0.6349,
      "step": 131200
    },
    {
      "epoch": 19.402985074626866,
      "grad_norm": 1.220428228378296,
      "learning_rate": 1.7661937835574603e-05,
      "loss": 0.6356,
      "step": 131300
    },
    {
      "epoch": 19.417762671789568,
      "grad_norm": 1.1933952569961548,
      "learning_rate": 1.7637308506970103e-05,
      "loss": 0.6101,
      "step": 131400
    },
    {
      "epoch": 19.43254026895227,
      "grad_norm": 1.447247862815857,
      "learning_rate": 1.76126791783656e-05,
      "loss": 0.6417,
      "step": 131500
    },
    {
      "epoch": 19.447317866114968,
      "grad_norm": 1.1295822858810425,
      "learning_rate": 1.7588049849761097e-05,
      "loss": 0.6468,
      "step": 131600
    },
    {
      "epoch": 19.46209546327767,
      "grad_norm": 1.3997957706451416,
      "learning_rate": 1.7563420521156594e-05,
      "loss": 0.6,
      "step": 131700
    },
    {
      "epoch": 19.476873060440372,
      "grad_norm": 0.9580318927764893,
      "learning_rate": 1.753879119255209e-05,
      "loss": 0.633,
      "step": 131800
    },
    {
      "epoch": 19.491650657603074,
      "grad_norm": 0.8116894364356995,
      "learning_rate": 1.751416186394759e-05,
      "loss": 0.6032,
      "step": 131900
    },
    {
      "epoch": 19.506428254765776,
      "grad_norm": 0.8964685797691345,
      "learning_rate": 1.7489532535343088e-05,
      "loss": 0.6208,
      "step": 132000
    },
    {
      "epoch": 19.506428254765776,
      "eval_loss": 0.6266363263130188,
      "eval_runtime": 72.195,
      "eval_samples_per_second": 166.632,
      "eval_steps_per_second": 20.832,
      "step": 132000
    },
    {
      "epoch": 19.521205851928478,
      "grad_norm": 1.2226303815841675,
      "learning_rate": 1.7464903206738585e-05,
      "loss": 0.6254,
      "step": 132100
    },
    {
      "epoch": 19.535983449091177,
      "grad_norm": 1.2218226194381714,
      "learning_rate": 1.7440273878134082e-05,
      "loss": 0.6213,
      "step": 132200
    },
    {
      "epoch": 19.55076104625388,
      "grad_norm": 0.8651322722434998,
      "learning_rate": 1.7415644549529582e-05,
      "loss": 0.6307,
      "step": 132300
    },
    {
      "epoch": 19.56553864341658,
      "grad_norm": 1.1860089302062988,
      "learning_rate": 1.739101522092508e-05,
      "loss": 0.6131,
      "step": 132400
    },
    {
      "epoch": 19.580316240579283,
      "grad_norm": 1.2922536134719849,
      "learning_rate": 1.7366385892320576e-05,
      "loss": 0.6129,
      "step": 132500
    },
    {
      "epoch": 19.595093837741985,
      "grad_norm": 0.7479858994483948,
      "learning_rate": 1.7341756563716073e-05,
      "loss": 0.6318,
      "step": 132600
    },
    {
      "epoch": 19.609871434904683,
      "grad_norm": 1.094513177871704,
      "learning_rate": 1.731712723511157e-05,
      "loss": 0.6119,
      "step": 132700
    },
    {
      "epoch": 19.624649032067385,
      "grad_norm": 1.028367519378662,
      "learning_rate": 1.729249790650707e-05,
      "loss": 0.5982,
      "step": 132800
    },
    {
      "epoch": 19.639426629230087,
      "grad_norm": 1.2763428688049316,
      "learning_rate": 1.7267868577902567e-05,
      "loss": 0.5966,
      "step": 132900
    },
    {
      "epoch": 19.65420422639279,
      "grad_norm": 1.1433478593826294,
      "learning_rate": 1.7243239249298064e-05,
      "loss": 0.6401,
      "step": 133000
    },
    {
      "epoch": 19.66898182355549,
      "grad_norm": 1.4333183765411377,
      "learning_rate": 1.721860992069356e-05,
      "loss": 0.5963,
      "step": 133100
    },
    {
      "epoch": 19.68375942071819,
      "grad_norm": 1.1131963729858398,
      "learning_rate": 1.719398059208906e-05,
      "loss": 0.5717,
      "step": 133200
    },
    {
      "epoch": 19.69853701788089,
      "grad_norm": 0.8713030219078064,
      "learning_rate": 1.7169351263484558e-05,
      "loss": 0.6239,
      "step": 133300
    },
    {
      "epoch": 19.713314615043593,
      "grad_norm": 0.853384256362915,
      "learning_rate": 1.7144721934880055e-05,
      "loss": 0.6379,
      "step": 133400
    },
    {
      "epoch": 19.728092212206295,
      "grad_norm": 0.8812764286994934,
      "learning_rate": 1.7120092606275552e-05,
      "loss": 0.6145,
      "step": 133500
    },
    {
      "epoch": 19.742869809368997,
      "grad_norm": 0.7709099054336548,
      "learning_rate": 1.709546327767105e-05,
      "loss": 0.593,
      "step": 133600
    },
    {
      "epoch": 19.7576474065317,
      "grad_norm": 1.074485182762146,
      "learning_rate": 1.707083394906655e-05,
      "loss": 0.6078,
      "step": 133700
    },
    {
      "epoch": 19.772425003694398,
      "grad_norm": 1.2670605182647705,
      "learning_rate": 1.7046204620462046e-05,
      "loss": 0.6349,
      "step": 133800
    },
    {
      "epoch": 19.7872026008571,
      "grad_norm": 1.073340892791748,
      "learning_rate": 1.7021575291857543e-05,
      "loss": 0.642,
      "step": 133900
    },
    {
      "epoch": 19.801980198019802,
      "grad_norm": 1.45033597946167,
      "learning_rate": 1.699694596325304e-05,
      "loss": 0.6109,
      "step": 134000
    },
    {
      "epoch": 19.801980198019802,
      "eval_loss": 0.6230224370956421,
      "eval_runtime": 72.3192,
      "eval_samples_per_second": 166.346,
      "eval_steps_per_second": 20.797,
      "step": 134000
    },
    {
      "epoch": 19.816757795182504,
      "grad_norm": 1.3643605709075928,
      "learning_rate": 1.697231663464854e-05,
      "loss": 0.6418,
      "step": 134100
    },
    {
      "epoch": 19.831535392345206,
      "grad_norm": 1.180132269859314,
      "learning_rate": 1.6947687306044037e-05,
      "loss": 0.6402,
      "step": 134200
    },
    {
      "epoch": 19.846312989507908,
      "grad_norm": 0.91524738073349,
      "learning_rate": 1.6923057977439534e-05,
      "loss": 0.6368,
      "step": 134300
    },
    {
      "epoch": 19.861090586670606,
      "grad_norm": 1.0434808731079102,
      "learning_rate": 1.689842864883503e-05,
      "loss": 0.6277,
      "step": 134400
    },
    {
      "epoch": 19.875868183833308,
      "grad_norm": 1.069912075996399,
      "learning_rate": 1.687379932023053e-05,
      "loss": 0.6275,
      "step": 134500
    },
    {
      "epoch": 19.89064578099601,
      "grad_norm": 1.1639376878738403,
      "learning_rate": 1.6849169991626028e-05,
      "loss": 0.5993,
      "step": 134600
    },
    {
      "epoch": 19.905423378158712,
      "grad_norm": 1.1696654558181763,
      "learning_rate": 1.6824540663021525e-05,
      "loss": 0.596,
      "step": 134700
    },
    {
      "epoch": 19.920200975321414,
      "grad_norm": 0.8893416523933411,
      "learning_rate": 1.6799911334417022e-05,
      "loss": 0.6307,
      "step": 134800
    },
    {
      "epoch": 19.934978572484113,
      "grad_norm": 1.002177119255066,
      "learning_rate": 1.6775282005812522e-05,
      "loss": 0.6255,
      "step": 134900
    },
    {
      "epoch": 19.949756169646815,
      "grad_norm": 1.0394929647445679,
      "learning_rate": 1.675065267720802e-05,
      "loss": 0.5932,
      "step": 135000
    },
    {
      "epoch": 19.964533766809517,
      "grad_norm": 1.1304854154586792,
      "learning_rate": 1.6726023348603516e-05,
      "loss": 0.596,
      "step": 135100
    },
    {
      "epoch": 19.97931136397222,
      "grad_norm": 1.4467524290084839,
      "learning_rate": 1.6701394019999013e-05,
      "loss": 0.6216,
      "step": 135200
    },
    {
      "epoch": 19.99408896113492,
      "grad_norm": 0.8112678527832031,
      "learning_rate": 1.6676764691394513e-05,
      "loss": 0.6511,
      "step": 135300
    },
    {
      "epoch": 20.00886655829762,
      "grad_norm": 1.1301482915878296,
      "learning_rate": 1.665213536279001e-05,
      "loss": 0.6313,
      "step": 135400
    },
    {
      "epoch": 20.02364415546032,
      "grad_norm": 1.3292312622070312,
      "learning_rate": 1.6627506034185507e-05,
      "loss": 0.6249,
      "step": 135500
    },
    {
      "epoch": 20.038421752623023,
      "grad_norm": 1.6457617282867432,
      "learning_rate": 1.6602876705581004e-05,
      "loss": 0.6123,
      "step": 135600
    },
    {
      "epoch": 20.053199349785725,
      "grad_norm": 1.1871410608291626,
      "learning_rate": 1.6578247376976504e-05,
      "loss": 0.6178,
      "step": 135700
    },
    {
      "epoch": 20.067976946948427,
      "grad_norm": 1.2658634185791016,
      "learning_rate": 1.6553618048372e-05,
      "loss": 0.6439,
      "step": 135800
    },
    {
      "epoch": 20.08275454411113,
      "grad_norm": 0.6881173253059387,
      "learning_rate": 1.6528988719767498e-05,
      "loss": 0.5849,
      "step": 135900
    },
    {
      "epoch": 20.097532141273827,
      "grad_norm": 1.356431245803833,
      "learning_rate": 1.6504359391163e-05,
      "loss": 0.6219,
      "step": 136000
    },
    {
      "epoch": 20.097532141273827,
      "eval_loss": 0.625742495059967,
      "eval_runtime": 72.1145,
      "eval_samples_per_second": 166.818,
      "eval_steps_per_second": 20.856,
      "step": 136000
    },
    {
      "epoch": 20.11230973843653,
      "grad_norm": 0.9014465808868408,
      "learning_rate": 1.6479730062558495e-05,
      "loss": 0.5914,
      "step": 136100
    },
    {
      "epoch": 20.12708733559923,
      "grad_norm": 1.6142610311508179,
      "learning_rate": 1.6455100733953992e-05,
      "loss": 0.5805,
      "step": 136200
    },
    {
      "epoch": 20.141864932761933,
      "grad_norm": 0.8899001479148865,
      "learning_rate": 1.643047140534949e-05,
      "loss": 0.5866,
      "step": 136300
    },
    {
      "epoch": 20.156642529924635,
      "grad_norm": 0.9849956631660461,
      "learning_rate": 1.640584207674499e-05,
      "loss": 0.6222,
      "step": 136400
    },
    {
      "epoch": 20.171420127087334,
      "grad_norm": 0.8544723391532898,
      "learning_rate": 1.6381212748140486e-05,
      "loss": 0.6434,
      "step": 136500
    },
    {
      "epoch": 20.186197724250036,
      "grad_norm": 1.2255371809005737,
      "learning_rate": 1.6356583419535983e-05,
      "loss": 0.5948,
      "step": 136600
    },
    {
      "epoch": 20.200975321412738,
      "grad_norm": 0.9437654614448547,
      "learning_rate": 1.633195409093148e-05,
      "loss": 0.6028,
      "step": 136700
    },
    {
      "epoch": 20.21575291857544,
      "grad_norm": 1.0166888236999512,
      "learning_rate": 1.630732476232698e-05,
      "loss": 0.593,
      "step": 136800
    },
    {
      "epoch": 20.230530515738142,
      "grad_norm": 1.2089570760726929,
      "learning_rate": 1.6282695433722477e-05,
      "loss": 0.6018,
      "step": 136900
    },
    {
      "epoch": 20.245308112900844,
      "grad_norm": 1.0924240350723267,
      "learning_rate": 1.6258066105117974e-05,
      "loss": 0.6007,
      "step": 137000
    },
    {
      "epoch": 20.260085710063542,
      "grad_norm": 0.9039511680603027,
      "learning_rate": 1.623343677651347e-05,
      "loss": 0.6109,
      "step": 137100
    },
    {
      "epoch": 20.274863307226244,
      "grad_norm": 0.9621041417121887,
      "learning_rate": 1.620880744790897e-05,
      "loss": 0.5801,
      "step": 137200
    },
    {
      "epoch": 20.289640904388946,
      "grad_norm": 0.926864743232727,
      "learning_rate": 1.618417811930447e-05,
      "loss": 0.6125,
      "step": 137300
    },
    {
      "epoch": 20.30441850155165,
      "grad_norm": 1.1878877878189087,
      "learning_rate": 1.6159548790699965e-05,
      "loss": 0.5981,
      "step": 137400
    },
    {
      "epoch": 20.31919609871435,
      "grad_norm": 1.0626572370529175,
      "learning_rate": 1.6134919462095462e-05,
      "loss": 0.6092,
      "step": 137500
    },
    {
      "epoch": 20.33397369587705,
      "grad_norm": 1.071658730506897,
      "learning_rate": 1.6110290133490963e-05,
      "loss": 0.6078,
      "step": 137600
    },
    {
      "epoch": 20.34875129303975,
      "grad_norm": 1.0162115097045898,
      "learning_rate": 1.608566080488646e-05,
      "loss": 0.5904,
      "step": 137700
    },
    {
      "epoch": 20.363528890202453,
      "grad_norm": 0.8803654313087463,
      "learning_rate": 1.6061031476281956e-05,
      "loss": 0.6141,
      "step": 137800
    },
    {
      "epoch": 20.378306487365155,
      "grad_norm": 0.9802116751670837,
      "learning_rate": 1.6036402147677453e-05,
      "loss": 0.6031,
      "step": 137900
    },
    {
      "epoch": 20.393084084527857,
      "grad_norm": 0.9681470394134521,
      "learning_rate": 1.6011772819072954e-05,
      "loss": 0.5896,
      "step": 138000
    },
    {
      "epoch": 20.393084084527857,
      "eval_loss": 0.6279779076576233,
      "eval_runtime": 72.1319,
      "eval_samples_per_second": 166.778,
      "eval_steps_per_second": 20.851,
      "step": 138000
    },
    {
      "epoch": 20.40786168169056,
      "grad_norm": 1.0434571504592896,
      "learning_rate": 1.598714349046845e-05,
      "loss": 0.6116,
      "step": 138100
    },
    {
      "epoch": 20.422639278853257,
      "grad_norm": 1.0170680284500122,
      "learning_rate": 1.5962514161863947e-05,
      "loss": 0.6002,
      "step": 138200
    },
    {
      "epoch": 20.43741687601596,
      "grad_norm": 0.9703355431556702,
      "learning_rate": 1.5937884833259444e-05,
      "loss": 0.6169,
      "step": 138300
    },
    {
      "epoch": 20.45219447317866,
      "grad_norm": 1.1929575204849243,
      "learning_rate": 1.5913255504654945e-05,
      "loss": 0.5844,
      "step": 138400
    },
    {
      "epoch": 20.466972070341363,
      "grad_norm": 1.0356638431549072,
      "learning_rate": 1.588862617605044e-05,
      "loss": 0.6046,
      "step": 138500
    },
    {
      "epoch": 20.481749667504065,
      "grad_norm": 0.7128339409828186,
      "learning_rate": 1.586399684744594e-05,
      "loss": 0.602,
      "step": 138600
    },
    {
      "epoch": 20.496527264666764,
      "grad_norm": 1.0091490745544434,
      "learning_rate": 1.5839367518841435e-05,
      "loss": 0.6067,
      "step": 138700
    },
    {
      "epoch": 20.511304861829466,
      "grad_norm": 1.1994352340698242,
      "learning_rate": 1.5814738190236936e-05,
      "loss": 0.6079,
      "step": 138800
    },
    {
      "epoch": 20.526082458992168,
      "grad_norm": 0.9796575307846069,
      "learning_rate": 1.5790108861632433e-05,
      "loss": 0.6005,
      "step": 138900
    },
    {
      "epoch": 20.54086005615487,
      "grad_norm": 0.6643020510673523,
      "learning_rate": 1.576547953302793e-05,
      "loss": 0.6201,
      "step": 139000
    },
    {
      "epoch": 20.55563765331757,
      "grad_norm": 1.741761326789856,
      "learning_rate": 1.5740850204423426e-05,
      "loss": 0.6155,
      "step": 139100
    },
    {
      "epoch": 20.570415250480274,
      "grad_norm": 0.9124384522438049,
      "learning_rate": 1.5716220875818927e-05,
      "loss": 0.598,
      "step": 139200
    },
    {
      "epoch": 20.585192847642972,
      "grad_norm": 1.095476746559143,
      "learning_rate": 1.5691591547214424e-05,
      "loss": 0.6237,
      "step": 139300
    },
    {
      "epoch": 20.599970444805674,
      "grad_norm": 0.9948062896728516,
      "learning_rate": 1.566696221860992e-05,
      "loss": 0.6309,
      "step": 139400
    },
    {
      "epoch": 20.614748041968376,
      "grad_norm": 1.0370771884918213,
      "learning_rate": 1.564233289000542e-05,
      "loss": 0.6195,
      "step": 139500
    },
    {
      "epoch": 20.629525639131078,
      "grad_norm": 1.2574557065963745,
      "learning_rate": 1.5617703561400918e-05,
      "loss": 0.6169,
      "step": 139600
    },
    {
      "epoch": 20.64430323629378,
      "grad_norm": 1.088126301765442,
      "learning_rate": 1.5593074232796415e-05,
      "loss": 0.6174,
      "step": 139700
    },
    {
      "epoch": 20.65908083345648,
      "grad_norm": 1.0007400512695312,
      "learning_rate": 1.556844490419191e-05,
      "loss": 0.589,
      "step": 139800
    },
    {
      "epoch": 20.67385843061918,
      "grad_norm": 1.0390781164169312,
      "learning_rate": 1.5543815575587412e-05,
      "loss": 0.5737,
      "step": 139900
    },
    {
      "epoch": 20.688636027781882,
      "grad_norm": 0.9975423812866211,
      "learning_rate": 1.551918624698291e-05,
      "loss": 0.5926,
      "step": 140000
    },
    {
      "epoch": 20.688636027781882,
      "eval_loss": 0.6256933212280273,
      "eval_runtime": 72.0871,
      "eval_samples_per_second": 166.881,
      "eval_steps_per_second": 20.864,
      "step": 140000
    },
    {
      "epoch": 20.703413624944584,
      "grad_norm": 1.048884630203247,
      "learning_rate": 1.5494556918378406e-05,
      "loss": 0.6332,
      "step": 140100
    },
    {
      "epoch": 20.718191222107286,
      "grad_norm": 1.1396923065185547,
      "learning_rate": 1.5469927589773903e-05,
      "loss": 0.6216,
      "step": 140200
    },
    {
      "epoch": 20.73296881926999,
      "grad_norm": 0.8116475939750671,
      "learning_rate": 1.5445298261169403e-05,
      "loss": 0.5882,
      "step": 140300
    },
    {
      "epoch": 20.747746416432687,
      "grad_norm": 1.155103087425232,
      "learning_rate": 1.54206689325649e-05,
      "loss": 0.6116,
      "step": 140400
    },
    {
      "epoch": 20.76252401359539,
      "grad_norm": 0.957368791103363,
      "learning_rate": 1.5396039603960397e-05,
      "loss": 0.6364,
      "step": 140500
    },
    {
      "epoch": 20.77730161075809,
      "grad_norm": 0.7039283514022827,
      "learning_rate": 1.5371410275355894e-05,
      "loss": 0.5997,
      "step": 140600
    },
    {
      "epoch": 20.792079207920793,
      "grad_norm": 1.0930935144424438,
      "learning_rate": 1.5346780946751394e-05,
      "loss": 0.6292,
      "step": 140700
    },
    {
      "epoch": 20.806856805083495,
      "grad_norm": 1.283557653427124,
      "learning_rate": 1.532215161814689e-05,
      "loss": 0.5982,
      "step": 140800
    },
    {
      "epoch": 20.821634402246193,
      "grad_norm": 0.9032182693481445,
      "learning_rate": 1.5297522289542388e-05,
      "loss": 0.6127,
      "step": 140900
    },
    {
      "epoch": 20.836411999408895,
      "grad_norm": 1.1310900449752808,
      "learning_rate": 1.5272892960937885e-05,
      "loss": 0.621,
      "step": 141000
    },
    {
      "epoch": 20.851189596571597,
      "grad_norm": 1.4581212997436523,
      "learning_rate": 1.5248263632333385e-05,
      "loss": 0.6199,
      "step": 141100
    },
    {
      "epoch": 20.8659671937343,
      "grad_norm": 0.973241925239563,
      "learning_rate": 1.5223634303728882e-05,
      "loss": 0.6115,
      "step": 141200
    },
    {
      "epoch": 20.880744790897,
      "grad_norm": 1.044333577156067,
      "learning_rate": 1.5199004975124379e-05,
      "loss": 0.5798,
      "step": 141300
    },
    {
      "epoch": 20.895522388059703,
      "grad_norm": 0.819963276386261,
      "learning_rate": 1.5174375646519876e-05,
      "loss": 0.606,
      "step": 141400
    },
    {
      "epoch": 20.9102999852224,
      "grad_norm": 1.3671616315841675,
      "learning_rate": 1.5149746317915376e-05,
      "loss": 0.595,
      "step": 141500
    },
    {
      "epoch": 20.925077582385104,
      "grad_norm": 2.1733486652374268,
      "learning_rate": 1.5125116989310873e-05,
      "loss": 0.6071,
      "step": 141600
    },
    {
      "epoch": 20.939855179547806,
      "grad_norm": 1.3671022653579712,
      "learning_rate": 1.510048766070637e-05,
      "loss": 0.6158,
      "step": 141700
    },
    {
      "epoch": 20.954632776710508,
      "grad_norm": 0.8970482349395752,
      "learning_rate": 1.5075858332101867e-05,
      "loss": 0.624,
      "step": 141800
    },
    {
      "epoch": 20.96941037387321,
      "grad_norm": 0.926842987537384,
      "learning_rate": 1.5051229003497367e-05,
      "loss": 0.6011,
      "step": 141900
    },
    {
      "epoch": 20.984187971035908,
      "grad_norm": 0.9773513674736023,
      "learning_rate": 1.5026599674892864e-05,
      "loss": 0.6392,
      "step": 142000
    },
    {
      "epoch": 20.984187971035908,
      "eval_loss": 0.6194217801094055,
      "eval_runtime": 72.0909,
      "eval_samples_per_second": 166.873,
      "eval_steps_per_second": 20.863,
      "step": 142000
    },
    {
      "epoch": 20.99896556819861,
      "grad_norm": 1.3770694732666016,
      "learning_rate": 1.500197034628836e-05,
      "loss": 0.6122,
      "step": 142100
    },
    {
      "epoch": 21.013743165361312,
      "grad_norm": 1.5359407663345337,
      "learning_rate": 1.4977341017683858e-05,
      "loss": 0.6015,
      "step": 142200
    },
    {
      "epoch": 21.028520762524014,
      "grad_norm": 0.7482562065124512,
      "learning_rate": 1.4952711689079358e-05,
      "loss": 0.5863,
      "step": 142300
    },
    {
      "epoch": 21.043298359686716,
      "grad_norm": 1.142001748085022,
      "learning_rate": 1.4928082360474855e-05,
      "loss": 0.6107,
      "step": 142400
    },
    {
      "epoch": 21.058075956849418,
      "grad_norm": 1.233336329460144,
      "learning_rate": 1.4903453031870352e-05,
      "loss": 0.5919,
      "step": 142500
    },
    {
      "epoch": 21.072853554012116,
      "grad_norm": 0.9201781153678894,
      "learning_rate": 1.4878823703265849e-05,
      "loss": 0.5935,
      "step": 142600
    },
    {
      "epoch": 21.08763115117482,
      "grad_norm": 1.1795470714569092,
      "learning_rate": 1.4854194374661347e-05,
      "loss": 0.6137,
      "step": 142700
    },
    {
      "epoch": 21.10240874833752,
      "grad_norm": 1.0192588567733765,
      "learning_rate": 1.4829565046056846e-05,
      "loss": 0.5952,
      "step": 142800
    },
    {
      "epoch": 21.117186345500222,
      "grad_norm": 1.1194206476211548,
      "learning_rate": 1.4804935717452343e-05,
      "loss": 0.6183,
      "step": 142900
    },
    {
      "epoch": 21.131963942662924,
      "grad_norm": 1.0438463687896729,
      "learning_rate": 1.478030638884784e-05,
      "loss": 0.6216,
      "step": 143000
    },
    {
      "epoch": 21.146741539825623,
      "grad_norm": 1.0011436939239502,
      "learning_rate": 1.4755677060243338e-05,
      "loss": 0.5947,
      "step": 143100
    },
    {
      "epoch": 21.161519136988325,
      "grad_norm": 1.211562991142273,
      "learning_rate": 1.4731047731638837e-05,
      "loss": 0.595,
      "step": 143200
    },
    {
      "epoch": 21.176296734151027,
      "grad_norm": 0.9795601963996887,
      "learning_rate": 1.4706418403034334e-05,
      "loss": 0.5983,
      "step": 143300
    },
    {
      "epoch": 21.19107433131373,
      "grad_norm": 1.3284543752670288,
      "learning_rate": 1.4681789074429833e-05,
      "loss": 0.5938,
      "step": 143400
    },
    {
      "epoch": 21.20585192847643,
      "grad_norm": 1.328900694847107,
      "learning_rate": 1.465715974582533e-05,
      "loss": 0.5963,
      "step": 143500
    },
    {
      "epoch": 21.220629525639133,
      "grad_norm": 1.067487359046936,
      "learning_rate": 1.4632530417220826e-05,
      "loss": 0.6077,
      "step": 143600
    },
    {
      "epoch": 21.23540712280183,
      "grad_norm": 0.8616328835487366,
      "learning_rate": 1.4607901088616325e-05,
      "loss": 0.6055,
      "step": 143700
    },
    {
      "epoch": 21.250184719964533,
      "grad_norm": 0.7476653456687927,
      "learning_rate": 1.4583271760011824e-05,
      "loss": 0.6054,
      "step": 143800
    },
    {
      "epoch": 21.264962317127235,
      "grad_norm": 0.8681860566139221,
      "learning_rate": 1.455864243140732e-05,
      "loss": 0.6298,
      "step": 143900
    },
    {
      "epoch": 21.279739914289937,
      "grad_norm": 1.205500602722168,
      "learning_rate": 1.4534013102802817e-05,
      "loss": 0.5826,
      "step": 144000
    },
    {
      "epoch": 21.279739914289937,
      "eval_loss": 0.6249657273292542,
      "eval_runtime": 72.2626,
      "eval_samples_per_second": 166.476,
      "eval_steps_per_second": 20.813,
      "step": 144000
    },
    {
      "epoch": 21.29451751145264,
      "grad_norm": 0.9297580122947693,
      "learning_rate": 1.4509383774198316e-05,
      "loss": 0.6233,
      "step": 144100
    },
    {
      "epoch": 21.309295108615338,
      "grad_norm": 1.2320948839187622,
      "learning_rate": 1.4484754445593815e-05,
      "loss": 0.6064,
      "step": 144200
    },
    {
      "epoch": 21.32407270577804,
      "grad_norm": 1.4961546659469604,
      "learning_rate": 1.4460125116989312e-05,
      "loss": 0.6035,
      "step": 144300
    },
    {
      "epoch": 21.33885030294074,
      "grad_norm": 1.1016772985458374,
      "learning_rate": 1.4435495788384808e-05,
      "loss": 0.5717,
      "step": 144400
    },
    {
      "epoch": 21.353627900103444,
      "grad_norm": 1.2992801666259766,
      "learning_rate": 1.4410866459780305e-05,
      "loss": 0.5913,
      "step": 144500
    },
    {
      "epoch": 21.368405497266146,
      "grad_norm": 0.9823611378669739,
      "learning_rate": 1.4386237131175806e-05,
      "loss": 0.5878,
      "step": 144600
    },
    {
      "epoch": 21.383183094428844,
      "grad_norm": 0.9567462801933289,
      "learning_rate": 1.4361607802571303e-05,
      "loss": 0.6152,
      "step": 144700
    },
    {
      "epoch": 21.397960691591546,
      "grad_norm": 0.7257108092308044,
      "learning_rate": 1.43369784739668e-05,
      "loss": 0.5925,
      "step": 144800
    },
    {
      "epoch": 21.412738288754248,
      "grad_norm": 0.7078368663787842,
      "learning_rate": 1.4312349145362296e-05,
      "loss": 0.5964,
      "step": 144900
    },
    {
      "epoch": 21.42751588591695,
      "grad_norm": 0.9462302327156067,
      "learning_rate": 1.4287719816757797e-05,
      "loss": 0.5971,
      "step": 145000
    },
    {
      "epoch": 21.442293483079652,
      "grad_norm": 1.2270675897598267,
      "learning_rate": 1.4263090488153294e-05,
      "loss": 0.5903,
      "step": 145100
    },
    {
      "epoch": 21.457071080242354,
      "grad_norm": 1.0288466215133667,
      "learning_rate": 1.423846115954879e-05,
      "loss": 0.6085,
      "step": 145200
    },
    {
      "epoch": 21.471848677405053,
      "grad_norm": 1.4544330835342407,
      "learning_rate": 1.4213831830944287e-05,
      "loss": 0.5971,
      "step": 145300
    },
    {
      "epoch": 21.486626274567755,
      "grad_norm": 1.156811237335205,
      "learning_rate": 1.4189202502339788e-05,
      "loss": 0.5979,
      "step": 145400
    },
    {
      "epoch": 21.501403871730457,
      "grad_norm": 1.1116019487380981,
      "learning_rate": 1.4164573173735285e-05,
      "loss": 0.6133,
      "step": 145500
    },
    {
      "epoch": 21.51618146889316,
      "grad_norm": 1.2717758417129517,
      "learning_rate": 1.4139943845130782e-05,
      "loss": 0.5934,
      "step": 145600
    },
    {
      "epoch": 21.53095906605586,
      "grad_norm": 1.1891213655471802,
      "learning_rate": 1.4115314516526278e-05,
      "loss": 0.5782,
      "step": 145700
    },
    {
      "epoch": 21.54573666321856,
      "grad_norm": 1.2258801460266113,
      "learning_rate": 1.4090685187921779e-05,
      "loss": 0.6069,
      "step": 145800
    },
    {
      "epoch": 21.56051426038126,
      "grad_norm": 1.2326524257659912,
      "learning_rate": 1.4066055859317276e-05,
      "loss": 0.5613,
      "step": 145900
    },
    {
      "epoch": 21.575291857543963,
      "grad_norm": 0.8466423749923706,
      "learning_rate": 1.4041426530712773e-05,
      "loss": 0.5901,
      "step": 146000
    },
    {
      "epoch": 21.575291857543963,
      "eval_loss": 0.6177476644515991,
      "eval_runtime": 72.2572,
      "eval_samples_per_second": 166.489,
      "eval_steps_per_second": 20.815,
      "step": 146000
    }
  ],
  "logging_steps": 100,
  "max_steps": 203010,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 30,
  "save_steps": 2000,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 4,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.9454264034304e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
