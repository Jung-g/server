# resource_loader.py

import os
import csv
from sentence_transformers import SentenceTransformer
from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast, T5ForConditionalGeneration, T5TokenizerFast
import torch

class ResourceManager:
    def __init__(self):
        print("="*50)
        print("ì¤‘ì•™ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ì ì´ˆê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

        # --- [ìˆ˜ì •] ê²½ë¡œ ì¬êµ¬ì„±: ì›ë³¸ CSV ê²½ë¡œ ì¶”ê°€ ë° ìƒì„±í•  íŒŒì¼ëª… ë³€ê²½ ---
        current_dir = os.path.dirname(os.path.abspath(__file__))
        dataset_dir = os.path.join(current_dir, "dataset")

        self.PATHS = {
            "kobart_model": os.path.join(dataset_dir, "kobart_checkpoint-10000"),
            "kot5_model": os.path.join(dataset_dir, "kot5_checkpoint-146000"),
            "source_csv": os.path.join(dataset_dir, "ë™ì˜ìƒê°€ëŠ¥ë°ì´í„°ì…‹.txt"), # ì›ë³¸ ë°ì´í„°ì…‹
            "lemma_txt": os.path.join(dataset_dir, "extract_lemma.txt"),   # ì¶”ì¶œí•´ì„œ ë§Œë“¤ íŒŒì¼
            "cache_sbert": os.path.join(dataset_dir, "our_lemma_cache_sbert.pt"),
            "cache_kosim_roberta": os.path.join(dataset_dir, "our_lemma_cache_kosim_roberta.pt"),
            "cache_kosim_multitask": os.path.join(dataset_dir, "our_lemma_cache_kosim_multitask.pt"),
            "cache_bge_m3_korean": os.path.join(dataset_dir, "our_lemma_cache_bge_m3_korean.pt"),
            "cache_ko_sroberta_multitask": os.path.join(dataset_dir, "our_lemma_cache_ko_sroberta_multitask.pt"),
        }

        # --- ëª¨ë¸ ë¡œë”© ë¡œì§ (ë³€ê²½ ì—†ìŒ) ---
        print("\n[1/4] KoBART & KoT5 ëª¨ë¸ ë¡œë”©...")
        self.kobart_tokenizer = PreTrainedTokenizerFast.from_pretrained(self.PATHS["kobart_model"])
        self.kobart_model = BartForConditionalGeneration.from_pretrained(self.PATHS["kobart_model"])
        self.kot5_tokenizer = T5TokenizerFast.from_pretrained(self.PATHS["kot5_model"])
        self.kot5_model = T5ForConditionalGeneration.from_pretrained(self.PATHS["kot5_model"])

        print("\n[2/4] SBERT ê³„ì—´ ëª¨ë¸ 5ì¢… ë¡œë”©...")
        self.SBERT_MODELS = { "sbert": SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS"), "kosim_roberta": SentenceTransformer("BM-K/KoSimCSE-roberta"), "kosim_multitask": SentenceTransformer("BM-K/KoSimCSE-RoBERTa-multitask"), "bge_m3_korean": SentenceTransformer("upskyy/bge-m3-korean"), "ko_sroberta_multitask": SentenceTransformer("jhgan/ko-sroberta-multitask") }
        
        # --- [ìˆ˜ì •] ë‹¨ì–´ ì‚¬ì „ ë¡œë”© ë¡œì§ ë³€ê²½ ---
        print("\n[3/4] ë‹¨ì–´ ì‚¬ì „ ë¡œë”©...")
        self.lemmas_set = self._initialize_lemma_data() # ìƒˆ í•¨ìˆ˜ í˜¸ì¶œ
        self.lemma_list = sorted(list(self.lemmas_set))

        # --- ì‚¬ì „ ì„ë² ë”© ë° ìºì‹œ ê´€ë¦¬ (ë³€ê²½ ì—†ìŒ) ---
        print("\n[4/4] ì‚¬ì „ ì„ë² ë”© ë° ìºì‹œ ê´€ë¦¬...")
        self.SBERT_EMBEDDINGS = { "sbert": self._get_or_create_embeddings("sbert", self.PATHS["cache_sbert"]), "kosim_roberta": self._get_or_create_embeddings("kosim_roberta", self.PATHS["cache_kosim_roberta"]), "kosim_multitask": self._get_or_create_embeddings("kosim_multitask", self.PATHS["cache_kosim_multitask"]), "bge_m3_korean": self._get_or_create_embeddings("bge_m3_korean", self.PATHS["cache_bge_m3_korean"]), "ko_sroberta_multitask": self._get_or_create_embeddings("ko_sroberta_multitask", self.PATHS["cache_ko_sroberta_multitask"]), }
        
        print("\nëª¨ë“  ë¦¬ì†ŒìŠ¤ ë¡œë”© ì™„ë£Œ!")
        print("="*50)

    def _create_lemma_txt_from_csv(self, csv_path, txt_path):
        """[ì‹ ê·œ] ì›ë³¸ CSVë¥¼ ì½ì–´ 'kor' ì—´ì„ ì¶”ì¶œí•˜ê³  TXT íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
        print(f"  â³ ì›ë³¸ CSV '{os.path.basename(csv_path)}'ì—ì„œ ë‹¨ì–´ ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤...")
        lemmas_set = set()
        try:
            with open(csv_path, 'r', encoding='utf-8') as f_in:
                reader = csv.reader(f_in)
                next(reader, None)  # í—¤ë” í–‰ ê±´ë„ˆë›°ê¸°
                for row in reader:
                    if len(row) >= 2:
                        word = row[1].strip()
                        if word:
                            lemmas_set.add(word)
            
            with open(txt_path, 'w', encoding='utf-8') as f_out:
                for word in sorted(list(lemmas_set)):
                    f_out.write(word + '\n')
            
            print(f"  âœ… '{os.path.basename(txt_path)}' ìƒì„± ì™„ë£Œ (ì´ {len(lemmas_set)}ê°œ ë‹¨ì–´).")
            return True
        except FileNotFoundError:
            print(f"  ğŸš¨ ì˜¤ë¥˜: ì›ë³¸ CSV íŒŒì¼ '{csv_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return False
        except Exception as e:
            print(f"  ğŸš¨ ì˜¤ë¥˜: CSV ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            return False

    def _initialize_lemma_data(self):
        """[ì‹ ê·œ] lemma.txtê°€ ì—†ìœ¼ë©´ ë§Œë“¤ê³ , ìˆìœ¼ë©´ ë¡œë“œí•˜ëŠ” í†µí•© í•¨ìˆ˜"""
        source_csv_path = self.PATHS["source_csv"]
        target_txt_path = self.PATHS["lemma_txt"]

        if not os.path.exists(target_txt_path):
            print(f"  â„¹ï¸ ë‹¨ì–´ ëª©ë¡ íŒŒì¼ '{os.path.basename(target_txt_path)}'ì´ ì—†ì–´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            creation_success = self._create_lemma_txt_from_csv(source_csv_path, target_txt_path)
            if not creation_success:
                print("  ğŸš¨ ë‹¨ì–´ ëª©ë¡ ìƒì„± ì‹¤íŒ¨. ë¹ˆ ëª©ë¡ìœ¼ë¡œ ê³„ì†í•©ë‹ˆë‹¤.")
                return set()

        lemmas_set = set()
        try:
            with open(target_txt_path, 'r', encoding='utf-8') as f:
                for line in f:
                    lemmas_set.add(line.strip())
            print(f"  âœ… ë‹¨ì–´ ëª©ë¡ íŒŒì¼ ë¡œë”© ì„±ê³µ (ì´ {len(lemmas_set)}ê°œ ë‹¨ì–´).")
        except FileNotFoundError:
            print(f"  ğŸš¨ ì˜¤ë¥˜: íŒŒì¼ì„ ìƒì„±í–ˆìŒì—ë„ '{os.path.basename(target_txt_path)}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return lemmas_set

    def _get_or_create_embeddings(self, model_name, cache_path):
        """(ë‚´ìš© ë³€ê²½ ì—†ìŒ)"""
        model = self.SBERT_MODELS[model_name]
        try:
            lemma_list_cache, lemma_embeddings = torch.load(cache_path)
            if lemma_list_cache == self.lemma_list:
                print(f"  âœ… '{model_name}' ëª¨ë¸ ìºì‹œ ë¡œë”© ì„±ê³µ.")
                return lemma_embeddings
            else:
                print(f"  ğŸ”„ '{model_name}' ëª¨ë¸ ìºì‹œ ìƒˆë¡œ ìƒì„± (ë‹¨ì–´ ëª©ë¡ ë³€ê²½ë¨)")
        except Exception:
            print(f"  â„¹ï¸ '{model_name}' ëª¨ë¸ ìºì‹œ ì—†ìŒ ë˜ëŠ” ì˜¤ë¥˜. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        
        lemma_embeddings = model.encode(self.lemma_list, convert_to_tensor=True, normalize_embeddings=True, show_progress_bar=True)
        torch.save((self.lemma_list, lemma_embeddings), cache_path)
        return lemma_embeddings

# í”„ë¡œê·¸ë¨ ì „ì²´ì—ì„œ ì‚¬ìš©ë  ë‹¨ í•˜ë‚˜ì˜ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
resources = ResourceManager()